{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e9df40-e6d7-4893-9a93-ef417b6e2f90",
   "metadata": {},
   "source": [
    "# A Practical Guide to Model Quantization in PyTorch\n",
    "\n",
    "Deploying deep learning models often requires optimizing them for efficiency, especially in environments with limited resources. While models are typically trained using high-precision 32-bit floating-point numbers to capture the small, exact adjustments needed during learning, this level of precision is not always necessary for inference once the model is trained. \n",
    "\n",
    "**Model quantization** addresses this by converting a model's weights and activations to a lower-precision format, like 8-bit integers, leading to smaller model sizes and faster performance.\n",
    "\n",
    "In this notebook, you will embark on a hands-on journey through the landscape of quantization with PyTorch. You will begin by establishing a performance benchmark with a standard floating-point model. Then, you will explore and apply three distinct strategies, each building on the last, to see their effects in practice.\n",
    "\n",
    "* **Dynamic Quantization**: You'll start with a straightforward, post-training technique that provides an immediate size reduction with minimal code by converting weights to integers and quantizing activations on-the-fly during inference.\n",
    "* **Static Quantization**: Next, you'll implement a more involved post-training method that can yield better performance. This technique involves a **calibration** step, where you run sample data through the model to determine the best way to quantize the activations ahead of time.\n",
    "* **Quantization-Aware Training (QAT)**: Finally, you'll use the most advanced technique, which simulates quantization effects during a fine-tuning phase. This allows the model to adapt its weights to the rounding noise, helping to achieve the highest possible accuracy in the final quantized model.\n",
    "\n",
    "To see these concepts applied in a practical scenario, an optional section will guide you through quantizing a large, pre-trained Visual Question Answering (VQA) model. Through this process, you will gain the practical knowledge to evaluate trade-offs and choose the right optimization strategy for your own projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fb22d-b36a-4c7a-b4e2-280ad12c38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as DisplayImage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bceeadf-bf37-4160-b730-bdff9fea3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717188b-66eb-432a-9b0e-52542c4f83b0",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Before applying advanced optimization techniques like quantization, it is crucial to first establish a benchmark. This benchmark is your baseline model â€” the original, fully-trained, and un-optimized version of the network.\n",
    "\n",
    "By measuring its key performance characteristics, such as model size and inference speed, you create a standard reference point. This baseline is essential because it allows you to concretely measure the effectiveness of your optimizations. Later, as you apply different quantization techniques, you will compare the results of each new model directly against this baseline to clearly see the improvements and any potential trade-offs. \n",
    "\n",
    "### CNN Model Architecture\n",
    "\n",
    "* Define the architecture for the Convolutional Neural Network (`CNN`) that will be used as a baseline throughout this lab.\n",
    "* The architecture includes:\n",
    "    * A sequence of `torch.nn.Conv2d` layers for feature extraction, each followed by a `torch.nn.BatchNorm2d` layer to stabilize learning.\n",
    "    * `torch.nn.MaxPool2d` layers to downsample the feature maps after each convolutional block.\n",
    "    * `torch.nn.Dropout` applied between the fully connected layers to reduce overfitting.\n",
    "    * A final classification head made of three `torch.nn.Linear` layers.\n",
    "    * The `forward` method orchestrates the flow of data through these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5800f-b01e-4379-9ea4-0f7977297246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the convolutional and batch normalization layers \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Define a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Define a dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # Define the fully connected layers for the classification head \n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        # Define the ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        # Pass input through the sequence of convolutional blocks \n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Flatten the output from the convolutional layers for the fully connected layers \n",
    "        x = x.view(-1, 512 * 2 * 2) \n",
    "        # Pass data through the fully connected layers \n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d224d89-2a83-4143-99bf-78750b225c60",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "* Call the helper function, `load_cifar10`, to download and prepare the `CIFAR10` dataset.\n",
    "* It creates and returns `trainloader` and `testloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fa92f-91f6-4c1f-8db1-7a5b7efdd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = helper_utils.load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998150d4-acb9-4950-b7bb-e87aa2703dde",
   "metadata": {},
   "source": [
    "### Model Training (Optional)\n",
    "\n",
    "The next logical step is to train the `CNN` model on the CIFAR10 dataset. However, training can be time-consuming, taking approximately 10 minutes for 30 epochs in this GPU environment.\n",
    "\n",
    "To save time, you are provided a pre-trained model that has already been trained. Below are the logs from its training session. \n",
    "\n",
    "<center>\n",
    "    <img src=\"pretrained_model_logs.png\" alt=\"Model Train Logs\" width=\"1000\">\n",
    "</center>\n",
    "\n",
    "The best results achieved were: a validation loss of **0.4086** and an accuracy of **88.20%**.\n",
    "\n",
    "* **Validation Loss:** 0.4086\n",
    "* **Validation Accuracy:** 88.20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b00aff-59e5-4945-83f1-959b898d2ae9",
   "metadata": {},
   "source": [
    "```python\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "# Set number of training epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Run the training loop\n",
    "helper_utils.training_loop(model, trainloader, testloader, num_epochs, DEVICE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16922d-fee8-40cc-a2a2-575341d916e3",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model\n",
    "\n",
    "* Define the path to the provided pretrained model, `cifar10_cnn_30_epochs_best.pt`.\n",
    "    * **NOTE**: If you trained your own model in the optional cell above, please make sure to set the path to `baseline_model_path = 'cifar10_cnn_best.pt'`, which is the name used by the training script for the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8074ee1-9195-4f5b-ab29-0d05adc64420",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_path = 'cifar10_cnn_30_epochs_best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f635fdd-7c97-47ab-b1c9-560da1bf31db",
   "metadata": {},
   "source": [
    "* Create an instance of the `CNN` model.\n",
    "* Load the pre-trained weights from the `baseline_model_path` into the model structure.\n",
    "* Switch the model to evaluation mode, which is a necessary step for the quantization APIs to work correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334ed83-5d63-4a1e-bf72-ecda56610df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN model architecture\n",
    "baseline_model = CNN()\n",
    "# Load the pre-trained weights from the file specified in baseline_model_path\n",
    "baseline_model.load_state_dict(torch.load(baseline_model_path))\n",
    "# Set the model to evaluation mode\n",
    "baseline_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07f3ad-cba5-47f9-931d-fd83f131e843",
   "metadata": {},
   "source": [
    "### Baseline Performance Metrics\n",
    "\n",
    "* Calculate the model's size and inference time to establish the baseline performance metrics for the quantization comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e6cf2-d6e4-4028-baa9-b45e767e96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's size in megabytes (MB)\n",
    "baseline_model_size = helper_utils.get_model_size(baseline_model)\n",
    "# Measure the average inference time in milliseconds (ms)\n",
    "baseline_model_inf_time = helper_utils.measure_average_inference_time_ms(baseline_model)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Baseline model size: {baseline_model_size:.2f} MB\")\n",
    "print(f\"Baseline model inference time: {baseline_model_inf_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a2ba6-bf14-4f4a-9d97-3526602234e1",
   "metadata": {},
   "source": [
    "## Dynamic Quantization\n",
    "\n",
    "With the baseline established, you can now begin optimizing the model. The first and simplest approach you will explore is **Dynamic Quantization**. \n",
    "\n",
    "This strategy is straightforward: it involves converting only the weights in the model from the standard 32-bit floating-point format to a more efficient, lower-precision format like 8-bit integers (INT8).  The activations, on the other hand, are calculated in floating-point during inference and are quantized on-the-fly as they are passed to the quantized weight operations.  This method is easy to apply and can yield significant benefits, such as reducing the model's size by half and speeding up inference time. \n",
    "\n",
    "Before you apply this technique, it's useful to inspect the current data types of the model's weights. The following code will print the dtype for each Conv2d and Linear layer, giving you a clear \"before\" picture that confirms all weights are in the standard torch.float32 format.\n",
    "\n",
    "* Print the `dtype` for each `Conv2d` and `Linear` layer, giving you a clear \"before\" picture that confirms all weights are in the standard `torch.float32` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a086c8-a117-41ff-9873-5739cf50b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes before quantization ---\")\n",
    "# Iterate through the model's layers\n",
    "for name, module in baseline_model.named_modules():\n",
    "    # Check if the layer is a Conv2d or Linear layer\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2a772-6dfe-49b5-acb5-d4d8594c8b9d",
   "metadata": {},
   "source": [
    "### Applying Dynamic Quantization\n",
    "\n",
    "* Use the [torch.quantization.quantize_dynamic](https://docs.pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html) function to perform this operation.\n",
    "    * `baseline_model`: The first argument is the floating-point model you want to quantize.\n",
    "    * `{nn.Linear, nn.Conv2d}`: This specifies the set of layer types you want to dynamically quantize.\n",
    "    * `dtype=torch.qint8`: This tells the function the target data type for the quantized weights.\n",
    "* Save the state dictionary of the newly created quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e203f-f14c-4b1f-918f-e552620dbfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "quantized_dynamic_model = torch.quantization.quantize_dynamic(\n",
    "    # Model to be quantized. \n",
    "    baseline_model,\n",
    "    # The layers to quantize\n",
    "    {nn.Linear, nn.Conv2d},\n",
    "    # The target data type for the quantized weights\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the state dictionary of the newly quantized model. \n",
    "torch.save(quantized_dynamic_model.state_dict(), 'cifar10_cnn_quantized_dynamic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92550f-5235-464f-9142-5ea1c61f9db3",
   "metadata": {},
   "source": [
    "*After* quantization, the specified layers are swapped out for new, dynamically quantized versions. This means:\n",
    "\n",
    "* Your previous `torch.nn.Linear` layers are now instances of `torch.nn.quantized.dynamic.Linear`.\n",
    "* Similarly, any `torch.nn.Conv2d` layers would become `torch.nn.quantized.dynamic.Conv2d`.\n",
    "\n",
    "You can now inspect the data types of these new layers to confirm the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444de6b0-363d-4a47-b1e4-fef9b69b5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes after dynamic quantization ---\")\n",
    "# Iterate through the quantized model's layers\n",
    "for name, module in quantized_dynamic_model.named_modules():\n",
    "    # Check if the layer is a dynamically quantized Conv2d or Linear layer\n",
    "    if isinstance(module, (torch.nn.quantized.dynamic.Conv2d, torch.nn.quantized.dynamic.Linear)):\n",
    "        # For quantized layers, the weight is packed and must be accessed as a method\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight().dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe085967-b06d-4d63-bf62-1bbe782cc913",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Notice something interesting? Even though you passed `nn.Conv2d` layers to the function, only the `Linear` layers were actually converted to the `torch.qint8` data type. This is because PyTorch's dynamic quantization implementation is highly optimized for operations where the weights are the primary bottleneck, which is most often the case with `Linear` layers.\n",
    "\n",
    "This isn't a limitation of quantization overall. The more advanced techniques you'll explore next, **Static Quantization** and **Quantization-Aware Training (QAT)**, are designed to handle and quantize `Conv2d` layers effectively, which is essential for getting the best performance out of CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a36f1-b7ac-49cc-ba73-266d50283f9e",
   "metadata": {},
   "source": [
    "### Compare Performance\n",
    "\n",
    "* Now that you've applied dynamic quantization, measure the size and inference speed of the new model compared with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e758ab-0c48-4c7e-a3f6-71f8a2c75d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's size in megabytes (MB)\n",
    "quantized_dynamic_model_size = helper_utils.get_model_size(quantized_dynamic_model)\n",
    "# Measure the average inference time in milliseconds (ms)\n",
    "quantized_dynamic_model_inf_time = helper_utils.measure_average_inference_time_ms(quantized_dynamic_model)\n",
    "\n",
    "# Generate the Markdown comparison table\n",
    "helper_utils.comparison_table(\n",
    "    baseline_model_size=baseline_model_size,\n",
    "    baseline_model_time=baseline_model_inf_time,\n",
    "    quantized_model_size=quantized_dynamic_model_size,\n",
    "    quantized_model_time=quantized_dynamic_model_inf_time,\n",
    "    quantization_type=\"Dynamic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a698d-6ea3-4d45-b2bd-54ef16953ff3",
   "metadata": {},
   "source": [
    "## Static Quantization\n",
    "\n",
    "Next, you will explore **Static Quantization**, a more powerful but also more involved optimization technique compared to its dynamic counterpart.\n",
    "\n",
    "The key difference is that static quantization converts both the model's weights and its activations to a lower-precision integer format, such as INT8. Because activations are also being quantized, the process requires an extra, critical step: **calibration**.\n",
    "\n",
    "The following cells will guide you through the complete workflow:\n",
    "\n",
    "### Statically Quantized CNN Architecture\n",
    "\n",
    "* Define a new `QuantizedCNN` class with two key additions:\n",
    "    * <code>self.quant = [torch.quantization.QuantStub()](https://docs.pytorch.org/docs/2.7/generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub)</code>: This is a \"quantization stub\" module that you will place at the very beginning of your `forward` pass. Its job is to convert the incoming floating-point tensors into quantized tensors.\n",
    "    * <code>self.dequant = [torch.quantization.DeQuantStub()](https://docs.pytorch.org/docs/2.7/generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub)</code>: This is a \"dequantization stub\" that you will place at the very end. It converts the quantized output tensors back into floating-point format.\n",
    "* The rest of the architecture remains identical to the original CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d314c-0946-490d-bc20-7f31ed613289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedCNN, self).__init__()\n",
    "        # Add a QuantStub module. This will convert the incoming floating-point tensor to a quantized tensor. \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "\n",
    "        # The core layers of the model remain the same as the baseline CNN. \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "        # Add a DeQuantStub module. This will convert the final quantized tensor back to a floating-point tensor. \n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Explicitly quantize the input tensor at the beginning of the forward pass. \n",
    "        x = self.quant(x)\n",
    "\n",
    "        # The main forward pass logic remains the same.\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        x = x.reshape(-1, 512 * 2 * 2)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Explicitly dequantize the output tensor before returning it. \n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3a9d0-4228-4a7e-a57c-a61ea41f361e",
   "metadata": {},
   "source": [
    "### Prepare Model for Static Quantization\n",
    "\n",
    "* Perform the initial setup required before you can calibrate and convert the model.\n",
    "    * First, create an instance of your new `QuantizedCNN` class.\n",
    "    * Next, copy the weights from the original trained `baseline_model` into this new quantization-ready instance.\n",
    "    * Set the model to evaluation mode.\n",
    "    * Finally, you attach a quantization configuration (`qconfig`) to the model. Here, you use `torch.quantization.get_default_qconfig('x86')`, which is the recommended default configuration for server-side inference on x86 CPUs. This step also implicitly prepares the model by inserting \"observer\" modules that will be used to analyze data flow in the next step.\n",
    "        * **NOTE:** While the older `'fbgemm'` backend is still available, `'x86'` is the new recommended default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ba93b-ab4e-425c-91a6-6adc7aa241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the new QuantizedCNN class\n",
    "quantized_static_model = QuantizedCNN()\n",
    "\n",
    "# Copy the learned weights from the pre-trained baseline_model into the new quantized_static_model\n",
    "quantized_static_model.load_state_dict(baseline_model.state_dict())\n",
    "# Set the model to evaluation mode\n",
    "quantized_static_model.eval()\n",
    "\n",
    "# Set the quantization configuration for the model. \n",
    "# 'fbgemm' is a configuration optimized for server-side inference on x86 CPUs. \n",
    "# This also attaches observer modules that will be used during calibration. \n",
    "quantized_static_model.qconfig = torch.quantization.get_default_qconfig('x86')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6786926-4184-4e63-bdf1-d617c76e1e0f",
   "metadata": {},
   "source": [
    "### Prepare and Calibrate\n",
    "\n",
    "* Use [torch.quantization.prepare](https://docs.pytorch.org/docs/stable/generated/torch.ao.quantization.prepare.html).\n",
    "    * This function takes the model with the attached `qconfig` and formally prepares it for calibration by activating the observer modules that were inserted in the previous step. These observers are now ready to watch the data that flows through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bc4cf-9ffd-4362-a47e-6b7507027ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for quantization\n",
    "torch.quantization.prepare(quantized_static_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f9ab4-5117-4533-b152-fb928f96364a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After preparing the model, you can inspect its layers. At this stage, no weights have been converted yet. The preparation step has inserted **observer modules** that will watch the data flow during calibration, but the core layers still use 32-bit floating-point weights.\n",
    "\n",
    "* Print the `dtype` for each `Conv2d` and `Linear` layer to confirm they are still in `torch.float32` format before calibration and conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f462d8-6a56-4ba1-b8fa-244a7bd6e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes before static conversion ---\")\n",
    "# Iterate through the model's layers\n",
    "for name, module in quantized_static_model.named_modules():\n",
    "    # Check if the layer is a Conv2d or Linear layer\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf9c17-e5b4-42b6-a000-d7ce07c02b6f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Calibration** is a step unique to static quantization where you must feed the model representative, unlabelled data. This allows the observers to watch the range and distribution of the activation tensors at different points in the model.\n",
    "\n",
    "* Define a `calibrate` function to perform this process.\n",
    "* The function takes in the prepared `quantized_static_model` and the `testloader`. It will iterate through a number of batches, and for each batch, it will run a forward pass.\n",
    "    * During these passes, the observers collect statistics that will be used to determine the optimal quantization parameters for the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efa692-796f-473e-9da1-511ba7ec59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model, data_loader, num_batches=50):\n",
    "    \"\"\"Feeds sample data through the model to calibrate quantization observers.\"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Determine the actual number of batches for calibration\n",
    "    # It's the smaller of num_batches or the total size of the data_loader\n",
    "    total_batches_available = len(data_loader)\n",
    "    calibration_batches = min(num_batches, total_batches_available)\n",
    "\n",
    "    # Use torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Wrap the data_loader with tqdm for a progress bar\n",
    "        progress_bar = tqdm(data_loader, total=calibration_batches, desc=\"Calibrating\")\n",
    "        \n",
    "        # Iterate through the specified number of batches\n",
    "        for i, (image, _) in enumerate(progress_bar):\n",
    "            # Break the loop if processed enough batches\n",
    "            if i >= calibration_batches:\n",
    "                break\n",
    "            \n",
    "            # Feed the data to the model. This activates the observers.\n",
    "            model(image)\n",
    "\n",
    "    print(f\"\\nCalibration finished after processing {calibration_batches} out of {total_batches_available} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea540f8-62bf-4334-9904-30a2cdd0927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate with sample data\n",
    "calibrate(quantized_static_model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d35ed-0b6c-4a5c-8756-cc49b5df61e3",
   "metadata": {},
   "source": [
    "### Convert and Save Model\n",
    "\n",
    "* Now that the model has been calibrated, [convert](https://docs.pytorch.org/docs/stable/generated/torch.ao.quantization.convert.html) it into a fully quantized model.\n",
    "    * This takes the calibrated model and uses the statistics gathered by the observers to permanently convert the model's weights and activations to the INT8 format.\n",
    "    * The `inplace=True` argument modifies the model directly, which saves memory by not creating a new copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422ae21-c46b-4dcb-8492-011b9aa66e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prepared and calibrated model to a fully quantized model\n",
    "torch.quantization.convert(quantized_static_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee3a5f-5b6c-4a32-a79b-7498ac51f5b5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that the model has been converted, you can inspect the data types again. \n",
    "\n",
    "*After* static quantization, the `Conv2d` and `Linear` layers are replaced with their fused and quantized counterparts. This means:\n",
    "\n",
    "* Your previous `torch.nn.Conv2d` layers are now instances of `torch.nn.quantized.Conv2d`.\n",
    "* Your previous `torch.nn.Linear` layers are now instances of `torch.nn.quantized.Linear`.\n",
    "\n",
    "Run the following cell to confirms that, unlike dynamic quantization, **both the convolutional and linear layers** have been successfully converted to use `torch.qint8` weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d4d59-38f4-49a1-a737-db88ebc1dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes after static conversion ---\")\n",
    "# Iterate through the quantized model's layers\n",
    "for name, module in quantized_static_model.named_modules():\n",
    "    # Check if the layer is a statically quantized Conv2d or Linear layer\n",
    "    if isinstance(module, (torch.nn.quantized.Conv2d, torch.nn.quantized.Linear)):\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight().dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802dda64-82fa-4921-8d06-6752b35250ea",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Save the state dictionary of the final, highly optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba60c4-69dd-4be5-85fc-3367ab848566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state dictionary of the final, statically quantized model to a file\n",
    "torch.save(quantized_static_model.state_dict(), 'cifar10_cnn_quantized_static.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826781b-434f-4447-88ab-aa58720a1422",
   "metadata": {},
   "source": [
    "### Compare Performance\n",
    "\n",
    "* Now that you've applied static quantization, measure the size and inference speed of the new model compared with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f6f41-5eab-424a-b4a3-846a799fb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's size in megabytes (MB)\n",
    "quant_static_model_size = helper_utils.get_model_size(quantized_static_model)\n",
    "# Measure the average inference time in milliseconds (ms)\n",
    "quant_static_model_inf_time = helper_utils.measure_average_inference_time_ms(quantized_static_model)\n",
    "\n",
    "# Generate the Markdown comparison table\n",
    "helper_utils.comparison_table(\n",
    "    baseline_model_size=baseline_model_size,\n",
    "    baseline_model_time=baseline_model_inf_time,\n",
    "    quantized_model_size=quant_static_model_size,\n",
    "    quantized_model_time=quant_static_model_inf_time,\n",
    "    quantization_type=\"Static\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c6a41-34ee-4cc3-8f1b-98aa23213a92",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training (QAT)\n",
    "\n",
    "Finally, you will explore **Quantization-Aware Training (QAT)**. This is the most involved quantization method, but it can yield the best performance and accuracy compared to post-training methods.\n",
    "\n",
    "Unlike the previous techniques, QAT simulates the effects of quantization *during* a training or fine-tuning phase. This allows the model to adapt its weights to the precision loss introduced by quantization, which can significantly improve the final accuracy of the quantized model.\n",
    "\n",
    "The workflow involves several key stages which you will now step through.\n",
    "\n",
    "### QAT-Ready CNN \n",
    "\n",
    "To perform QAT and the necessary optimization of \"layer fusion,\" you must first modify the baseline CNN architecture.\n",
    "\n",
    "* **Separate ReLU Modules**: Activation functions like `ReLU` are defined as separate layers (e.g., `self.relu1`) instead of being called functionally within the `forward` pass. This is a requirement for layer fusion.\n",
    "* **Layer Fusion Method**: A `fuse_model` method is added to perform layer fusion. It uses the [torch.quantization.fuse_modules](https://docs.pytorch.org/docs/stable/generated/torch.ao.quantization.fuse_modules.fuse_modules.html) function to combine common sequences like (Conv, BN, ReLU) into single, optimized layers.\n",
    "* **Quantization Stubs**: Just like with static quantization, `QuantStub` and `DeQuantStub` are added to mark the entry and exit points for quantization.\n",
    "* **QAT-Compatible Operations**: Some operations are not supported in the QAT workflow. For example, `.view()` has been replaced with `.reshape()`, which is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b1f3-378d-4306-9a0b-b7a91c624fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QATCNN, self).__init__()\n",
    "        # Add a QuantStub module to convert the input from float to quantized format.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # Add a DeQuantStub module to convert the output from quantized to float format.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        # --- Define the model's layers ---\n",
    "        # Note: For QAT and layer fusion, ReLU activations must be defined as separate nn.Module layers.\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Convolutional Block 4\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Shared layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Fully Connected Block 1\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.relu_fc1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Fully Connected Block 2\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu_fc2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Output Layer\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Quantize the input tensor.\n",
    "        x = self.quant(x)\n",
    "\n",
    "        # 2. Pass data through the network, using the separated layer modules.\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Reshape for the fully connected layers.\n",
    "        x = x.reshape(-1, 512 * 2 * 2)\n",
    "        \n",
    "        x = self.relu_fc1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu_fc2(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # 3. Dequantize the output tensor before returning.\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        # Define a new method to fuse layers. This is a key optimization for QAT.\n",
    "        # It combines a sequence of layers (e.g., Conv, BN, ReLU) into a single, more efficient layer.\n",
    "        \n",
    "        # Fuse convolutional blocks\n",
    "        torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu1'], inplace=True)\n",
    "        torch.quantization.fuse_modules(self, ['conv2', 'bn2', 'relu2'], inplace=True)\n",
    "        torch.quantization.fuse_modules(self, ['conv3', 'bn3', 'relu3'], inplace=True)\n",
    "        torch.quantization.fuse_modules(self, ['conv4', 'bn4', 'relu4'], inplace=True)\n",
    "        \n",
    "        # Fuse fully connected blocks\n",
    "        torch.quantization.fuse_modules(self, ['fc1', 'relu_fc1'], inplace=True)\n",
    "        torch.quantization.fuse_modules(self, ['fc2', 'relu_fc2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f6905-d718-4999-b172-8d6925821414",
   "metadata": {},
   "source": [
    "### Prepare Model for QAT\n",
    "\n",
    "Since the `QATCNN` architecture is different from the original `CNN`, you cannot simply load the weights. You must first create a mapping to transfer the learned weights from the baseline model to the new QAT-ready architecture.\n",
    "\n",
    "#### Instantiate Model and Load Weights\n",
    "\n",
    "* First, create an instance of the new `QATCNN` model.\n",
    "* Next, load the `state_dict` (which contains the learned weights) from the pre-trained `baseline_model` into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9114dd9-33c1-4172-bd4c-93482e305ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the QAT-ready model architecture\n",
    "qat_model = QATCNN()\n",
    "\n",
    "# Load the state dictionary from the pre-trained baseline model using the defined path\n",
    "baseline_model_state_dict = torch.load(baseline_model_path)\n",
    "\n",
    "# Get the state dictionary of the new, un-trained QAT model.\n",
    "qat_model_state_dict = qat_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b9d53-2607-4873-ac97-360707aee29a",
   "metadata": {},
   "source": [
    "#### Map and Transfer Weights\n",
    "\n",
    "* Now, define a `mapping` dictionary to explicitly link the layer names from the old model architecture to the new one.\n",
    "* Finally, iterate through this mapping to transfer the weights and load the updated `state_dict` into your `qat_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffdeda-4ed4-41b1-bfd1-26a6428a0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping to transfer weights\n",
    "mapping = {\n",
    "    # conv1\n",
    "    \"conv1.weight\": \"conv1.weight\",\n",
    "    \"conv1.bias\": \"conv1.bias\",\n",
    "    \"bn1.weight\": \"bn1.weight\",\n",
    "    \"bn1.bias\": \"bn1.bias\",\n",
    "    \"bn1.running_mean\": \"bn1.running_mean\",\n",
    "    \"bn1.running_var\": \"bn1.running_var\",\n",
    "    # conv2\n",
    "    \"conv2.weight\": \"conv2.weight\",\n",
    "    \"conv2.bias\": \"conv2.bias\",\n",
    "    \"bn2.weight\": \"bn2.weight\",\n",
    "    \"bn2.bias\": \"bn2.bias\",\n",
    "    \"bn2.running_mean\": \"bn2.running_mean\",\n",
    "    \"bn2.running_var\": \"bn2.running_var\",\n",
    "    # conv3\n",
    "    \"conv3.weight\": \"conv3.weight\",\n",
    "    \"conv3.bias\": \"conv3.bias\",\n",
    "    \"bn3.weight\": \"bn3.weight\",\n",
    "    \"bn3.bias\": \"bn3.bias\",\n",
    "    \"bn3.running_mean\": \"bn3.running_mean\",\n",
    "    \"bn3.running_var\": \"bn3.running_var\",\n",
    "    # conv4\n",
    "    \"conv4.weight\": \"conv4.weight\",\n",
    "    \"conv4.bias\": \"conv4.bias\",\n",
    "    \"bn4.weight\": \"bn4.weight\",\n",
    "    \"bn4.bias\": \"bn4.bias\",\n",
    "    \"bn4.running_mean\": \"bn4.running_mean\",\n",
    "    \"bn4.running_var\": \"bn4.running_var\",\n",
    "    # fc layers\n",
    "    \"fc1.weight\": \"fc1.weight\",\n",
    "    \"fc1.bias\": \"fc1.bias\",\n",
    "    \"fc2.weight\": \"fc2.weight\",\n",
    "    \"fc2.bias\": \"fc2.bias\",\n",
    "    \"fc3.weight\": \"fc3.weight\",\n",
    "    \"fc3.bias\": \"fc3.bias\",\n",
    "}\n",
    "\n",
    "# Transfer weights using the mapping\n",
    "for old_key, new_key in mapping.items():\n",
    "    if old_key in baseline_model_state_dict and new_key in qat_model_state_dict:\n",
    "        qat_model_state_dict[new_key] = baseline_model_state_dict[old_key]\n",
    "\n",
    "# Load the newly mapped weights into the QAT model\n",
    "qat_model.load_state_dict(qat_model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836df0e-6026-46e6-b092-53306939771e",
   "metadata": {},
   "source": [
    "### Fuse Layers and Set Configuration\n",
    "\n",
    "With the weights transferred, you can now begin preparing the model for the QAT fine-tuning process.\n",
    "\n",
    "* First, you must set the model to evaluation mode (`.eval()`) as a prerequisite for layer fusion.\n",
    "* Call the `fuse_model()` method to combine the sequential layers into single, optimized units.\n",
    "* Set the QAT-specific quantization configuration. While other configurations like `x86 / 'fbgemm'` are available for server-side CPUs, for a QAT workflow where the fine-tuning loop is run on a GPU (which is what you will do), the `'qnnpack'` backend is required for operator compatibility.\n",
    "    * The `'qnnpack'` backend itself is optimized for mobile (ARM) CPUs, which is a common deployment target for quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf60b7-a8ac-4aca-80e2-6d80177a55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode. This is a required step before layer fusion.\n",
    "qat_model.eval()\n",
    "\n",
    "# Call the custom `fuse_model` method to combine sequential Conv-BN-ReLU and Linear-ReLU layers.\n",
    "qat_model.fuse_model()\n",
    "\n",
    "# Set the Quantization-Aware Training specific quantization configuration.\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e244aaa-5e94-4bda-bc70-e5c08b68a3ba",
   "metadata": {},
   "source": [
    "### Prepare Model for QAT Training\n",
    "\n",
    "* Crucially, you must switch the model back to training mode (`.train()`) before the final preparation step.\n",
    "* Use the [torch.quantization.prepare_qat](https://docs.pytorch.org/docs/stable/generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat) function to insert \"fake quantization\" modules. These modules simulate quantization errors during the fine-tuning process, allowing the model to learn to be robust to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec82719-57f2-4625-99ab-9b59aa870ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model back to training mode. This is a requirement for the `prepare_qat` function.\n",
    "qat_model.train()\n",
    "\n",
    "# Use `torch.quantization.prepare_qat` to prepare the model for Quantization-Aware Training.\n",
    "qat_model = torch.quantization.prepare_qat(qat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11398ecc-b36a-41fb-8bf1-f212d74cba3f",
   "metadata": {},
   "source": [
    "### QAT Fine-Tuning\n",
    "\n",
    "This is the core \"training\" part of Quantization-Aware Training, where you will fine-tune the prepared model for a small number of epochs to help it adapt to quantization noise.\n",
    "\n",
    "* The training loop itself is a standard PyTorch training loop and contains no special operations for QAT. You will notice it does not include a validation step; that is because this loop operates on the floating-point model that only simulates quantization effects.\n",
    "* The \"awareness\" comes from the model itself, which simulates these quantization effects during the forward and backward passes.\n",
    "* This process allows the model's weights to adjust to the precision loss, which helps recover the final model's accuracy.\n",
    "* The true performance evaluation is reserved for the final, converted integer model, which you will create in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50a239-3539-4ae5-8d0a-1dfee02ce27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the QAT fine-tuning loop\n",
    "qat_model = helper_utils.train_qat(qat_model, trainloader, DEVICE, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ecd57-9203-4d5f-a94d-56be54b953f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After fine-tuning, the `qat_model` is now \"aware\" of quantization effects, but it is not yet a true integer model. The combination of layer fusion and the `prepare_qat` function has transformed the model's original architecture, swapping standard layers for specialized QAT-ready modules. These new modules now have **fake quantization** observers attached, which simulated the noise and rounding of INT8 operations during training.\n",
    "\n",
    "Specifically, the model now contains these key layer types with learnable weights:\n",
    "\n",
    "* `torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d`\n",
    "* `torch.ao.nn.intrinsic.qat.modules.linear_relu.LinearReLU`\n",
    "* `torch.ao.nn.qat.modules.linear.Linear`\n",
    "\n",
    "Despite these structural changes, the underlying weights of these new modules are still in `torch.float32` format to allow for gradient updates during the fine-tuning process. The following code verifies this state right before the final conversion to a true integer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77841c-7efc-4f25-a5c1-abfc0d914237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes before final QAT conversion ---\")\n",
    "# Iterate through the model's layers\n",
    "for name, module in qat_model.named_modules():\n",
    "    # Check for all QAT-prepared layers using their full module paths\n",
    "    if isinstance(module, (torch.ao.nn.intrinsic.qat.modules.conv_fused.ConvReLU2d, \n",
    "                           torch.ao.nn.intrinsic.qat.modules.linear_relu.LinearReLU, \n",
    "                           torch.ao.nn.qat.modules.linear.Linear)):\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517ba89-0e4c-49e4-81dc-6a4e06f49090",
   "metadata": {},
   "source": [
    "### Final Conversion and Evaluation\n",
    "\n",
    "The fine-tuning process has prepared the floating-point model for the inaccuracies of quantization. Now, before you can evaluate its final performance, you must convert this \"aware\" model into a true integer-only model.\n",
    "\n",
    "#### Convert and Save the Final Model\n",
    "\n",
    "* The model must first be moved to the CPU, if it already wasn't, as the conversion to a fully quantized model is a CPU-only operation.\n",
    "* Use `torch.quantization.convert` to finalize the process. This replaces the fake quantization modules with real integer-based operations and quantizes the weights to INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aaf9f0-4511-4fb8-b3c9-58ca8d247ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU for conversion\n",
    "qat_model.to(\"cpu\")\n",
    "\n",
    "# Convert the QAT-trained model to a final, fully quantized integer model\n",
    "final_quantized_qat_model = torch.quantization.convert(qat_model.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a38c14-673d-41b7-a06e-d7c0cf2e746b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "With the final conversion complete, the \"aware\" floating-point model is now a true, high-performance integer model. The fake quantization modules have been removed and replaced with actual INT8 operations.\n",
    "\n",
    "You can inspect the final data types to confirm that all the targeted `Conv2d` and `Linear` layers have been successfully converted to use `torch.qint8` weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736a24a-6952-4f76-bcce-aef45bb6e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Weight dtypes after final QAT conversion ---\")\n",
    "# Iterate through the final quantized model's layers\n",
    "for name, module in final_quantized_qat_model.named_modules():\n",
    "    # Check if the layer is a statically quantized Conv2d or Linear layer\n",
    "    if isinstance(module, (torch.nn.quantized.Conv2d, torch.nn.quantized.Linear)):\n",
    "        print(f\"Layer: {name:<10} | Weight dtype: {module.weight().dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb20292-c3e2-4e7f-a6cb-a224c92f5d0b",
   "metadata": {},
   "source": [
    "* Finally, save the state dictionary of the converted, integer-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a6805-5cca-4df5-b47b-ecc0c59e1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final quantized model\n",
    "torch.save(final_quantized_qat_model.state_dict(), 'cifar10_cnn_qat_quantized.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b852a2-f764-4052-b8cc-83620538c3f4",
   "metadata": {},
   "source": [
    "#### Evaluate the Quantized Model's Accuracy\n",
    "\n",
    "* Now that the model has been converted into its final INT8 format, you can perform the definitive evaluation to measure its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba846d3-f602-4931-b94f-7182a478353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "helper_utils.evaluate_qat(final_quantized_qat_model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f5a0f-7bce-4134-94ee-44f046a7635c",
   "metadata": {},
   "source": [
    "### Compare Performance\n",
    "\n",
    "* Now that you've performed QAT, measure the size and inference speed of the new model compared with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d62e0d-2a40-4896-b31b-75fdcda92046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's size in megabytes (MB)\n",
    "qat_model_size = helper_utils.get_model_size(final_quantized_qat_model)\n",
    "# Measure the average inference time in milliseconds (ms)\n",
    "qat_model_inf_time = helper_utils.measure_average_inference_time_ms(final_quantized_qat_model)\n",
    "\n",
    "# Generate the Markdown comparison table\n",
    "helper_utils.comparison_table(\n",
    "    baseline_model_size=baseline_model_size,\n",
    "    baseline_model_time=baseline_model_inf_time,\n",
    "    quantized_model_size=qat_model_size,\n",
    "    quantized_model_time=qat_model_inf_time,\n",
    "    quantization_type=\"QAT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832b642-9bd9-41f6-b9fd-b762bce4a867",
   "metadata": {},
   "source": [
    "## Overall Model Quantization Results\n",
    "\n",
    "Finally, the following code gathers all the calculated performance metrics into a dictionary and calls a helper function to display a summary table, allowing you to compare the results of all quantization methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93073b8-e65d-4572-bfbc-e17dd68df27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the calculated statistics into a dictionary\n",
    "all_model_stats = {\n",
    "    \"Baseline\": (baseline_model_size, baseline_model_inf_time),\n",
    "    \"Dynamic Quantized\": (quantized_dynamic_model_size, quantized_dynamic_model_inf_time),\n",
    "    \"Static Quantized\": (quant_static_model_size, quant_static_model_inf_time),\n",
    "    \"QAT Quantized\": (qat_model_size, qat_model_inf_time)\n",
    "}\n",
    "\n",
    "# Display the final comparison table\n",
    "helper_utils.display_full_comparison(all_model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd2446-3b1b-4c2a-9fa4-2c835db83efe",
   "metadata": {},
   "source": [
    "Now that you have explored dynamic, static, and quantization-aware training in detail, you might be wondering when to apply each method in your own work. Understanding the strengths, trade-offs, and suitable contexts for each technique is crucial to maximizing the benefits of your model optimization efforts. Let's delve into some best practices and use cases for each strategy to help you decide which approach aligns best with your specific needs and goals.\n",
    "\n",
    "**Dynamic Quantization**\n",
    "\n",
    "* **Best Practices**: This is the simplest method to apply and serves as an excellent starting point for quantization. It doesn't require any changes to the model architecture or a calibration dataset.\n",
    "* **Use Cases**: Dynamic quantization is most effective when the model's weights take up a significant amount of memory and the bottleneck is memory bandwidth. It is particularly useful for models where providing a representative dataset for calibration is difficult, such as models with highly variable activations like LSTMs and Transformers. Use it when you need a quick and easy way to reduce model size with a moderate speed-up.\n",
    "  \n",
    "**Static Quantization**\n",
    "\n",
    "* **Best Practices**: To get the best results, you must provide a representative dataset for the calibration step. This allows the model to accurately determine the quantization parameters for the activations. Fusing layers (e.g., Conv-BN-ReLU) before calibration is also highly recommended for better performance.\n",
    "* **Use Cases**: This is the recommended method for maximizing inference speed on CPUs, especially for models with stable activation patterns like CNNs. It is ideal for server-side applications where you can calibrate on a sample of the validation data and deploy on x86 CPUs (using the `'x86'` backend).\n",
    "\n",
    "**Quantization-Aware Training (QAT)**\n",
    "\n",
    "* **Best Practices**: QAT requires the most setup, including modifying the model architecture for layer fusion and running a short fine-tuning loop (typically for 1-5 epochs with a small learning rate). Always start with a well-trained floating-point model before beginning the QAT process.\n",
    "* **Use Cases**: QAT should be your go-to method when you need the highest possible accuracy for your quantized model. Use it when post-training static or dynamic quantization results in an unacceptable drop in performance. It is essential for mission-critical applications where you need the size and speed benefits of quantization without significantly compromising on accuracy.\n",
    "\n",
    "  \n",
    "By understanding these techniques and their implications, you can now tailor your quantization strategy to best fit your application's needs, whether you are prioritizing ease of implementation, maximum inference speed, or the highest possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8968f-e6b4-475e-a02f-7218c0a29072",
   "metadata": {},
   "source": [
    "## (Optional) A Real-World Application: Quantization of a Hugging Face VQA \n",
    "\n",
    "In this section, you will move beyond the custom CNN and apply your knowledge to a practical, real-world scenario. You'll take a large, pre-trained model from the Hugging Face Hub and optimize it using quantization.\n",
    "\n",
    "You will see the end-to-end process of loading a powerful Visual Question Answering (VQA) model, establishing its baseline performance, applying dynamic quantization, and then comparing the final model's size and inference speed against the original. Crucially, you'll also compare the actual answers generated by both models to see how optimization impacts output quality. This addresses the key question: even if the model is smaller and faster, are its results still just as good?\n",
    "\n",
    "**Why a VQA model and why Dynamic Quantization?**\n",
    "\n",
    "This example uses a VQA model because it represents a modern, complex architecture â€” a Transformerâ€”which is widely used but often large and computationally intensive, making it a perfect candidate for optimization.\n",
    "\n",
    "For this example, you will use **Dynamic Quantization**. Based on the best practices discussed earlier, this method is the ideal choice for several reasons:\n",
    "\n",
    "* It is the simplest technique to apply, serving as an excellent starting point for quantization as it requires no changes to the model architecture or a calibration dataset.\n",
    "* It is particularly effective for models with highly variable activations, such as **LSTMs** and **Transformers**.\n",
    "* It is especially useful in cases where providing a representative dataset for calibration is difficult, which can be the case for complex multi-modal inputs like in VQA.\n",
    "* It provides a quick and easy way to significantly reduce model size with a moderate speed-up, which is often the primary goal when deploying large, pre-trained models.\n",
    "\n",
    "### Load and Prepare the VQA Model\n",
    "\n",
    "* Use the helper function, `get_blip_vqa_model_and_processor` to load the necessary components for performing VQA with the [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base) model. The function returns two objects:\n",
    "    * **blip_vqa_model**: The pre-trained `Salesforce/blip-vqa-base` model.\n",
    "    * **blip_vqa_processor**: The helper that handles the model's (`blip_vqa_model`) required **preprocessing**, converting raw images and text into the exact tensor format it needs.\n",
    "        * In the Hugging Face ecosystem, it is standard for models to be paired with these specific processors. You can think of the processor as a mandatory first step that ensures the data is perfectly formatted for the PyTorch model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d39d78-ea8e-474e-8717-916d390d8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_vqa_model, blip_vqa_processor = helper_utils.get_blip_vqa_model_and_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ccc60-ceef-4617-8f26-20a08022cb5f",
   "metadata": {},
   "source": [
    "* Move the blip_vqa_model and all its parameters to the specified `DEVICE` (`cpu`).\n",
    "    * This is ideal because PyTorch's quantization techniques are highly optimized for CPU inference, making the performance benefits most apparent.\n",
    "* Set the model to evaluation mode.\n",
    "* Finally, calculate the model's size to establish the first part of your performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9148e86-41cb-4dbb-9fed-0ca534d4c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute device as \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Move the model and all of its parameters to the specified device (the CPU)\n",
    "blip_vqa_model.to(DEVICE)\n",
    "\n",
    "# Set the model to evaluation model\n",
    "blip_vqa_model.eval()\n",
    "\n",
    "# Now that the model is fully prepared, calculate its size for your baseline\n",
    "blip_baseline_model_size = helper_utils.get_model_size(blip_vqa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8506ba6-ba73-4bf0-8b41-41a1279bbb3a",
   "metadata": {},
   "source": [
    "## Upload and Verify Your Own Image\n",
    "\n",
    "Alright, now for the fun part! You've loaded the model and prepared it for inference; it's time to see it in action.\n",
    "\n",
    "You're about to perform **Visual Question Answering (VQA)**. VQA is exactly what it sounds like: you can provide the model with an image and then ask it questions in plain English, just like you would with a person.\n",
    "\n",
    "The code below will display an upload widget. Feel free to upload any image you like from your computer â€” a photo of your pet, a vacation picture, anything! After your image is displayed, you'll be able to ask the model questions about it, such as \"What is the dog doing?\" or \"How many people are in this photo?\".\n",
    "\n",
    "Running the function `helper_utils.upload_jpg_widget()` will display a widget that allows you to upload your own images into the workspace.\n",
    "\n",
    "* You can only upload images that have a `.jpg` extension.\n",
    "* Each image should not exceed **5 MB** in **file size**.\n",
    "* Once an image is successfully uploaded, you'll see its file path dsiplayed, which you can directly copy and paste into the cell below.\n",
    "\n",
    "Also, once the widget is displayed, you can use it multiple times to upload images; you don't have to re-run the `helper_utils.upload_jpg_widget()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5e26a-ec52-4956-b71e-563a75893340",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.upload_jpg_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58415fea-90b5-4621-8862-042b7f74d71a",
   "metadata": {},
   "source": [
    "* Set the path to your image (as displayed above).\n",
    "    * For convenience, a default image path is already provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942f9d2-231d-46e3-a385-0c796212a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './images/eiffel_tower.jpg' ### <-- Replace with your image path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb765e0d-91e5-440f-9bcf-e485000086da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "DisplayImage(image_path, width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e53172-09bb-41f0-a281-8740a1e3c2ce",
   "metadata": {},
   "source": [
    "## Run Inference on the Full-Precision Model\n",
    "\n",
    "* Define the question you want to ask about the image.\n",
    "    * You can change the text in the string to ask your own custom question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b3a09-a39c-4ddc-bcb0-7bd7a855c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Describe the scene in the image.\" ### <-- Replace with your question here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24a2b3-ea4b-4c6d-9df9-9079730372f9",
   "metadata": {},
   "source": [
    "* Now, you will run the VQA task using the full-precision baseline model.\n",
    "* Call the `perform_vqa` helper function, which handles the entire inference process and returns both the generated answer and the inference time it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4100c-3618-4d49-af14-f6733cd493f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_answer, blip_model_inf_time = helper_utils.perform_vqa(blip_vqa_model, blip_vqa_processor, image_path, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c0790-d499-4aba-b746-aab6cd8e44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print Results ---\n",
    "print(f\"Baseline Model Size: {blip_baseline_model_size:.2f} MB\")\n",
    "print(f\"Baseline Inference Time: {blip_model_inf_time:.4f} s\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Baseline Model Answer: {baseline_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816c174-fad7-4b78-93c1-2ddf0e10391d",
   "metadata": {},
   "source": [
    "## Run Inference on Dynamically Quantized Model\n",
    "\n",
    "* First, apply dynamic quantization on the BLIP VQA model.\n",
    "    * `blip_vqa_model`: The variable holding the baseline model that you want to quantize.\n",
    "        * **Note** that this operation only targets the model. The \"processor\" is simply a data-preparation helper; it doesn't contain the large, weighted neural network layers suitable for this optimization, so there's no need to consider it for quantization.\n",
    "    * `{torch.nn.Linear}`: Only the `Linear` layers will be quantized. For Transformer-based models like BLIP, these layers contain the vast majority of the model's parameters and computational work, making them the most effective target for optimization.\n",
    "    * `dtype=torch.qint8`: This sets the target data type for the quantized weights to 8-bit signed integers.\n",
    "* After quantization, calculate the new model's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71640b14-0375-4fa2-ac99-59fcb8274966",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Apply dynamic quantization to the Linear layers of the BLIP model\n",
    "    quantized_blip_vqa_model = torch.quantization.quantize_dynamic(\n",
    "        blip_vqa_model, \n",
    "        {torch.nn.Linear}, \n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    \n",
    "    print(\"Dynamic quantization applied successfully to the BLIP model.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to apply dynamic quantization. Error: {e}\")\n",
    "\n",
    "\n",
    "# Get the quantized model's size.\n",
    "quantized_blip_model_size = helper_utils.get_model_size(quantized_blip_vqa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d7ed6-6da4-4619-9f60-103b380022f3",
   "metadata": {},
   "source": [
    "* Now, you will run the VQA task using the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dde3f3-feeb-4402-adf8-56dc0e6d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_answer, quantized_inf_time = helper_utils.perform_vqa(quantized_blip_vqa_model, blip_vqa_processor, image_path, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2333c-99ec-41a8-91e7-ef953145023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Print Results ---\n",
    "print(f\"Quantized Model Size: {quantized_blip_model_size:.2f} MB\")\n",
    "print(f\"Quantized Inference Time: {quantized_inf_time:.4f} s\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Quantized Model Answer: {quantized_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d74d9-e076-4f2d-a911-1035d6dbb7e8",
   "metadata": {},
   "source": [
    "## Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b3d6c-0293-4464-8e02-d98aa8f747e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.blip_comparison_table(\n",
    "    question=question,\n",
    "    baseline_answer=baseline_answer,\n",
    "    quantized_answer=quantized_answer,\n",
    "    baseline_size=blip_baseline_model_size,\n",
    "    quantized_size=quantized_blip_model_size,\n",
    "    baseline_time_s=blip_model_inf_time,\n",
    "    quantized_time_s=quantized_inf_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3aab3-65c4-43ce-8a87-bb1e260e6b53",
   "metadata": {},
   "source": [
    "Fantastic results! This is the moment where theory comes to life, showing the real-world power of quantization. Take a moment to look at what you've just accomplished, it's impressive!\n",
    "\n",
    "First, look at the **Model Size**. You should see a massive reduction, likely around 65%, shrinking the model to a fraction of its original size! This is a huge victory. In the real world, this means faster download times, lower storage costs, and the ability to deploy powerful models on memory-constrained devices like mobile phones.\n",
    "\n",
    "Now, take a look at the **Answers** from both models. What did they generate for your image? Are they the same? Do you agree with their description? It's incredible to see how even after compressing the model so significantly, the quantized version can produce an answer that is identical or makes just as much sense as the original. This is the goal of optimization: to gain efficiency while keeping the model intelligent and effective.\n",
    "\n",
    "Finally, look at your **Inference Time**. Depending on your specific hardware and the current run, you might see a speed-up, a slow-down, or very little change. While theoretically, 8-bit integer math should be faster, the actual result depends on several factors:\n",
    "\n",
    "* **Quantization Overhead**: Dynamic quantization has a small, fixed cost because it converts data types on-the-fly. For a single inference run, this overhead can sometimes outweigh the speed-up from the faster calculations.\n",
    "* **Hardware and Environment**: Performance gains are highly dependent on the specific CPU architecture. The full benefits are often most visible in highly optimized, production-level environments.\n",
    "* **Batch Size**: Optimization benefits are typically more pronounced when processing large batches of data at once. Since you are only inferring on a single image, the full potential may not be realized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fb2a4-9fc4-47cd-adaf-5f94787db3ea",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you have performed a comprehensive, end-to-end exploration of model quantization. Starting with a floating-point baseline model, you have successfully applied three powerful PyTorch techniques and observed their impact on model size, inference speed, and accuracy.\n",
    "\n",
    "You began with **Dynamic Quantization**, seeing how easily you can achieve a significant reduction in model size, a method that works particularly well for models like RNNs and transformers. You then moved to **Static Quantization**, a more nuanced post-training technique that requires a calibration step to also quantize activations, leading to even greater speed and memory savings, especially for CNNs. Finally, you dove deep into **Quantization-Aware Training (QAT)**. By modifying the model architecture for layer fusion and fine-tuning it to be \"aware\" of quantization noise, you produced a highly compressed model that preserved nearly all of its original accuracy. To ground these concepts, you also applied dynamic quantization to a large, pre-trained VQA model, observing firsthand the significant size reduction and its impact on output quality.\n",
    "\n",
    "The skills you have developed here are essential for moving models from research to production. You are now equipped to analyze the trade-offs between implementation effort, model size, inference speed, and accuracy, allowing you to select the most appropriate quantization strategy for deploying efficient, high-performance models in a variety of real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
