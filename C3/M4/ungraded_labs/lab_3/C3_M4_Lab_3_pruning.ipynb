{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pruning with PyTorch\n",
    "\n",
    "Creating a highly accurate model is a significant achievement, but for many real-world applications, accuracy alone is not enough. A model must also be **fast, lightweight, and efficient** to run on devices with limited memory or processing power, such as mobile phones. This is where model compression techniques become essential, allowing you to trim down your model without a major loss in performance.\n",
    "\n",
    "Pruning is a powerful compression method that operates on a simple principle: not all parts of a trained network are equally important. Many of the millions of parameters in a large model contribute very little to the final predictions. By systematically removing these less useful connections, you can create a more streamlined and efficient network.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <b>Before Pruning</b>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./nb_images/pruning_before.png\" alt=\"Pruning a neural network\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <b>After Pruning</b>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./nb_images/pruning_after.png\" alt=\"Pruning a neural network\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "This lab provides a hands-on guide to implementing various pruning strategies using PyTorch's `torch.nn.utils.prune` module. Through practical examples, you will learn how to apply these techniques, verify their effects, and make the resulting sparsity permanent in your models. You will explore three fundamental approaches:\n",
    "\n",
    "* **Unstructured Pruning**: Removing individual weights based on their magnitude, creating a sparse model without altering its architecture.\n",
    "\n",
    "* **Structured Pruning**: Removing entire structural units, such as channels in a convolutional layer or neurons in a linear layer, which can lead to direct speedups.\n",
    "\n",
    "* **Global Pruning**: Applying a single pruning criterion across multiple layers at once to achieve a target sparsity for the entire model.\n",
    "\n",
    "Additionally, an optional section will guide you through an end-to-end pipeline of applying global pruning to a pre-trained ResNet18 model. By comparing its performance against an unpruned baseline, you will see firsthand how a significantly sparser network can achieve comparable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Us2kNzlMLk52",
    "outputId": "ae2f7dfd-960c-4cc5-84fe-507c75f7a806"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "To demonstrate various pruning techniques, you'll use a `SimpleModel` for this purpose.\n",
    "\n",
    "* Define `SimpleModel`.\n",
    "    * It contains the two key layers you will target for pruning in the upcoming steps: a `torch.nn.Conv2d` layer (`conv1`) and a `torch.nn.Linear` layer (`fc1`).\n",
    "    * The model's architecture is intentionally simple, consisting of the convolutional layer, a ReLU activation, and the final fully connected layer that produces 10 output features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 10)  # Assuming input is 6x6\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Pruning\n",
    "\n",
    "Unstructured pruning is a technique used to reduce a model's size by removing **individual weights** that have a minimal impact on performance, regardless of their position in the model's architecture. The core idea is that many weights in a trained network are often very close to zero and don't contribute significantly to the output. By removing them, you can create a smaller, more efficient model.\n",
    "\n",
    "### Applying Unstructured Pruning\n",
    "\n",
    "* Initialize the `model` from `SimpleModel` class to perform Unstructured Pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying pruning, you'll first inspect the initial state of your `model` instance. This baseline will help you see exactly what changes after you apply a pruning technique.\n",
    "\n",
    "* **Model Parameters**: Lists of all the learnable parameters in the model. You will see the initial `weight` and `bias` for both the `conv1` and `fc1` layers.\n",
    "* **Model Buffers**: List of the model's **buffers**. Buffers are stateful tensors within the model that are not considered parameters to be trained by the optimizer (for example, the running mean and variance in a batch normalization layer).\n",
    "    * You are checking the buffers because, as you'll see later, the pruning process adds a `weight_mask` as a buffer to track which weights have been zeroed out.\n",
    "    * Initially, this list of buffers will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state before pruning\n",
    "print(\"BEFORE UNSTRUCTURED PRUNING:\\n\")\n",
    "\n",
    "# Model Parameters\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "# Model Buffers\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print out the raw numerical values of a kernel from the `conv1` layer.\n",
    "    * This provides a direct, unambiguous baseline that you can compare against after the pruning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- conv1 Weights Before Pruning ---\\n\")\n",
    "helper_utils.show_weights(model, ['conv1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is where you'll perform the actual pruning operation on your model. \n",
    "\n",
    "* Use the [prune.l1_unstructured](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html#torch-nn-utils-prune-l1-unstructured) function to apply pruning to the `conv1` layer.\n",
    "    * The function identifies and removes **individual weights** based on their **L1 norm** (their absolute value), targeting the least important ones.\n",
    "    * `model.conv1`: This specifies the target module â€” the first convolutional layer in your model.\n",
    "    * `name=\"weight\"`: This tells the function to prune the **weight** tensor within that layer.\n",
    "    * `amount=0.3`: This sets the fraction of weights to remove, meaning `30%` of the connections in the `conv1` layer will be pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unstructured pruning to conv1\n",
    "prune.l1_unstructured(model.conv1, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Unstructured Pruning\n",
    "\n",
    "To verify the results of the pruning, you'll inspect both the model's internal structure and its numerical weights.\n",
    "\n",
    "* Check the Model State:\n",
    "    * **Expected Buffers**: While this list was empty before pruning, you'll now see `conv1.weight_mask` appear in the model's buffers list. This new buffer is the tensor of 0s and 1s that PyTorch uses to track which weights have been removed.\n",
    "    * **Expected Parameters**: You'll notice the original `conv1.weight` is gone from the parameters list and has been replaced by `conv1.weight_orig`. This new parameter holds the underlying, un-masked weight values.\n",
    "* Check the Numerical Weights:\n",
    "    * **Expected Weights**: In the numerical output, you will now clearly see several `0.` values scattered throughout the matrix. These are the individual weights that were zeroed out by the pruning function because they had the lowest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state after pruning\n",
    "print(\"AFTER UNSTRUCTURED PRUNING:\\n\")\n",
    "print(f\"Model parameters:\", [name for name, _ in model.named_parameters()])\n",
    "print(f\"Model buffers:\", [name for name, _ in model.named_buffers()])\n",
    "\n",
    "# --- Show numerical weights after pruning ---\n",
    "print(\"\\n--- conv1 Weights After Pruning ---\\n\")\n",
    "helper_utils.show_weights(model, ['conv1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Check Pruning Statistics and Types**: This final check dives deeper by looking at the exact sparsity achieved and the change in the weight attribute's type. **Sparsity** is simply the percentage of weights that are zero; a higher sparsity means more of the layer has been pruned.\n",
    "    * **Pruning Statistics**: By counting the zeros in the `weight_mask`, the code confirms the Sparsity is `~30%`, matching the amount you requested.\n",
    "    * **Weight Types**: This inspection reveals that `weight_orig` is now the true, learnable `Parameter`, while `.weight` has become a non-learnable `Tensor` that is dynamically computed from `weight_orig` and `weight_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pruning mask exists.\n",
    "if hasattr(model.conv1, 'weight_mask'):\n",
    "    \n",
    "    # Count weights where the mask is 0.\n",
    "    pruned_count = torch.sum(model.conv1.weight_mask == 0).item()\n",
    "    \n",
    "    # Get the total number of weights.\n",
    "    total_count = model.conv1.weight_mask.numel()\n",
    "    \n",
    "    # Print statistics.\n",
    "    print(f\"\\nPruning statistics:\")\n",
    "    print(f\"Total weights in conv1: {total_count}\")\n",
    "    print(f\"Pruned weights (set to zero) in conv1: {pruned_count}\")\n",
    "    print(f\"Sparsity: {pruned_count/total_count:.2%}\")\n",
    "\n",
    "    # Verify the change in weight attribute types.\n",
    "    print(f\"\\nWeight type: {type(model.conv1.weight)}\")\n",
    "    print(f\"Weight_orig type: {type(model.conv1.weight_orig)}\")\n",
    "else:\n",
    "    # Handle pruning failure.\n",
    "    print(\"Pruning did not work as expected! No weight_mask found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Unstructured Pruning Permanent\n",
    "\n",
    "You've successfully applied pruning to the `conv1` layer and verified that roughly `30%` of its weights are now zero. However, the process isn't finished yet. It's important to make this pruning permanent because, right now, your model is in a temporary \"pruned state\" and is carrying extra baggage.\n",
    "\n",
    "As you saw when you inspected the model's parameters and buffers, it's holding onto both the original weights (in the `conv1.weight_orig` parameter) and the `conv1.weight_mask` buffer. This means that during every forward pass, it has to compute the sparse weights on-the-fly by multiplying these two tensors.\n",
    "\n",
    "Calling [prune.remove()](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch-nn-utils-prune-remove) cleans this up. It removes the mask and the original parameter, saving the final, sparse weight tensor back into the standard `.weight` attribute. This removes the computational overhead and simplifies the model back to a standard structure, making it truly efficient and ready for deployment.\n",
    "\n",
    "* Use the [prune.remove()](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.remove.html#torch-nn-utils-prune-remove) utility to finalize the `conv1` layer.\n",
    "    * This function removes the `weight_mask` and `weight_orig` attributes from the module.\n",
    "    * It replaces them with a single, final `weight` parameter that contains the sparse (zeroed-out) weights, making the model's structure clean and efficient again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pruning permanent for conv1\n",
    "prune.remove(model.conv1, 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confirm that the `prune.remove()` function worked correctly and the model has been returned to a standard, clean state.\n",
    "    * **Model Parameters**: You'll see that the `conv1.weight_orig` parameter is gone, and the standard `conv1.weight` has returned to the list of parameters. This shows the model's structure is back to normal.\n",
    "    * **Model Buffers**: The `conv1.weight_mask` has been removed, and the buffers list is now empty again, just as it was before you started pruning. This confirms that all the pruning-related overhead is gone.\n",
    "    * [prune.is_pruned](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.is_pruned.html#torch-nn-utils-prune-is-pruned): This confirms the pruning is permanent. The layer has returned to a standard module that contains sparse (zeroed-out) weights, and the temporary pruning machinery has been removed.\n",
    "        * This would have returned `True` if you had called it before using `prune.remove()`, as the layer was still in its temporary \"pruned state\" at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state to see if pruning is permanent\n",
    "print(\"AFTER MAKING UNSTRUCTURED PRUNING PERMANENT:\\n\")\n",
    "\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")\n",
    "print(f\"\\nIs conv1 still considered pruned? {prune.is_pruned(model.conv1)}\\n\")\n",
    "\n",
    "helper_utils.show_weights(model, ['conv1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Pruning\n",
    "\n",
    "Now, you'll explore **structured pruning**. Unlike unstructured pruning which removes individual weights, this technique removes entire structural units â€” in this case, entire neurons from a linear layer. This approach can lead to more significant improvements in computational efficiency without requiring specialized hardware.\n",
    "\n",
    "You'll apply structured pruning to the `fc1` layer of the `SimpleModel`.\n",
    "\n",
    "### Applying Structured Pruning\n",
    "\n",
    "* First, you'll re-initialize the model to demonstrate structured pruning on a fresh, un-pruned instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, you'll inspect the initial state of the `fc1` layer's weights before applying any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state before pruning\n",
    "print(\"BEFORE STRUCTURED PRUNING:\\n\")\n",
    "# Model State\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")\n",
    "\n",
    "print(\"\\n--- fc1 Weights Before Pruning ---\\n\")\n",
    "helper_utils.show_weights(model, ['fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you'll apply the structured pruning using the [prune.ln_structured](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.ln_structured.html#torch-nn-utils-prune-ln-structured) function.\n",
    "    * This function removes entire rows or columns based on their **Ln-norm**.\n",
    "    * You're targeting the `fc1` layer and removing `50%` (`amount=0.5`) of its neurons.\n",
    "    * `n=2`: specifies using the L2 norm (the vector's magnitude or Euclidean distance) to determine which neurons are least important. \n",
    "    * `dim=0`, is crucial: for a linear layer's weight matrix of shape `[output_features, input_features]`, this prunes along dimension `0`, removing entire output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply structured pruning to fc1\n",
    "prune.ln_structured(model.fc1, name=\"weight\", amount=0.5, n=2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Structured Pruning\n",
    "\n",
    "To verify the results, you'll inspect the model's state and weights again.\n",
    "\n",
    "* As before, you'll see a `weight_mask` and `weight_orig` have been created, but this time for the `fc1` layer.\n",
    "* The key difference to look for in the numerical output is that entire **rows** of the `fc1` weight matrix are now zero, corresponding to the pruned neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check state after pruning\n",
    "print(\"AFTER STRUCTURED PRUNING:\\n\")\n",
    "print(f\"Model parameters:\", [name for name, _ in model.named_parameters()])\n",
    "print(f\"Model buffers:\", [name for name, _ in model.named_buffers()])\n",
    "\n",
    "# --- Show numerical weights after pruning ---\n",
    "print(\"\\n--- fc1 Weights After Pruning ---\\n\")\n",
    "helper_utils.show_weights(model, ['fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This check calculates the pruning statistics.\n",
    "    * Note that instead of counting individual zero-weights, this code now counts how many entire rows are zero to confirm that 50% of the output neurons were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pruning mask exists for fc1.\n",
    "if hasattr(model.fc1, 'weight_mask'):\n",
    "    # For structured pruning, you count entire rows of zeros.\n",
    "    # Determine if each row in the weight tensor is all zeros.\n",
    "    zero_rows = torch.sum(model.fc1.weight == 0, dim=1) == model.fc1.weight.shape[1]\n",
    "    # Count the number of all-zero rows.\n",
    "    pruned_count = torch.sum(zero_rows).item()\n",
    "    # Get the total number of rows (output neurons).\n",
    "    total_count = model.fc1.weight.shape[0]\n",
    "    \n",
    "    # Print the statistics.\n",
    "    print(f\"\\nPruning statistics:\\n\")\n",
    "    print(f\"Total output neurons in fc1: {total_count}\")\n",
    "    print(f\"Pruned output neurons (set to zero) in fc1: {pruned_count}\")\n",
    "    print(f\"Sparsity: {pruned_count/total_count:.2%}\")\n",
    "    \n",
    "    # Verify the change in attribute types.\n",
    "    print(f\"\\nWeight type: {type(model.fc1.weight)}\")\n",
    "    print(f\"Weight_orig type: {type(model.fc1.weight_orig)}\")\n",
    "else:\n",
    "    # Handle pruning failure.\n",
    "    print(\"\\nStructured pruning did not work as expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Structured Pruning Permanent\n",
    "\n",
    "* Finally, you'll make the structured pruning permanent. This process is the same as before: it removes the pruning mask and finalizes the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make structured pruning permanent for fc1\n",
    "prune.remove(model.fc1, 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This final check confirms the model's parameters and buffers have returned to their normal state and that the `fc1` layer is no longer considered \"pruned\" by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if structured pruning is permanent\n",
    "print(\"AFTER MAKING STRUCTURED PRUNING PERMANENT:\\n\")\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")\n",
    "print(f\"\\nIs fc1 still considered pruned? {prune.is_pruned(model.fc1)}\\n\")\n",
    "\n",
    "helper_utils.show_weights(model, ['fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Pruning\n",
    "\n",
    "The final technique you'll explore is global pruning. Unlike the previous methods where you set a pruning amount for each layer individually, global pruning considers all the specified layers together. It removes a percentage of the total number of weights from all layers combined, targeting the weakest connections across the entire model.\n",
    "\n",
    "This often yields better performance because the model can choose to prune more heavily from layers that are less sensitive (like `fc1`) and less from layers that are more sensitive (`conv1`), rather than being forced to prune each by a fixed amount.\n",
    "\n",
    "### Applying Global Pruning\n",
    "\n",
    "* As before, you'll start by re-initializing the model to ensure a clean slate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, you'll inspect the initial state of both the `conv1` and `fc1` layers before applying any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's initial state before pruning.\n",
    "print(\"BEFORE GLOBAL PRUNING:\\n\")\n",
    "\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")\n",
    "\n",
    "# Show the initial weights of both layers.\n",
    "print(\"\\n--- Initial Weights of conv1 and fc1 ---\\n\")\n",
    "helper_utils.show_weights(model, ['conv1', 'fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you'll apply global unstructured pruning across both the `conv1` and `fc1` layers.\n",
    "    * First, you define a tuple of `(module, 'weight')` pairs that specifies all the parameters to be considered for pruning.\n",
    "* The [prune.global_unstructured](https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html#torch-nn-utils-prune-global-unstructured) function will then remove the weakest `20%` (`amount=0.2`) of weights from this entire collection of parameters.\n",
    "    * This utilizes the `prune.L1Unstructured` method as its pruning criterion. By leveraging the L1 unstructured method in the global context, you apply this principle across all specified layers, thereby achieving a uniform level of sparsity throughout the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the collection of parameters to be pruned globally.\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    ")\n",
    "\n",
    "# Apply global unstructured pruning across the specified parameters.\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Global Pruning\n",
    "\n",
    "* To verify the results, you'll inspect the model's state and weights again.\n",
    "    * You should now see `_orig` and `_mask` attributes for both `conv1` and `fc1`.\n",
    "* The numerical outputs will show scattered zeros in both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model's state after pruning.\n",
    "print(\"AFTER GLOBAL PRUNING:\\n\")\n",
    "\n",
    "print(f\"Model parameters:\", [name for name, _ in model.named_parameters()])\n",
    "print(f\"Model buffers:\", [name for name, _ in model.named_buffers()])\n",
    "\n",
    "print(\"\\n--- Weights After Global Pruning ---\\n\")\n",
    "helper_utils.show_weights(model, ['conv1', 'fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This next check calculates the sparsity for each layer individually.\n",
    "    * You will likely see that the sparsity for `conv1` and `fc1` are **not equal** to each other or to 20%. This demonstrates the \"global\" nature of the pruning â€” it removed the weakest weights regardless of which layer they were in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check global pruning results.\n",
    "print(\"--- Sparsity per Layer After Global Pruning ---\\n\")\n",
    "\n",
    "# Verify sparsity in each layer individually.\n",
    "conv1_sparsity = 100. * float(torch.sum(model.conv1.weight == 0)) / float(model.conv1.weight.nelement())\n",
    "fc1_sparsity = 100. * float(torch.sum(model.fc1.weight == 0)) / float(model.fc1.weight.nelement())\n",
    "\n",
    "print(f\"Sparsity in conv1: {conv1_sparsity:.2f}%\")\n",
    "print(f\"Sparsity in fc1: {fc1_sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Global Pruning Permanent\n",
    "\n",
    "* The final step is to make the changes permanent for all layers that were part of the global pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pruning permanent for all pruned layers by iterating through the list.\n",
    "for module, param_name in parameters_to_prune:\n",
    "    prune.remove(module, param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This final check confirms that all pruned layers have been restored to their standard structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAFTER MAKING GLOBAL PRUNING PERMANENT:\\n\")\n",
    "\n",
    "print(f\"Model parameters: {[name for name, _ in model.named_parameters()]}\")\n",
    "print(f\"Model buffers: {[name for name, _ in model.named_buffers()]}\")\n",
    "\n",
    "print(f\"\\nIs conv1 still considered pruned? {prune.is_pruned(model.conv1)}\")\n",
    "print(f\"Is fc1 still considered pruned? {prune.is_pruned(model.fc1)}\\n\")\n",
    "\n",
    "helper_utils.show_weights(model, ['conv1', 'fc1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have explored these pruning techniques in detail, you might be wondering when and in what scenarios each method can be applied effectively in your work. Understanding the strengths and suitable contexts for each technique is crucial to maximizing their benefits in your neural network optimization efforts. Let's delve into some best practices and use cases for unstructured, structured, and global pruning, helping you decide which approach aligns best with your specific needs and goals.\n",
    "\n",
    "1. **Unstructured Pruning**:\n",
    "    * **Best Practices**: Unstructured pruning is often used when you want to maximize sparsity while maintaining flexibility. It is best suited for models where the architecture should remain unchanged, and you only need to prune less important weights.\n",
    "    * **Use Cases**: This technique is useful in scenarios where hardware support for sparsity is present, allowing the sparsely pruned model to be efficiently executed. It's suitable for environments like server-side deployments where computational resources are relatively abundant but memory savings are crucial.\n",
    "2. **Structured Pruning**:\n",
    "    * **Best Practices**: Use structured pruning when you need to improve inference speed and reduce the model's memory footprint effectively. This technique can significantly accelerate the model since entire structures like channels or neurons are removed.\n",
    "    * **Use Cases**: It's ideal for edge devices or mobile applications where computational resources are limited, as structured pruning can help maintain a balance between model size and accuracy while enhancing computational efficiency.\n",
    "3. **Global Pruning**:\n",
    "    * **Best Practices**: Global pruning is advantageous when aiming for a consistent level of sparsity across multiple layers. It minimizes the risk of over-pruning critical parts of the model by evaluating the importance of weights globally rather than locally.\n",
    "    * **Use Cases**: This approach is beneficial in situations where the overall model performance is more critical than the performance of individual layers. It's particularly effective for creating uniformly sparse models that can adapt to various processing environments without heavily compromising accuracy.\n",
    "\n",
    "By understanding these techniques and their implications, you can tailor your pruning strategy to best fit your application's needs, whether it's speeding up inference, reducing memory usage, or balancing efficiency with accuracy.\n",
    "\n",
    "In the optional section below, you'll get a hands-on opportunity to apply one of these techniques in a real-world scenario. Specifically, you will apply global pruning to a pre-trained model and observe the significant before-and-after results, providing a practical demonstration of its impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Practical Application: Global Pruning and Comparative Analysis of a Pre-trained Model\n",
    "\n",
    "This optional section transitions from theoretical understanding to a hands-on demonstration of model pruning in a more realistic setting. Here, you will observe a practical pipeline for applying **global pruning** to a pre-trained **ResNet18** model. You'll see the impact of global pruning by comparing the performance results and model characteristics of a **pruned** ResNet18 against its **unpruned** counterpart, both trained on the same dataset for the same number of epochs. The goal is to provide a concrete example of how pruning can be integrated into a typical deep learning optimization workflow, **demonstrating that comparable performance can be achieved on a pruned model**, thus showcasing its benefits.\n",
    "\n",
    "**You might be wondering: among the various pruning techniques, why opt for global pruning, especially when working with a powerful pre-trained model like ResNet18?**\n",
    "\n",
    "Global pruning is chosen for this practical application with a pre-trained model like ResNet18 for several strategic reasons:\n",
    "\n",
    "* **Consistent Sparsity Across Layers**: Global pruning is advantageous when aiming for a consistent level of sparsity across multiple layers. For a complex, multi-layered pre-trained model, it ensures that pruning is applied holistically, rather than layer by layer in isolation.\n",
    "* **Optimized Resource Allocation**: It minimizes the risk of over-pruning critical parts of the model by evaluating the importance of weights globally rather than locally. This allows the model to intelligently prune more from less sensitive layers and less from more sensitive layers, which is crucial for retaining performance on a pre-trained network.\n",
    "* **Overall Performance Focus**: This approach is beneficial when the overall model performance is more critical than the performance of individual layers. For pre-trained models used in transfer learning, maintaining high overall accuracy after compression is usually paramount.\n",
    "* **Uniformly Sparse Models**: Global pruning is particularly effective for creating uniformly sparse models that can adapt to various processing environments without heavily compromising accuracy. This makes the pruned pre-trained model more versatile for deployment.\n",
    "  \n",
    "While unstructured pruning can maximize sparsity, it relies on hardware support for efficiency, which may not always be available, especially for fine-grained sparsity. Structured pruning, while improving inference speed and reducing memory footprint by removing entire units, might be too aggressive if not carefully managed, potentially leading to a larger accuracy drop on complex pre-trained architectures compared to global pruning's more nuanced approach. Global pruning offers a balanced strategy for optimizing pre-trained models by considering the network's entire weight distribution, making it a highly relevant choice for practical deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Training Configuration\n",
    "\n",
    "* Configure `DEVICE` to use a CUDA-enabled GPU if available, otherwise it defaults to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "\n",
    "For the dataset, revisit the subset of [Fruit and Vegetable Disease (Healthy vs Rotten)](https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten). As a quick refresher on its statistics, this particular subset is structured with 28 classes, each corresponding to a subdirectory name and containing 200 images.\n",
    "\n",
    "* Define the path to the image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./fruit_and_vegetable_subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create data loaders for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = helper_utils.get_dataloaders(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the number of classes as `28`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Training Epochs\n",
    "\n",
    "* Set number of epochs for training.\n",
    "    * Feel free to set a different number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpruned Model Baseline\n",
    "\n",
    "Now that you have set up your dataset and training configurations, which will be **identical** for both the unpruned and pruned models, it's time to proceed with model training. \n",
    "\n",
    "First, establish the unpruned model baseline, which will serve as the critical reference point for your comparative analysis.\n",
    "\n",
    "### Loading and Training the Unpruned Model\n",
    "\n",
    "* Initialize the ResNet18 model.\n",
    "    * `helper_utils.load_resnet18()` loads the ResNet18 architecture with weights pre-trained on ImageNet from a local file.\n",
    "* `helper_utils.replace_final_layer()` adapts the model for transfer learning by replacing only the final fully connected layer to match the number of classes.\n",
    "    * This function **does not freeze any middle layers**, ensuring all layers remain trainable. This is vital for conducting a fair comparison with the pruned model later, as frozen layers would not be affected by weight removal or subsequent fine-tuning during the pruning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Resnet18 model with ImageNet weights\n",
    "resnet18_model_unpruned = helper_utils.load_resnet18()\n",
    "\n",
    "# Adapt the model for transfer learning with the new number of classes\n",
    "model_unpruned = helper_utils.replace_final_layer(resnet18_model_unpruned, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This cell executes a standard training loop to fine-tune the unpruned model on your dataset.\n",
    "* The loop will return the trained model and its performance metrics.\n",
    "    * These metrics include final Accuracy, Precision, Recall, and F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the unpruned model\n",
    "trained_unpruned_model, unpruned_metrics = helper_utils.training_loop(model_unpruned,\n",
    "                                                                      train_loader,\n",
    "                                                                      val_loader,\n",
    "                                                                      num_epochs,\n",
    "                                                                      DEVICE,\n",
    "                                                                      num_classes\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the state dictionary and metrics of the trained unpruned model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.save_unpruned_model_and_metrics(trained_unpruned_model, unpruned_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the key performance metrics of the unpruned model after training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "-- After Training for {num_epochs} Epoch(s) --\n",
    "Final Accuracy:           {unpruned_metrics['accuracy']:.2f}%\n",
    "Final Precision (Macro):  {unpruned_metrics['precision']:.4f}\n",
    "Final Recall (Macro):     {unpruned_metrics['recall']:.4f}\n",
    "Final F1-Score (Macro):   {unpruned_metrics['f1_score']:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Training the Pruned Model\n",
    "\n",
    "Now that you have successfully established the unpruned model baseline, it's time to explore the effects of pruning. Now, you will train a pruned version of the ResNet18 model, ensuring that all dataset and training configurations remain identical to those used for the unpruned baseline. This approach guarantees a fair and direct comparison of performance and model characteristics, allowing you to accurately assess the benefits of pruning.\n",
    "\n",
    "* You'll start by re-initializing the Resnet18 model to ensure a clean slate.\n",
    "* Similar to the unpruned baseline model, replace the final classifier layer for transfer learning.\n",
    "    * All of the middle layers are **not frozen**, allowing their weights to be modified by pruning and subsequent fine-tuning. This is essential for demonstrating the impact of pruning on a pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Resnet18 model with ImageNet weights\n",
    "resnet18_model_pruned = helper_utils.load_resnet18()\n",
    "\n",
    "# Adapt the model for transfer learning with the new number of classes\n",
    "model_pruned = helper_utils.replace_final_layer(resnet18_model_pruned, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier sections with the `SimpleModel`, you manually checked things like `model.named_parameters()` and `model.named_buffers()` to see the `_orig` and `_mask` attributes. You also calculated sparsity for individual layers to verify the pruning.\n",
    "\n",
    "But now, with a much larger model like ResNet18 and the intention to apply global pruning across many layers, checking each layer individually or manually calculating sparsity for every part will be quite cumbersome.\n",
    "\n",
    "Instead, you'll use the `analyze_model_sparsity` function, which will provide a consolidated, overall view of the model's sparsity across all its weighted layers. Instead of going layer-by-layer, it will efficiently give you a single percentage that reflects how much of the entire network has been pruned. This is particularly relevant for global pruning, where the zeros are distributed across layers based on a global criterion, rather than a fixed amount per layer. It will provide a quick and quantitative assessment of the overall impact of pruning efforts on this larger, more complex architecture.\n",
    "\n",
    "* Initialize counters for total and zero parameters.\n",
    "* Iterate through all named modules (layers) in the model.\n",
    "* For modules with a `weight` tensor, sums their elements to `total_params` and counts zero elements to `total_zero_params`.\n",
    "* Calculates overall `sparsity` as the percentage of zero parameters out of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculates and prints the sparsity of a PyTorch model\n",
    "    by inspecting the computed 'weight' attribute of each module.\n",
    "    \"\"\"\n",
    "    # Initialize total parameter count for weighted layers.\n",
    "    total_params = 0\n",
    "    # Initialize total count of zero parameters.\n",
    "    total_zero_params = 0\n",
    "\n",
    "    # Loop through all named modules (layers) in the model.\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module has a 'weight' attribute and if it's a PyTorch tensor.\n",
    "        if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor):\n",
    "            # Add the number of elements in the current module's weight tensor to total_params.\n",
    "            total_params += module.weight.nelement()\n",
    "            # Count and add the number of zero elements in the current weight tensor to total_zero_params.\n",
    "            # Note: module.weight reflects the actual (masked) weights after pruning.\n",
    "            total_zero_params += torch.sum(module.weight == 0).item()\n",
    "\n",
    "    # Check if any weighted layers were found to avoid division by zero.\n",
    "    if total_params > 0:\n",
    "        # Calculate sparsity as a percentage.\n",
    "        sparsity = 100. * float(total_zero_params) / float(total_params)\n",
    "        # Print the calculated model sparsity.\n",
    "        print(f\"Model Sparsity: {sparsity:.2f}%\")\n",
    "        # Print the total number of parameters in weighted layers.\n",
    "        print(f\"Total parameters (in weighted layers): {total_params}\")\n",
    "        # Print the total count of zero parameters.\n",
    "        print(f\"Total zero parameters (in weighted layers): {total_zero_params}\")\n",
    "    else:\n",
    "        # Inform if no weighted layers were found.\n",
    "        print(\"No weighted layers found to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performs an initial analysis of the `model_pruned ` to establish its baseline state before any pruning is applied.\n",
    "* **Sparsity Analysis Output**:\n",
    "    * You will observe `Model Sparsity: 0.00%` and `Total zero parameters: 0`. This is expected, as no pruning has been performed yet, meaning all trainable weights in the model are non-zero.\n",
    "    * `Total parameters (in weighted layers)` will show the full count of parameters in the model's convolutional and linear layers (e.g., 11,186,048 for ResNet18), reflecting the complete, unpruned network.\n",
    "* **Selected Layer Weights Output**:\n",
    "    * The output from `helper_utils.show_weights` for the `conv1` layer will display its raw numerical values. Crucially, you should not see any zeros scattered within this matrix, confirming the unpruned state of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis Before Pruning ---\n",
    "print(\"--- Analysis Before Pruning ---\")\n",
    "analyze_model_sparsity(model_pruned)\n",
    "\n",
    "# # --- Show initial weights of selected layers ---\n",
    "# print(\"\\n--- Selected conv1 Layer Weights Before Pruning ---\\n\")\n",
    "# helper_utils.show_weights(model_pruned, ['conv1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell identifies the target layers for global pruning and then applies the pruning operation.\n",
    "\n",
    "* **Identifying Layers**: You first compile a list (`parameters_to_prune`) of all convolutional (`torch.nn.Conv2d`) and linear (`torch.nn.Linear`) layers within your `model_pruned`.\n",
    "* **Excluding the Final Classifier**: Crucially, the final classifier layer (`fc`) is explicitly excluded from this list. This is a common practice when pruning pre-trained models, as this layer was recently replaced for your dataset's classes. Aggressively pruning it might disproportionately impact accuracy, as its weights are often less redundant compared to the extensively pre-trained feature extraction layers.\n",
    "* **Applying Global Pruning**: The `prune.global_unstructured` function is then used to remove weights across all identified layers. You are applying a significant `amount=0.5` (50%) of global unstructured pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all convolutional and linear layers to be pruned, excluding the final classifier\n",
    "parameters_to_prune = []\n",
    "for module_name, module in model_pruned.named_modules():\n",
    "    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)) and 'fc' not in module_name:\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "# Apply global unstructured pruning\n",
    "print(\"\\n--- Applying 50% Global Unstructured Pruning ---\\n\")\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.5,\n",
    ")\n",
    "print(\"Global pruning applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verify the immediate effects of applying global unstructured pruning to your model.\n",
    "* **Sparsity Analysis Output**:\n",
    "    * You will now observe `Model Sparsity` to be approximately close to `50%`, with a corresponding increase in `Total zero parameters`. This confirms that a significant portion of the model's weights have been successfully zeroed out.\n",
    "    * The `Total parameters (in weighted layers)` remains unchanged, indicating that weights were zeroed, not entirely removed from the model's structure at this stage.\n",
    "    * **Note:** This specific sparsity percentage is observed when `amount=0.5` (50%) is used during the global unstructured pruning step. The exact value may vary slightly due to floating-point precision.\n",
    "* **Selected Layer Weights Output**:\n",
    "    * The output from `helper_utils.show_weights` for the `conv1` layer will now clearly display several `0.` values scattered throughout the matrix. These zeros represent the individual weights that were removed by the global pruning operation. This is the temporary state of the weights before pruning is made permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis After Pruning (Before Making it Permanent) ---\n",
    "print(\"--- Analysis After Pruning ---\")\n",
    "analyze_model_sparsity(model_pruned)\n",
    "\n",
    "# # --- Show weights of selected layers after temporary pruning ---\n",
    "# print(\"\\n--- Selected Layer Weights After Temporary Pruning ---\\n\")\n",
    "# helper_utils.show_weights(model_pruned, ['conv1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Execute a standard training loop to fine-tune the `model_pruned` on your dataset, similar to how the unpruned model was trained.\n",
    "* **This training step occurs before making the pruning permanent**.\n",
    "    * When pruning is first applied, PyTorch introduces a `weight_mask` and retains the original weights (`weight_orig`).\n",
    "    * During this fine-tuning process, the optimizer updates `weight_orig`, while the `weight_mask` ensures that the pruned (zeroed-out) connections remain zero. This allows the model to recover performance by adjusting the remaining non-zero weights.\n",
    "    * If pruning were made permanent *before* training, the `weight_orig` and `weight_mask` would be removed, and subsequent training could inadvertently update the hard zeros back to non-zero values, effectively undoing the pruning and diminishing its benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pruned model\n",
    "trained_pruned_model, pruned_metrics = helper_utils.training_loop(model_pruned,\n",
    "                                                                  train_loader,\n",
    "                                                                  val_loader,\n",
    "                                                                  num_epochs,\n",
    "                                                                  DEVICE,\n",
    "                                                                  num_classes\n",
    "                                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finalize the pruning by calling `prune.remove()` on each pruned layer, baking the zeros permanently into the weights.\n",
    "* A final sparsity analysis confirms that the zeroed weights are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Making Pruning Permanent ---\n",
    "print(\"--- Making Pruning Permanent ---\")\n",
    "for module, param_name in parameters_to_prune:\n",
    "    prune.remove(module, param_name)\n",
    "print(\"Pruning has been made permanent.\")\n",
    "\n",
    "# --- Final Analysis ---\n",
    "print(\"\\n--- Final Analysis of Trained Model (Post-Permanent Pruning) ---\")\n",
    "analyze_model_sparsity(trained_pruned_model)\n",
    "\n",
    "# # --- Show weights of selected layers after permanent pruning ---\n",
    "# print(\"\\n--- Selected Layer Weights After Permanent Pruning ---\\n\")\n",
    "# helper_utils.show_weights(trained_pruned_model, ['conv1']) # Confirming persistence of zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the state dictionary and metrics of the trained pruned model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.save_pruned_model_and_metrics(trained_pruned_model, pruned_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! The moment of truth has arrived. Run the next cell to finally unveil the actual effects and results of your pruning efforts, providing a comprehensive comparison of your unpruned and pruned models.\n",
    "\n",
    "* The report will display key metrics for both models, allowing you to directly assess the impact of pruning.\n",
    "* For each model, you will see:\n",
    "    * **Total Parameters**: The overall count of parameters in the model.\n",
    "    * **Non-Zero Parameters**: Specifically for the pruned model, this will highlight the \"Effective Parameters: weights retained for computation\" after pruning.\n",
    "    * **Saved model size**: The disk size of the saved model files.\n",
    "    * **Final Accuracy**: The validation accuracy achieved after training.\n",
    "    * **Final Precision (Macro)**: The macro-averaged precision.\n",
    "    * **Final Recall (Macro)**: The macro-averaged recall.\n",
    "    * **Final F1-Score (Macro)**: The macro-averaged F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.comparison_report(\n",
    "    unpruned_state_dict_path=\"unpruned_model_state_dict.pth\",\n",
    "    unpruned_metrics_path=\"unpruned_metrics.pkl\",\n",
    "    pruned_state_dict_path=\"pruned_model_permanent_state_dict.pth\",\n",
    "    pruned_metrics_path=\"pruned_metrics.pkl\",\n",
    "    num_epochs=num_epochs,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in! As you've now seen from the comparison report, the tangible effects of pruning on your model are truly impressive!\n",
    "\n",
    "* **Parameters**: You've seen that while the `Total Parameters` for both the unpruned and pruned models appear similar, the `Non-Zero Parameters` for your pruned model show a groundbreaking reduction â€“ effectively **halving** the network's active weights! This highlights the substantial sparsity you've achieved.\n",
    "* **Performance Metrics**: And here's the truly exciting part: despite pruning a remarkable `50% of the network`, your `Final Accuracy`, `Final Precision (Macro)`, `Final Recall (Macro)`, and `Final F1-Score (Macro)` for the pruned model remained largely comparable to the unpruned baseline! This isn't just a minor win; it's a powerful demonstration that you can achieve massive model compression without sacrificing predictive power.\n",
    "\n",
    "However, another observation you might have made is that despite pruning a significant 50% of the network, the `Saved model size` for both the unpruned and pruned models appears to be the same. You might be wondering: what exactly was the purpose of pruning then, if the model file size doesn't change?\n",
    "\n",
    "The short answer is that standard PyTorch saving (`torch.save`) stores all parameters, including those that have been zeroed out by pruning, in a dense format. Pruning, as demonstrated here, primarily reduces the **effective** number of parameters used for computation, by setting unimportant weights to zero. While this can lead to faster inference on hardware that supports sparse operations, it doesn't inherently reduce the model's disk footprint or dense memory usage without further steps.\n",
    "\n",
    "So, how can you truly reduce the model's file size and memory footprint for deployment in resource-constrained environments? This is where techniques like **quantization** come into play, reducing the precision of the model's weights and activations. You'll learn more about quantization next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a practical guide to model pruning with PyTorch. You have seen how to apply different pruning strategies, from removing individual weights with **unstructured pruning** to entire neurons with **structured pruning**. You also explored how to apply these techniques across multiple layers simultaneously using **global pruning**.\n",
    "\n",
    "In the optional section, you put these concepts into practice by applying global pruning to a pre-trained ResNet18 model. The results demonstrated that you can achieve a massive reduction in a model's effective parameters while maintaining performance comparable to an unpruned baseline. This outcome highlights a vital aspect of model optimization: a model's performance does not always depend on the sheer number of its parameters.\n",
    "\n",
    "You may have noticed that even after zeroing out half the network, the model's file size did not decrease. This is because standard PyTorch methods save the pruned weights in a dense format, masks and all. While this approach is perfect for understanding pruning's impact, achieving actual size reduction requires additional steps or other compression techniques.\n",
    "\n",
    "\n",
    "The skills learned here are a foundational step in optimizing deep learning models for deployment. By reducing a model's complexity, you make it more suitable for resource-constrained environments, paving the way for the next step in model compression: **quantization**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
