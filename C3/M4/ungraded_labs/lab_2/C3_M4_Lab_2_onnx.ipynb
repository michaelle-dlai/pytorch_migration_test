{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d47d0d2-c595-4de9-8745-6c68c8f54bbe",
   "metadata": {},
   "source": [
    "# From PyTorch to ONNX\n",
    "\n",
    "After investing significant effort to train a high-performing model, the next pivotal step is preparing it for deployment. In a real-world setting, your model needs to be efficient and capable of running on various platforms. However, a model is often a set of Python objects that only its native framework, like PyTorch, knows how to execute. This can make it incompatible with environments that cannot run Python or PyTorch directly, such as mobile apps or embedded devices.\n",
    "\n",
    "This is where  **[ONNX (Open Neural Network Exchange)](https://onnx.ai)** becomes essential. ONNX is an open standard designed to represent machine learning models, enabling them to be used across different frameworks and runtimes. By converting your model to the ONNX format, you make it **portable** and unlock a wide range of deployment possibilities.\n",
    "\n",
    "In this notebook, you will walk through the practical steps of this process. You will:\n",
    "* Take a fully trained PyTorch model and export it to the ONNX format.\n",
    "\n",
    "* Use the **ONNX Runtime** to perform inference with the newly converted `.onnx` file.\n",
    "\n",
    "* As a further demonstration of ONNX's flexibility, an optional section will also guide you through converting the ONNX model to a TensorFlow representation and running inference with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca7ff3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170778f-d397-4806-b258-8791e08c13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision.models as tv_models\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c85bd3-e1e0-4889-ab75-2c4a5de045ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73504f10",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "To demonstrate a practical, real-world, end-to-end pipeline, you'll use a subset of the [Fruit and Vegetable Disease (Healthy vs Rotten)](https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten) instead of a standard academic dataset. This dataset is organized into 28 subdirectories, where each directory name serves as a class label. For this particular subset, each of the 28 classes contains 200 images.\n",
    "\n",
    "* Define the path to the image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6686b-5268-45bf-a2c7-84c00224e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./fruit_and_vegetable_subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac6a56-71a5-4dcf-8382-6553345a8ddf",
   "metadata": {},
   "source": [
    "* Use the helper function (`dataset_images_per_class`) to verify the dataset's structure.\n",
    "    * This function iterates through each class subdirectory, counts the number of valid image files, and prints a summary of the image count for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e045acd-1550-4a36-b133-1928cbe73859",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.dataset_images_per_class(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06c9fd-6351-4e88-8a1f-9e278d7220e7",
   "metadata": {},
   "source": [
    "* Define the list of class names in a Python list.\n",
    "    * For better readability during visualization, these names have been edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f473ce4-d5f3-4893-922c-3f5cecc4a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'Apple (Healthy)', 'Apple (Rotten)', 'Banana (Healthy)', 'Banana (Rotten)',\n",
    "    'Bellpepper (Healthy)', 'Bellpepper (Rotten)', 'Carrot (Healthy)', 'Carrot (Rotten)',\n",
    "    'Cucumber (Healthy)', 'Cucumber (Rotten)', 'Grape (Healthy)', 'Grape (Rotten)',\n",
    "    'Guava (Healthy)', 'Guava (Rotten)', 'Jujube (Healthy)', 'Jujube (Rotten)',\n",
    "    'Mango (Healthy)', 'Mango (Rotten)', 'Orange (Healthy)', 'Orange (Rotten)',\n",
    "    'Pomegranate (Healthy)', 'Pomegranate (Rotten)', 'Potato (Healthy)', 'Potato (Rotten)',\n",
    "    'Strawberry (Healthy)', 'Strawberry (Rotten)', 'Tomato (Healthy)', 'Tomato (Rotten)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387cbb8-ab99-42fe-9c19-435e53299a97",
   "metadata": {},
   "source": [
    "* Use the helper function (`get_dataloaders`) to create data loaders to efficiently load the training and validation images in batches.\n",
    "    * Splits the full dataset into separate training (80%) and validation (20%) sets.\n",
    "    * Applies a distinct set of transformations to each set. \n",
    "    * Wraps both datasets in PyTorch DataLoaders, which efficiently loads the data in shuffled batches for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9a180-1336-4210-bdac-ac061e87e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = helper_utils.get_dataloaders(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc54ac7-5671-4bb4-9131-8778a3929d5b",
   "metadata": {},
   "source": [
    "* Run the cell below to visualise some images from the train loader.\n",
    "    * Each execution will shuffle images to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be998ad7-6bd1-48c5-ad5e-eee154e14684",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.show_image_grid(train_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c12e3",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Training a deep network from scratch on a small dataset like this (~4,500 images) risks poor performance. A more practical approach for an end-to-end pipeline is to use a pre-trained model.\n",
    "\n",
    "**ResNet18** provides a significant head start by leveraging powerful visual features already learned from millions of images, which saves time and improves results. ResNet18 is an ideal choice because it balances strong performance with a small, efficient architecture, making it a reliable and practical option for deployment.\n",
    "\n",
    "* Initialize a [ResNet18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model instance without any pre-trained weights (`weights=None`).\n",
    "* Load the `state_dict` from a local `./pretrained_resnet18_weights/resnet18-f37072fd.pth` file containing the weights from a model pre-trained on ImageNet.\n",
    "* Apply these pre-trained weights to the ResNet18 model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1feee58-8f5a-4538-a5d7-e8737c1b4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet18 model architecture\n",
    "resnet18_model = tv_models.resnet18(weights=None)\n",
    "\n",
    "# Path to the local pre-trained weights file\n",
    "weights_path = './pretrained_resnet18_weights/resnet18-f37072fd.pth'\n",
    "# Load the weights from the file\n",
    "state_dict = torch.load(weights_path)\n",
    "\n",
    "# Apply the loaded weights to the model instance\n",
    "resnet18_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39774d3-e0e4-46a4-b961-587be5c1637a",
   "metadata": {},
   "source": [
    "* Adapt the final layer of the model to match the number of classes in your specific dataset.\n",
    "    * The `adapt_model_for_transfer_learning` function is:\n",
    "        * Freezing the pre-trained layers.\n",
    "        * Replacing the classifier head with a new one that is customized to output predictions for the specific number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3acc6-34ae-4d32-8c94-e145033a88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of classes of the dataset\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Adapt the model for transfer learning with the new number of classes\n",
    "model = helper_utils.adapt_model_for_transfer_learning(resnet18_model, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf59530",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "* Set the number of training epochs.\n",
    "    * Running for 1 epoch should get you a validation accuracy of `>75%`\n",
    "    * Feel free to set a different number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535c7c5-c384-4113-8426-11857b5e35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b4ba1-f703-4396-9ee7-76bc6987efa4",
   "metadata": {},
   "source": [
    "* Run the training loop to fine-tune the model on the dataset.\n",
    "* The output is a fully trained model ready for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cc808-7432-49e5-8303-c82c41a5fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = helper_utils.training_loop(model, train_loader, val_loader, num_epochs, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e0fb7",
   "metadata": {},
   "source": [
    "## Model Conversion: PyTorch to ONNX\n",
    "\n",
    "Now that you have a trained PyTorch model, the next practical step is to prepare it for deployment. To solve the challenge of cross-platform compatibility, you will convert your model to the ONNX format. You will achieve this conversion using the [torch.onnx](https://docs.pytorch.org/docs/stable/onnx.html) module, specifically by calling its `export()` function to capture the model's design and weights into a portable `.onnx` file.\n",
    "\n",
    "* Set the model to evaluation mode (`model.eval()`) before export.\n",
    "* Create a dummy input tensor with the shape the model expects (`1, 3, 224, 224` in this case), which is necessary for the exporter to trace the model's architecture.\n",
    "* Call the [torch.onnx.export](https://docs.pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export) function with several key parameters:\n",
    "    * The trained PyTorch model is the first argument.\n",
    "    * The `export_params=True` argument ensures that the model's trained weights are stored in the ONNX file.\n",
    "    * `opset_version` is set to specify the ONNX version to use.\n",
    "    * `do_constant_folding=True` is an optimization that pre-computes constant values in the graph.\n",
    "    * `input_names` and `output_names` assign explicit names to the model's input and output tensors, which makes them easier to identify and use later.\n",
    "    * `dynamic_axes` specifies that the first dimension (the batch size) can vary in size during inference, allowing for flexible batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89d07e-5fad-4993-9848-38f8d13504ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode (important first step)\n",
    "model.eval()\n",
    "\n",
    "print(\"Exporting model to ONNX...\")\n",
    "\n",
    "# Create a dummy input tensor with the expected shape\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=DEVICE)\n",
    "\n",
    "# Export the model to the ONNX format\n",
    "torch.onnx.export(\n",
    "    trained_model,                # The trained PyTorch model to be exported. \n",
    "    dummy_input,                  # A dummy input tensor with the expected shape for tracing. \n",
    "    \"fruit_veg_model.onnx\",       # The name of the output .onnx file. \n",
    "    export_params=True,           # Exports the model's trained weights (parameters). \n",
    "    opset_version=11,             # The ONNX version to use. Using 11 to be compatible with TF in the Optional section\n",
    "    do_constant_folding=True,     # An optimization that pre-computes constant expressions in the model. \n",
    "    input_names=['input'],        # Assigns a name to the model's input tensor for easier identification. \n",
    "    output_names=['output'],      # Assigns a name to the model's output tensor. \n",
    "    # Specifies which dimensions can vary in size, like the batch size. \n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}       \n",
    ")\n",
    "\n",
    "print(\"\\nModel successfully exported as fruit_veg_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b294663-c0a1-418b-b9c4-f926cdd43191",
   "metadata": {},
   "source": [
    "## Running Inference with ONNX\n",
    "\n",
    "* Load the exported `fruit_veg_model.onnx` model into an ONNX Runtime [InferenceSession](https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.InferenceSession).\n",
    "* Retrieve the model's input name, which was assigned during the export step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f58606-594d-431f-adf5-39d0943b37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model into an ONNX Runtime inference session\n",
    "ort_session = ort.InferenceSession(\"fruit_veg_model.onnx\")\n",
    "\n",
    "# Get the name of the model's input layer\n",
    "input_name = ort_session.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47549e49-674e-461c-95d9-8e275e86cb8d",
   "metadata": {},
   "source": [
    "* Prepare a batch of validation images, ensuring the data is in the tensor shape the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d456620-15e4-4fcd-8f82-8301ce7d55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of validation data to run predictions on\n",
    "val_iter = iter(val_loader)\n",
    "images, labels = next(val_iter)\n",
    "\n",
    "# Use the first 9 images of the batch for prediction\n",
    "input_data = images[:9].cpu().numpy()\n",
    "# Get the corresponding true labels\n",
    "true_labels = labels[:9].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94660675-ee21-4fb5-898a-77135583f331",
   "metadata": {},
   "source": [
    "* Call [session.run()](https://onnxruntime.ai/docs/api/python/api_summary.html#onnxruntime.InferenceSession.run) to perform inference.\n",
    "    * Passing `None` for the output names tells the runtime to return all model outputs.\n",
    "    * The input data is passed as a dictionary where the key is the input name and the value is the image tensor.\n",
    "    * For a model with a single output, the predictions are contained in the first element of the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385bc29-4838-481e-ae14-47f78e8f3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference using the ONNX Runtime session\n",
    "ort_outputs = ort_session.run(None, {input_name: input_data})\n",
    "# Extract the predictions from the output\n",
    "predictions = ort_outputs[0]\n",
    "\n",
    "print(\"Displaying predictions from the ONNX model...\\n\")\n",
    "# Visualize the predictions against the true labels\n",
    "helper_utils.show_prediction_grid(input_data, true_labels, predictions, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a16022-c1b7-448e-9510-37b6998a891e",
   "metadata": {},
   "source": [
    "## (Optional) Convert ONNX Model to TensorFlow\n",
    "\n",
    "This optional section demonstrates one of the biggest advantages of using the ONNX format: its portability. Once your model is in the ONNX format, you are no longer locked into the PyTorch ecosystem and can use it in other frameworks. You might do this to integrate your model into an existing TensorFlow-based application or deployment pipeline. This is achieved by loading the `.onnx` file and using the [onnx-tf](https://github.com/onnx/onnx-tensorflow) backend converter library to prepare a TensorFlow-compatible version of the model.\n",
    "\n",
    "* First, load the `.onnx` model and prepare it by using the `onnx-tf` [backend](https://github.com/onnx/onnx-tensorflow/blob/main/onnx_tf/backend.py).\n",
    "    * This creates an in-memory TensorFlow representation (`tf_rep`), which acts as a bridge for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6544f-d94c-4735-8ff9-d9439d8d3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model from the file\n",
    "print(\"Loading ONNX model and creating TF representation...\")\n",
    "\n",
    "onnx_model = onnx.load(\"fruit_veg_model.onnx\")\n",
    "# Prepare the TensorFlow representation of the ONNX model\n",
    "tf_rep = prepare(onnx_model)\n",
    "\n",
    "# Print a success message\n",
    "print(\"In-memory TensorFlow representation created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb99f95-9e0e-4fe2-85fb-ebb0d2f6bf3c",
   "metadata": {},
   "source": [
    "* Now, export this in-memory representation to TensorFlow's standard `SavedModel` format.\n",
    "    * This saves the model as a standalone, native TensorFlow artifact.\n",
    " \n",
    "**Note**: The following execution may generate some warnings, which can be safely **ignored**. A successful export is indicated by the `Export complete.` message at the end of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ace2b-95d4-4d63-973b-d62bfea17744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path and export the graph\n",
    "tf_model_path = \"./fruit_veg_tf_savedmodel\"\n",
    "\n",
    "print(f\"Exporting native TensorFlow model to: {tf_model_path}\")\n",
    "\n",
    "try:\n",
    "    # Attempt to export the graph\n",
    "    tf_rep.export_graph(tf_model_path)\n",
    "    \n",
    "    # This success message will only be printed if the line above runs without errors\n",
    "    print(\"\\nExport complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # This message will be printed if any error occurs during the export process\n",
    "    print(f\"\\nAn error occurred during export: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81585ae6-c3a0-4e0f-8de9-fc65e34dc7c8",
   "metadata": {},
   "source": [
    "* Prepare a batch of validation data for prediction and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da071c9b-bae0-4344-bd88-495d86155a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh batch of validation images and labels\n",
    "val_iter = iter(val_loader)\n",
    "images, labels = next(val_iter)\n",
    "\n",
    "# Prepare the first 9 images and their labels for the prediction grid\n",
    "input_data = images[:9].cpu().numpy()\n",
    "true_labels = labels[:9].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a161c-3216-4fd7-a09f-b0653d8fd56b",
   "metadata": {},
   "source": [
    "* Load the native TensorFlow model from the path where you previously saved it.\n",
    "    * This uses the standard `tf.saved_model.load()` function and retrieves the specific inference function from the model's signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1719164-4f81-450a-98bb-ae10faff0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NATIVE TENSORFLOW SYNTAX: Load the model from the SavedModel format\n",
    "print(f\"Loading native TensorFlow model from: {tf_model_path}\")\n",
    "loaded_tf_model = tf.saved_model.load(tf_model_path)\n",
    "\n",
    "# NATIVE TENSORFLOW SYNTAX: Get the specific function for serving/inference\n",
    "inference_func = loaded_tf_model.signatures[\"serving_default\"]\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd713d6-2d14-47c4-a208-3c5fd89533df",
   "metadata": {},
   "source": [
    "* Finally, run inference using the loaded TensorFlow model.\n",
    "    * This involves converting your NumPy image data into a TensorFlow tensor, calling the model's inference function, and converting the resulting predictions back to a NumPy array for easy use.\n",
    "* Visualize the model's predictions against the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c234f-155f-47c6-8752-f0b00c87028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NATIVE TENSORFLOW SYNTAX: Convert the input data to a TensorFlow tensor\n",
    "input_tensor = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "\n",
    "# NATIVE TENSORFLOW SYNTAX: Run prediction using the loaded function\n",
    "predictions_dict = inference_func(input=input_tensor)\n",
    "\n",
    "# NATIVE TENSORFLOW SYNTAX: Extract the output tensor and convert to a NumPy array\n",
    "tf_predictions_np = predictions_dict['output'].numpy()\n",
    "\n",
    "# Visualize the results\n",
    "print(\"Displaying predictions from the NATIVE TensorFlow model...\\n\")\n",
    "helper_utils.show_prediction_grid(input_data, true_labels, tf_predictions_np, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deeb7b-f8b4-4530-8851-76962449a698",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on successfully converting a PyTorch model to the ONNX format and running inference with it!\n",
    "\n",
    "By completing this process, you have seen firsthand how to decouple your model from its original training framework. Your model is no longer just a PyTorch artifact; it is now in a standardized format that can be deployed across a multitude of environments that support the ONNX standard. As another practical example, the optional section of this notebook showed you how to take this portability a step further by converting the ONNX model into a TensorFlow representation and running inference with it.\n",
    "\n",
    "\n",
    "This is a key step in preparing your models, making them more versatile and efficient for production environments. You are now equipped with a vital skill for bridging the gap between model development and real-world deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
