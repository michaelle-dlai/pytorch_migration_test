{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanStreetNet — Edge Vision for Cleaner Cities\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You have already trained many models; now it is time to shape one for life outside a notebook. This lab guides you through taking a pre-trained VGG16 based StreetClassifier and turning it into a version that is easier to store, faster on CPU, and ready for deployment on edge targets. Along the way you will practice saving and reloading model state, pruning parameters, applying quantization, and evaluating the trade offs among accuracy, latency, and size.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- Load the CleanStreetDataset, initialize a pre-trained StreetClassifier, and establish a baseline evaluation.\n",
    "\n",
    "- Work with checkpoints by saving and restoring state_dict objects for both training and inference.\n",
    "\n",
    "- Implement magnitude based pruning across convolutional and linear layers, with options for unstructured and structured strategies, and verify sparsity and accuracy.\n",
    "\n",
    "- Apply dynamic quantization to linear layers for a fast CPU speedup and benchmark the effect.\n",
    "\n",
    "- Fuse common layer patterns and prepare a quantization aware variant, fine tune briefly, then convert to an int8 model.\n",
    "\n",
    "- Compare accuracy, inference time, and file size before and after compression to understand the impact of each step.\n",
    "\n",
    "By the end, you will have a compact classifier that keeps performance close to the original while being far more efficient to run and ship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- - To submit your notebook for grading, first save it by clicking the 💾 icon on the top left of the page and then click on the <span style=\"background-color: blue; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as aoq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clean StreetClassifier data and model\n",
    "\n",
    "**StreetClassifier** is a deep learning model designed to classify urban scene images into three categories:  \n",
    "- **clean**  \n",
    "- **litter**  \n",
    "- **recycle** \n",
    "\n",
    "### Dataset\n",
    "\n",
    "For this task, we will use the **CleanStreetDataset**, which is already divided into **training**, **development**, and **test** splits.  \n",
    "In the code below, you’ll see how the datasets are loaded and how data preprocessing and augmentation transforms are applied to prepare the data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/workspace/code/pytorch_mixed/c4Assignment/data/CleanStreetDataset\"\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasetsfrom torch.nn.utils import prune\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=train_transform)\n",
    "dev_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'dev'), transform=eval_transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'test'), transform=eval_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(dev_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))\n",
    "print(\"\\nClass mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize some examples with he following helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.display_some_images(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "The **StreetClassifier** model builds on **VGG16**, a well-known convolutional neural network pretrained on ImageNet.  \n",
    "In this implementation the final classifier layer was replaces so the model can predict the three target classes of our dataset (*clean*, *litter*, and *recycle*).  \n",
    "\n",
    "The class definition is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class StreetClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Image classifier built on a pretrained **VGG16** backbone for three urban-scene classes\n",
    "    (default: *clean*, *litter*, *recycle*).\n",
    "\n",
    "    The VGG16 backbone (pretrained on ImageNet) is loaded and **frozen** so only the final\n",
    "    classifier layer is trainable. The last `nn.Linear` layer is replaced to match the\n",
    "    requested number of output classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int, optional (default=3)\n",
    "        Number of target classes. Used to size the final fully connected layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    backbone : nn.Module\n",
    "        The wrapped VGG16 network with all original parameters frozen except the\n",
    "        replaced final `nn.Linear` classifier layer.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Expected input: a float tensor of shape **(N, 3, H, W)**, typically resized to\n",
    "      **224×224**, normalized with **ImageNet** stats:\n",
    "        - mean = [0.485, 0.456, 0.406]\n",
    "        - std  = [0.229, 0.224, 0.225]\n",
    "    - Output: unnormalized class **logits** of shape **(N, num_classes)**.\n",
    "    - To fine-tune the backbone as well, set `requires_grad=True` on its parameters\n",
    "      before training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(StreetClassifier, self).__init__()\n",
    "        # Load the pretrained VGG16 model (trained on ImageNet)\n",
    "        self.backbone = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "            \n",
    "        # Replace the classifier with a new one for our specific task\n",
    "        in_features = self.backbone.classifier[-1].in_features\n",
    "        self.backbone.classifier[-1] = nn.Linear(in_features=in_features, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "The function below handles the **training loop** for the StreetClassifier.  \n",
    "It takes care of:  \n",
    "- Running the model for a given number of epochs.  \n",
    "- Calculating the training loss.  \n",
    "- Evaluating accuracy on the validation set after each epoch.  \n",
    "- Saving the best model checkpoint based on validation accuracy.  \n",
    "\n",
    "For this exercise, you don’t need to train the model from scratch — a **pretrained checkpoint** has already been provided to save time.  \n",
    "Still, you can run the function for one epoch to see how the training behaves, or even train the model fully from scratch if you’d like to experiment.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and train\n",
    "model = StreetClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "trained_model, best_accuracy = helper_utils.train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader, \n",
    "    dev_loader=dev_loader,\n",
    "    num_epochs=1,\n",
    "    optimizer=optimizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pretrained Checkpoint\n",
    "\n",
    "For the exercises, we will work with a **pretrained StreetClassifier model**.  \n",
    "The training checkpoint, including both the model weights and optimizer state, is stored in the file **`vgg_16.pt`**.  \n",
    "\n",
    "We will load this checkpoint and then use the `compute_accuracy` function to evaluate the model’s performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Use the base model\n",
    "model = StreetClassifier().to(device)\n",
    "\n",
    "# Load the final model checkpoint\n",
    "checkpoint = torch.load(\"vgg_16.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Compute accuracy of the loaded model\n",
    "base_accuracy = helper_utils.compute_accuracy(model, test_loader, device)\n",
    "print(f\"Model accuracy: {base_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Utilities\n",
    "\n",
    "In this section, we will explore **model pruning**, a technique to reduce the size and complexity of neural networks by removing less important weights.  \n",
    "To simplify the process, we provide a couple of helper functions:  \n",
    "\n",
    "- `_iter_prunable_modules(model)` – iterates over all `Conv2d` and `Linear` layers, which are the layers we will target for pruning.  \n",
    "- `finalize_pruning(model)` – makes pruning permanent by removing reparametrization and storing the pruned weights directly.  \n",
    "\n",
    "These utilities will make it easier to apply and finalize pruning across the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def _iter_prunable_modules(model):\n",
    "    \"\"\"\n",
    "    Iterate over modules that are eligible for pruning.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Tuple[str, nn.Module]\n",
    "        Pairs of (fully-qualified module name, module) for layers that are\n",
    "        prunable in this assignment: `nn.Conv2d` and `nn.Linear`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The qualified name comes from `model.named_modules()` and reflects the\n",
    "      path within the module hierarchy (e.g., \"block.0\", \"classifier.fc\").\n",
    "    - Use this generator to systematically apply pruning across the model.\n",
    "    \"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            yield name, m\n",
    "\n",
    "def finalize_pruning(model):\n",
    "    \"\"\"\n",
    "    Make pruning permanent by removing reparametrization wrappers.\n",
    "\n",
    "    This converts any pruned parameter from the (`weight_orig`, `weight_mask`)\n",
    "    reparametrization back to a regular `weight` `nn.Parameter` where the\n",
    "    zeros are **materialized** in the stored tensor.\n",
    "    \"\"\"\n",
    "    for _, module in _iter_prunable_modules(model):\n",
    "        # Only remove if the parameter has been pruned\n",
    "        if hasattr(module, \"weight_orig\") and hasattr(module, \"weight_mask\"):\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementing Model Pruning\n",
    "\n",
    "Now you will implement the function `prune_model`, which applies pruning to all `Conv2d` and `Linear` layers in the **StreetClassifier** model.  \n",
    "You can choose between two pruning modes:  \n",
    "\n",
    "- **`l1_unstructured`** – removes a fraction of the smallest-magnitude weights.  \n",
    "- **`ln_structured`** – removes entire output channels (structured pruning), which can lead to faster inference on some hardware.  \n",
    "\n",
    "#### Why Prune a Model?\n",
    "- **Smaller models** → reduced memory footprint and easier deployment on resource-constrained devices.  \n",
    "- **Faster inference** → especially with structured pruning, which eliminates entire channels.  \n",
    "- **Regularization effect** → pruning can sometimes improve generalization by removing redundant connections.  \n",
    "\n",
    "### prune_model -- details\n",
    "\n",
    "Implement ```prune_model(model, amount=0.3, mode=\"l1_unstructured\")```, a function that:\n",
    "\n",
    "Applies magnitude-based pruning to the weights of every nn.Conv2d and nn.Linear layer in a given model using PyTorch’s pruning reparametrization API. The pruning is applied in-place (adds weight_orig and weight_mask) and does not change any tensor shapes. To permanently bake zeros into the stored weights (remove reparametrization), a separate helper finalize_pruning(model) is provided for you to call afterward.\n",
    "\n",
    "The function proceeds through these stages:\n",
    "\n",
    "Validate Inputs\n",
    "\n",
    "1. Ensure amount is a float in [0, 1].\n",
    "\n",
    "2. Ensure mode is one of {\"l1_unstructured\", \"ln_structured\"}.\n",
    "\n",
    "3. Find Prunable Modules\n",
    "    - Iterate over the model and select only ```nn.Conv2d``` and ```nn.Linear``` layers (a helper _iter_prunable_modules(model) is available).\n",
    "    - Skip any module that does not have a weight attribute.\n",
    "4. Apply Pruning (In-Place Reparametrization)\n",
    "     - For unstructured pruning (default): Use ```prune.l1_unstructured(module, name=\"weight\", amount=amount)``` to zero the smallest-magnitude individual weights within each tensor.\n",
    "     - For structured pruning: Use ```prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)``` to zero out entire output channels (L2-norm across channel filters), leaving tensor shapes unchanged but producing channel-wise sparsity masks.\n",
    "5. Return the Same Model Instance\n",
    "Return the input model with pruning reparametrization attached (i.e., weight becomes a computed tensor from weight_orig * weight_mask).\n",
    "\n",
    "Note: Parameters and buffers now include weight_orig and weight_mask for pruned layers.\n",
    "\n",
    "Downstream code can call ```finalize_pruning(model)``` to remove the reparametrization objects and write the masked values into the raw weight tensors.\n",
    "\n",
    "Even after finalization, shapes remain the same; channels are zeroed, not physically removed.\n",
    "\n",
    "The output of this function—a model with pruning masks attached—will later be used by grading code that (a) checks the presence of reparametrized weights, (b) inspects sparsity levels, and (c) optionally calls finalize_pruning(model) before export or quantization.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "1) Iterate only over prunable layers\n",
    "\n",
    "You’ll be given a helper like:\n",
    "\n",
    "```\n",
    "    for _, module in _iter_prunable_modules(model):\n",
    "```\n",
    "\n",
    "\n",
    "This should yield nn.Conv2d and nn.Linear modules. Still, defensively skip modules that don’t have module.weight.\n",
    "\n",
    "2) Two pruning modes you must support\n",
    "\n",
    "Unstructured (default):\n",
    "```\n",
    "    prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "```\n",
    "\n",
    "Zeros individual weights with the smallest L1 magnitudes.\n",
    "\n",
    "Structured:\n",
    "```\n",
    "    prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "```\n",
    "\n",
    "Zeros entire output channels based on L2 norms. Shapes do not change here; channels are masked.\n",
    "\n",
    "3) Validate arguments early\n",
    "\n",
    "Raise:\n",
    "```\n",
    "if not (0.0 <= amount <= 1.0):\n",
    "    raise ValueError(f\"amount must be in [0,1], got {amount}\")\n",
    "```\n",
    "\n",
    "and for unsupported modes:\n",
    "```\n",
    "    raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
    "```\n",
    "\n",
    "4) Remember: pruning is in-place & reparametrized\n",
    "\n",
    "After pruning a layer, you’ll see attributes:\n",
    "\n",
    "    - weight_orig (the original parameter)\n",
    "    - weight_mask (a buffer of 0/1s)\n",
    "    - weight becomes a computed tensor (not a leaf parameter).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: prune_model\n",
    "\n",
    "def prune_model(model, amount=0.3, mode=\"l1_unstructured\"):\n",
    "    \"\"\"\n",
    "    Apply pruning to **weights** of all `Conv2d` and `Linear` layers.\n",
    "\n",
    "    This uses PyTorch's pruning reparametrization (adds `weight_orig` and\n",
    "    `weight_mask`) without changing the tensor shape. To permanently embed\n",
    "    zeros into the stored weights, call `finalize_pruning(model)` afterward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to prune. Pruning is applied **in-place** via\n",
    "        `torch.nn.utils.prune`.\n",
    "    amount : float, optional (default=0.3)\n",
    "        Fraction in [0, 1] to prune.\n",
    "        - For **unstructured** pruning: fraction of smallest-magnitude weights\n",
    "          within each tensor.\n",
    "        - For **structured (ln)** pruning: fraction of **output channels**\n",
    "          (dimension 0) to remove using L2-norm (n=2).\n",
    "    mode : {\"l1_unstructured\", \"ln_structured\"}, optional\n",
    "        Pruning strategy:\n",
    "        - `\"l1_unstructured\"` → `prune.l1_unstructured(..., name=\"weight\", amount=amount)`\n",
    "        - `\"ln_structured\"`   → `prune.ln_structured(..., name=\"weight\", amount=amount, n=2, dim=0)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        The same model instance with pruning **reparametrization** applied\n",
    "        (not yet made permanent).\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    if not (0.0 <= amount <= 1.0): # @REPLACE if True: # Check if amount is in [0,1]\n",
    "        raise ValueError(f\"amount must be in [0,1], got {amount}\") # @KEEP\n",
    "\n",
    "    for _, module in _iter_prunable_modules(model): # @KEEP\n",
    "        if not hasattr(module, \"weight\"): # @REPLACE if True: # Check if module has \"weight\" attribute\n",
    "            continue # @KEEP\n",
    "\n",
    "        if mode == \"l1_unstructured\": # @REPLACE if True: # Check if mode is \"l1_unstructured\"\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount) # @REPLACE prune the module using l1_unstructured\n",
    "        elif mode == \"ln_structured\": # @REPLACE if True: # Check if mode is \"ln_structured\"\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)  # @REPLACE prune the module using ln_structured\n",
    "        else: # @KEEP\n",
    "            raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your code, run the following cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "base = helper_utils.sparsity_report(model)\n",
    "print(\"[BASE] global_sparsity:\", base[\"global_sparsity\"])\n",
    "print(\"[BASE] per-layer:\", base[\"layers\"])\n",
    "\n",
    "# We prune 50% of the model\n",
    "prune_model(model, amount=0.5, mode=\"l1_unstructured\")\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "after = helper_utils.sparsity_report(model)\n",
    "print(\"[AFTER PRUNE] global_sparsity:\", after[\"global_sparsity\"])\n",
    "print(\"[AFTER PRUNE] per-layer:\", after[\"layers\"])\n",
    "\n",
    "after_acc = helper_utils.compute_accuracy(model, test_loader, device=\"cpu\")\n",
    "print(\"[AFTER PRUNE] accuracy:\", after_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "[BASE] global_sparsity: 0.0\n",
    "[BASE] per-layer: {'backbone.features.0.weight': 0.0, 'backbone.features.2.weight': 0.0, 'backbone.features.5.weight': 0.0, 'backbone.features.7.weight': 0.0, 'backbone.features.10.weight': 0.0, 'backbone.features.12.weight': 0.0, 'backbone.features.14.weight': 0.0, 'backbone.features.17.weight': 0.0, 'backbone.features.19.weight': 0.0, 'backbone.features.21.weight': 0.0, 'backbone.features.24.weight': 0.0, 'backbone.features.26.weight': 0.0, 'backbone.features.28.weight': 0.0, 'backbone.classifier.0.weight': 0.0, 'backbone.classifier.3.weight': 0.0, 'backbone.classifier.6.weight': 0.0}\n",
    "[AFTER PRUNE] global_sparsity: 0.5\n",
    "[AFTER PRUNE] per-layer: {'backbone.features.0.weight': 0.5, 'backbone.features.2.weight': 0.5, 'backbone.features.5.weight': 0.5, 'backbone.features.7.weight': 0.5, 'backbone.features.10.weight': 0.5, 'backbone.features.12.weight': 0.5, 'backbone.features.14.weight': 0.5, 'backbone.features.17.weight': 0.5, 'backbone.features.19.weight': 0.5, 'backbone.features.21.weight': 0.5, 'backbone.features.24.weight': 0.5, 'backbone.features.26.weight': 0.5, 'backbone.features.28.weight': 0.5, 'backbone.classifier.0.weight': 0.5, 'backbone.classifier.3.weight': 0.5, 'backbone.classifier.6.weight': 0.5}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test 1: Prune Model\n",
    "\n",
    "unittests.exercise1(prune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization in Deep Learning\n",
    "\n",
    "Quantization is a technique that reduces the **precision** of model weights and activations, typically converting them from 32-bit floating point (FP32) to lower-precision formats like 8-bit integers (INT8).  \n",
    "This can drastically reduce the size of the model and improve inference speed, especially on CPUs and edge devices, while keeping accuracy close to that of the original FP32 model.  \n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Quantization\n",
    "\n",
    "- **Smaller model size** → INT8 weights require 4× less storage than FP32.  \n",
    "- **Faster inference** → Integer operations are usually faster than floating-point on CPUs.  \n",
    "- **Lower memory bandwidth** → Reduced precision means less data transfer, improving latency.  \n",
    "- **Deployment-friendly** → Ideal for running models on devices with limited resources (mobile, IoT, embedded).  \n",
    "\n",
    "---\n",
    "\n",
    "### Dynamic Quantization\n",
    "\n",
    "There are different approaches to quantization:  \n",
    "- **Static Quantization**: requires calibration with representative data before inference.  \n",
    "- **Quantization Aware Training (QAT)**: simulates quantization effects during training for maximum accuracy retention.  \n",
    "- **Dynamic Quantization**: the simplest approach — it keeps weights in INT8 but performs activations in FP32, dynamically quantizing them at runtime.  \n",
    "\n",
    "**Dynamic quantization is particularly effective for models dominated by `nn.Linear` layers (e.g., Transformers, LSTMs, fully-connected classifiers).**  \n",
    "It requires **no retraining or calibration** and is CPU-only, making it the fastest way to get the benefits of quantization.  \n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Implementing Dynamic Quantization\n",
    "\n",
    "In this exercise, you will complete the function `quantize_dynamic_linear`, which should:  \n",
    "\n",
    "- Make a **deep copy** of the original model (do not modify the original).  \n",
    "- Apply **dynamic quantization** to **only** the `nn.Linear` layers, converting them to INT8.  \n",
    "- Return the quantized model in `eval()` mode.  \n",
    "- Ensure it works in CPU-only environments.  \n",
    "\n",
    "This will give you hands-on practice with PyTorch’s `torch.quantization.quantize_dynamic` utility and help you understand how quantization can be applied selectively to certain model components.\n",
    "\n",
    "### Details -- `quantize_dynamic_linear`\n",
    "\n",
    "Implement `quantize_dynamic_linear`, a function that: returns a new model in eval() mode where all nn.Linear layers are dynamically quantized to INT8. Dynamic quantization stores weights as INT8 and quantizes activations on-the-fly at runtime, giving CPU speed/memory wins without calibration.\n",
    "\n",
    "What your function must do\n",
    "\n",
    "1. Clone the model (don’t mutate the original) and switch to eval mode\n",
    "    - copy.deepcopy(model) so the input model is untouched.\n",
    "    - Call .eval() on the copy.\n",
    "\n",
    "2. (CPU) Select a sensible quantization engine if available\n",
    "    - On x86 CPUs, fbgemm is common. Set it if present:\n",
    "    - torch.backends.quantized.engine = \"fbgemm\" inside a try/except.\n",
    "\n",
    "3. Apply dynamic INT8 quantization to nn.Linear only\n",
    "    - Use torch.quantization.quantize_dynamic(...) on the FP32 copy.\n",
    "    - Pass the module set {nn.Linear} so only linear layers are quantized.\n",
    "    - Use dtype=torch.qint8.\n",
    "\n",
    "4. Return the quantized model in eval() mode\n",
    "\n",
    "Ensure you return .eval() (no training-time behavior).\n",
    "\n",
    "The function should work CPU-only (no CUDA, no calibration steps).\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "1. Work on a copy, in eval mode\n",
    "    ```\n",
    "        model_fp32 = copy.deepcopy(model).eval()\n",
    "    ```\n",
    "2. Pick a CPU quantization engine if available (harmless if not)\n",
    "    ```\n",
    "        if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
    "            try:\n",
    "                torch.backends.quantized.engine = \"fbgemm\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    ```\n",
    "\n",
    "3. Quantize ONLY Linear layers to INT8 dynamically\n",
    "    ```\n",
    "        qmodel = torch.quantization.quantize_dynamic(\n",
    "            model_fp32,\n",
    "            {nn.Linear},          # target modules\n",
    "            dtype=torch.qint8,    # INT8 weights\n",
    "        ).eval()                   # ensure eval mode\n",
    "    ```\n",
    "\n",
    "4. Return the quantized model\n",
    "    ```\n",
    "        return qmodel\n",
    "    ```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: quantize_dynamic_linear\n",
    "\n",
    "def quantize_dynamic_linear(model):\n",
    "    \"\"\"\n",
    "    Return a **new** model where all nn.Linear layers are dynamically quantized to INT8.\n",
    "\n",
    "    Requirements checked by the autograder\n",
    "    --------------------------------------\n",
    "    - Do NOT mutate the original model; use a deepcopy.\n",
    "    - Quantize ONLY Linear modules (e.g., {nn.Linear}).\n",
    "    - Use dynamic quantization with INT8 dtype.\n",
    "    - Return the quantized model in eval() mode.\n",
    "    - Should run on CPU-only environments (no CUDA, no calibration).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        An eval-mode copy of `model` with Linear layers using INT8 dynamic quantization.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    model_fp32 = copy.deepcopy(model).eval() # @REPLACE model_fp32 = None # Create a deep copy of the model and set it to eval mode\n",
    "\n",
    "    # Ensure a sensible engine on CPU (x86). If unavailable, this line is harmless.\n",
    "    if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"): # @REPLACE if True: # Check if torch.backends has quantized and torch.backends.quantized has engine\n",
    "        try:\n",
    "            torch.backends.quantized.engine = \"fbgemm\" # @KEEP\n",
    "        except Exception: # @KEEP\n",
    "            pass  # keep whatever the runtime supports # @KEEP\n",
    "    # Quantize only Linear layers to INT8\n",
    "    quantized = torch.quantization.quantize_dynamic( # @REPLACE quantized = None ( # Use quantize_dynamic from quantization in torch to quantize the model_fp32 to INT8\n",
    "        model_fp32, # @REPLACE None, # The model to quantize\n",
    "        {nn.Linear}, # @REPLACE None, # The layers to quantize\n",
    "        dtype=torch.qint8, # @REPLACE None, # The dtype to quantize to qint8\n",
    "    )\n",
    "    \n",
    "    quantized.eval() # @REPLACE None, # Set the quantized model to eval mode\n",
    "\n",
    "    return quantized # @REPLACE return None, # Return the quantized model\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "# Use the base model to start fresh\n",
    "model = StreetClassifier().to(device)\n",
    "\n",
    "# Load the final model checkpoint\n",
    "checkpoint = torch.load(\"vgg_16.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Load the base model model\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Quantize the model\n",
    "qmodel = quantize_dynamic_linear(model)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "qacc = helper_utils.compute_accuracy(qmodel, test_loader, device=\"cpu\")\n",
    "print(f\"\\nAccuracy on test dataset after quantization: {qacc:.2f}%\")\n",
    "\n",
    "# Benchmark the models  \n",
    "t_fp32 = helper_utils.bench(model)\n",
    "t_int8 = helper_utils.bench(qmodel)\n",
    "print(\"\\n[TIMING] avg forward per batch (CPU)\")\n",
    "print(f\"  - FP32 : {t_fp32*1e3:.2f} ms\")\n",
    "print(f\"  - INT8 : {t_int8*1e3:.2f} ms (↓ is better)\")\n",
    "print(f\"  - Improvement: {((t_fp32 - t_int8)/t_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Accuracy on test dataset after quantization: 0.97%\n",
    "\n",
    "[TIMING] avg forward per batch (CPU)\n",
    "  - FP32 : 200.54 ms\n",
    "  - INT8 : 185.24 ms (↓ is better)\n",
    "  - Improvement: 7.6%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "unittests.exercise2(quantize_dynamic_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Fusion to Quantization-Aware Training (QAT)\n",
    "\n",
    "In the previous sections you reduced model size and latency with pruning and (optionally) dynamic quantization.  \n",
    "Now you’ll make the model **quantization-friendly** and prepare it for **INT8 training** by:\n",
    "\n",
    "1) **Fusing ops** that commonly appear together (e.g., `Conv + BN + ReLU`) into single fused modules.  \n",
    "2) **Preparing for QAT**, which inserts observers and fake-quantization modules so the model “feels” INT8 during training and learns robust, quantization-tolerant weights.\n",
    "\n",
    "### What is QAT and why use it?\n",
    "**Quantization-Aware Training (QAT)** simulates INT8 behavior during training (via fake-quant) so that, after conversion, the **final INT8 model preserves more accuracy** than post-training quantization alone—especially on CNNs.\n",
    "\n",
    "**Benefits:**\n",
    "- **Higher accuracy under INT8** vs. dynamic/static PTQ on many convolutional models.  \n",
    "- **Production-ready path**: train with fake-quant → convert to real INT8 → deploy.  \n",
    "- **Works with standard PyTorch tooling** (eager mode, observers, qconfigs).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Function (used by Exercise 3)\n",
    "\n",
    "### `_try_fuse(module, names)`\n",
    "This helper attempts PyTorch eager-mode fusion on a sequence of child layers inside a `nn.Sequential`.  \n",
    "It **gracefully ignores unsupported patterns** (some backbones or PyTorch versions won’t fuse every combo), so your fusion pass can be “best-effort” without breaking.\n",
    "\n",
    "**You will use it to** fuse common patterns such as:\n",
    "- `Conv2d + BatchNorm2d + ReLU`  \n",
    "- `Conv2d + BatchNorm2d`  \n",
    "- `Conv2d + ReLU`  \n",
    "- `Linear + ReLU`\n",
    "\n",
    "Fusing reduces op count and numerical overhead at inference and **sets up cleaner patterns for QAT**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def _try_fuse(seq: nn.Sequential, names):\n",
    "    \"\"\"\n",
    "    Best-effort wrapper around torch.ao.quantization.fuse_modules.\n",
    "    Replaces fused positions with intrinsic fused ops / Identity in-place.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fuse_modules = getattr(torch.ao.quantization, \"fuse_modules\", None)\n",
    "        if fuse_modules is None:  # PyTorch < 1.13 fallback\n",
    "            fuse_modules = getattr(torch.quantization, \"fuse_modules\")\n",
    "        # Inplace fusion inside the *same* Sequential\n",
    "        fuse_modules(seq, names, inplace=True)\n",
    "    except Exception:\n",
    "        # Best-effort: ignore unsupported patterns / backends\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 — Implement `fuse_model_inplace(model)`\n",
    "\n",
    "You’ll write a recursive, **best-effort** fusion routine that:\n",
    "- Walks the model tree (`named_children`) and recurses into submodules.\n",
    "- When it finds a `nn.Sequential`, scans adjacent layers and tries to fuse the patterns listed above using `_try_fuse`.\n",
    "- Leaves unsupported cases untouched (no errors).\n",
    "\n",
    "**What to expect after fusion**\n",
    "- In `print(model)`, some sequences will be replaced by intrinsic fused modules (e.g., `ConvBnReLU2d`, `ConvReLU2d`, `LinearReLU`).\n",
    "- Forward outputs in `eval()` should remain (nearly) identical, showing fusion preserved behavior.\n",
    "- This step improves **CPU inference efficiency** and provides **better numerics for QAT**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: fuse_model_inplace\n",
    "\n",
    "def fuse_model_inplace(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Recursively apply best-effort eager fusion to:\n",
    "      Conv+BN+ReLU, Conv+BN, Conv+ReLU, Linear+ReLU\n",
    "    Only fuses *adjacent* modules inside nn.Sequential blocks.\n",
    "    Modifies `model` in-place and returns the *same instance*.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    for _, child in model.named_children(): # @REPLACE for _, child in [ (None, None)]: # Iterate over the named children of the model\n",
    "        # Recurse first\n",
    "        fuse_model_inplace(child) # @REPLACE fuse_model_inplace(None) # Recursively apply best-effort eager fusion to the child\n",
    "\n",
    "        # Then scan this child if it's a Sequential\n",
    "        if isinstance(child, nn.Sequential) and len(child) >= 2: # @REPLACE if True: # Check if the child is a Sequential and has at least 2 layers\n",
    "            # BN folding prefers eval; don’t mutate outer state permanently\n",
    "            was_training = child.training # @REPLACE was_training = None # Get the training state of the child\n",
    "            child.eval() # @REPLACE child.eval() # Set the child to eval mode\n",
    "            i = 0 # @KEEP   \n",
    "            while i < len(child) - 1: # Iterate over the child layers - 1 # @KEEP\n",
    "                a, b = child[i], child[i + 1] # @REPLACE a, b = None, None # Get the two adjacent layers at i and i + 1\n",
    "                if i + 2 < len(child): # @REPLACE if True: # Check if the third layer exists\n",
    "                    c = child[i + 2] # @REPLACE c = None # Get the third layer\n",
    "                else: #  @KEEP\n",
    "                    c = None # @REPLACE c = None # set the third layer to None\n",
    "\n",
    "                # Conv + BN + ReLU\n",
    "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d) and isinstance(c, nn.ReLU): # @REPLACE if True: # Check if the first layer is a Conv2d, the second layer is a BatchNorm2d, and the third layer is a ReLU\n",
    "                    _try_fuse(child, [str(i), str(i+1), str(i+2)]) # @REPLACE _try_fuse(None, [\"\", \"\", \"\"]) # Try to fuse the three layers\n",
    "                    i += 3 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Conv + BN\n",
    "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d): # @REPLACE if True: # Check if the first layer is a Conv2d and the second layer is a BatchNorm2d\n",
    "                    _try_fuse(child, [str(i), str(i+1)]) # @REPLACE _try_fuse(None, [\"\", \"\"]) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Conv + ReLU\n",
    "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU): # @REPLACE if True: # Check if the first layer is a Conv2d and the second layer is a ReLU\n",
    "                    _try_fuse(child, [str(i), str(i+1)]) # @REPLACE _try_fuse(None, [\"\", \"\"]) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Linear + ReLU\n",
    "                if isinstance(a, nn.Linear) and isinstance(b, nn.ReLU): # @REPLACE if True: # Check if the first layer is a Linear and the second layer is a ReLU\n",
    "                    _try_fuse(child, [str(i), str(i+1)]) # @REPLACE _try_fuse(None, [\"\", \"\"]) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                \n",
    "                # Conv + ReLU\n",
    "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU): # @REPLACE if True: # Check if the first layer is a Conv2d and the second layer is a ReLU\n",
    "                    _try_fuse(child, [str(i), str(i+1)]) # @REPLACE _try_fuse(None, [\"\", \"\"]) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Linear + ReLU\n",
    "                if isinstance(a, nn.Linear) and isinstance(b, nn.ReLU): # @REPLACE if True: # Check if the first layer is a Linear and the second layer is a ReLU\n",
    "                    _try_fuse(child, [str(i), str(i+1)]) # @REPLACE _try_fuse(None, [\"\", \"\"]) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "\n",
    "                i += 1 # @KEEP\n",
    "\n",
    "            if was_training: # @REPLACE if True: # Check if the child was training\n",
    "                child.train() # @REPLACE child.train() # Set the child to train mode\n",
    "\n",
    "    # IMPORTANT: return the same object (tests check identity)\n",
    "    return model # @REPLACE return None, # Return the model\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code\n",
    "\n",
    "# Create a toy model to test your code\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "toy = helper_utils.ToyNet().eval().to(device)\n",
    "\n",
    "# Keep a copy for numerical comparison\n",
    "toy_copy = helper_utils.ToyNet().eval().to(device)\n",
    "toy_copy.load_state_dict(toy.state_dict())\n",
    "\n",
    "# Show BEFORE\n",
    "helper_utils.list_children(toy, \"Before fusion\")\n",
    "\n",
    "# Forward pass BEFORE\n",
    "x = torch.randn(2, 3, 32, 32, device=device)\n",
    "with torch.no_grad():\n",
    "    y_before = toy(x)\n",
    "\n",
    "# Apply your fusion function (assumes fuse_model_inplace is defined + _try_fuse available)\n",
    "ret_model = fuse_model_inplace(toy).eval()\n",
    "# Show AFTER\n",
    "helper_utils.list_children(toy, \"After fusion\")\n",
    "\n",
    "# Forward pass AFTER\n",
    "with torch.no_grad():\n",
    "    y_after = toy(x)\n",
    "\n",
    "# Report numerical closeness and fused-layer counts\n",
    "max_abs_diff = (y_before - y_after).abs().max().item()\n",
    "fused_counts = helper_utils.count_fused_layers(toy)\n",
    "\n",
    "\n",
    "print(\"\\n== Verification ==\")\n",
    "print(f\"Max |y_before - y_after|: {max_abs_diff:.6g}  (expect ~0)\")\n",
    "print(\"Fused intrinsic layers found:\", fused_counts if fused_counts else \"{} (none)\")\n",
    "\n",
    "# sanity check on output shape\n",
    "print(\"Output shapes -> before:\", tuple(y_before.shape), \", after:\", tuple(y_after.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "```\n",
    "== Before fusion ==\n",
    "\n",
    "[stem]\n",
    "  0: Conv2d\n",
    "  1: BatchNorm2d\n",
    "  2: ReLU\n",
    "\n",
    "[block]\n",
    "  0: Conv2d\n",
    "  1: ReLU\n",
    "  2: Conv2d\n",
    "  3: BatchNorm2d\n",
    "  4: ReLU\n",
    "\n",
    "[head]\n",
    "  0: AdaptiveAvgPool2d\n",
    "  1: Flatten\n",
    "  2: Linear\n",
    "  3: ReLU\n",
    "  4: Linear\n",
    "\n",
    "== After fusion ==\n",
    "\n",
    "[stem]\n",
    "  0: ConvReLU2d\n",
    "  1: Identity\n",
    "  2: Identity\n",
    "\n",
    "[block]\n",
    "  0: ConvReLU2d\n",
    "  1: Identity\n",
    "  2: ConvReLU2d\n",
    "  3: Identity\n",
    "  4: Identity\n",
    "\n",
    "[head]\n",
    "  0: AdaptiveAvgPool2d\n",
    "  1: Flatten\n",
    "  2: LinearReLU\n",
    "  3: Identity\n",
    "  4: Linear\n",
    "\n",
    "== Verification ==\n",
    "Max |y_before - y_after|: 1.11759e-08  (expect ~0)\n",
    "Fused intrinsic layers found: {'ConvReLU2d': 3, 'LinearReLU': 1}\n",
    "Output shapes -> before: (2, 3) , after: (2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "unittests.exercise3(fuse_model_inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Auxiliary Functions (used by Exercise 4)\n",
    "To complete exercise 4 you will need the following functions:\n",
    "\n",
    "#### `QATWrapper`\n",
    "Wraps your FP32 model with `QuantStub`/`DeQuantStub`. During QAT/inference prep:\n",
    "- Inputs are quantized → model body runs with fake-quant/observers → outputs dequantized.\n",
    "- This scaffolding allows eager-mode QAT to be applied cleanly.\n",
    "\n",
    "#### `convert_qat(model)`\n",
    "After you finish QAT fine-tuning:\n",
    "- Switch the model to `eval()`\n",
    "- Use the configured backend (e.g., **`fbgemm`** on x86, **`qnnpack`** on ARM)\n",
    "- **Convert** the QAT model to a **real INT8** model for deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def convert_qat(model):\n",
    "    \"\"\"\n",
    "    Convert a trained QAT model to a quantized INT8 model for inference.\n",
    "    Call `model.eval()` before measuring latency/accuracy.\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    model.eval()\n",
    "    try:\n",
    "        torch.backends.quantized.engine = getattr(torch.backends.quantized, \"engine\", \"fbgemm\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    aoq.convert(model, inplace=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrapper for QAT\n",
    "class QATWrapper(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.quant = aoq.QuantStub()\n",
    "        self.m = m\n",
    "        self.dequant = aoq.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.m(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 — Implement `prepare_qat(model, backend=\"fbgemm\")`\n",
    "\n",
    "Create and return a **QAT-ready copy** of the input model. Your function should:\n",
    "\n",
    "1. **Do not mutate the original**: `deepcopy` the model and work on the copy.  \n",
    "2. **Set backend**: choose the quantized engine (`\"fbgemm\"` for x86, `\"qnnpack\"` for ARM).  \n",
    "3. **Fuse first**: call your fusion pass (`fuse_model_inplace`) on the copy.  \n",
    "4. **Attach a QAT qconfig**: use `get_default_qat_qconfig(backend)` (fall back to a sensible default if needed).  \n",
    "5. **Prepare for QAT**: run eager-mode `prepare_qat` to insert observers/fake-quant modules.  \n",
    "6. **Return in `train()` mode** so the learner can fine-tune with QAT.\n",
    "\n",
    "**Outcome:**  \n",
    "You’ll have a training-ready module that, after a brief fine-tuning, can be converted with `convert_qat(...)` into a compact, fast **INT8 inference model** with strong accuracy retention.\n",
    "\n",
    "#### Details — `prepare_qat`\n",
    "\n",
    "Implement `prepare_qat(model, backend=\"fbgemm\")`, a function that: returns a QAT-ready copy of an FP32 model by selecting an appropriate quantized backend, fusing eligible blocks (e.g., `Conv+BN(+ReLU)`), attaching a default QAT qconfig, and running eager-mode prepare_qat to insert observers and fake-quant modules.\n",
    "\n",
    "***Note:*** The original model must remain unmodified; the returned module must be in train() mode.\n",
    "\n",
    "The function has the following stages:\n",
    "\n",
    "1. Clone & Switch to Train Mode (No Mutation)\n",
    "    - Create a deep copy of the input model so the original remains intact.\n",
    "    - Put the copy in training mode: .train() (QAT requires training mode).\n",
    "2. Select Quantized Backend\n",
    "    - Set torch.backends.quantized.engine to the requested backend if available.\n",
    "    - Defaults: \"fbgemm\" (x86) or \"qnnpack\" (ARM).\n",
    "    - If unsupported, keep the runtime’s current engine (best-effort).\n",
    "3. Fuse Eligible Modules (Best-Effort)\n",
    "    - Call the created helper fuse_model_inplace(qat) to fuse common patterns like (Conv, BN, ReLU), (Conv, BN), (Conv, ReLU), (Linear, ReLU).\n",
    "    \n",
    "4. Attach a Default QAT qconfig\n",
    "    - Obtain a backend-appropriate QAT config (e.g., aoq.get_default_qat_qconfig(backend)).\n",
    "    - If that fails, fall back to aoq.get_default_qat_qconfig(\"fbgemm\").\n",
    "    - Assign it to qat.qconfig.\n",
    "\n",
    "5. Insert Observers & Fake-Quant (Eager QAT Prepare)\n",
    "    - Call aoq.prepare_qat(qat, inplace=True) to add observers and fake-quant modules throughout the network.\n",
    "    - These modules simulate quantization effects during training.\n",
    "    - Ensure the returned module is in train() mode and ready for QAT fine-tuning.\n",
    "\n",
    "Return the new (deep-copied) QAT-ready model.\n",
    "\n",
    "The output of this function—a QAT-ready model—will then be fine-tuned for a few epochs. During training, the inserted observers and fake-quantization modules learn appropriate scales/zero-points, enabling a high-accuracy post-training convert to INT8 later.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "1. ***Backend selection (best-effort)***\n",
    "    Prefer:\n",
    "        - \"fbgemm\" on x86/AVX2+ CPUs\n",
    "        - \"qnnpack\" on ARM\n",
    "    Safe pattern:\n",
    "    ```\n",
    "    if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
    "        try:\n",
    "            torch.backends.quantized.engine = backend  # e.g., \"fbgemm\" or \"qnnpack\"\n",
    "        except Exception:\n",
    "            pass  # leave current engine if unsupported\n",
    "    ```\n",
    "\n",
    "2. ***Fusion helper***\n",
    "    You are given fuse_model_inplace(qat). Call it after copying and before preparing QAT:\n",
    "    ```\n",
    "    fuse_model_inplace(qat)  # best-effort; no-op if pattern not found\n",
    "    ```\n",
    "    Fusing improves numerical stability and performance for quantization workflows.\n",
    "\n",
    "3. ***QAT configuration***\n",
    "    Pick a default QAT config tied to the backend; fall back to \"fbgemm\" if needed:\n",
    "    ```\n",
    "    try:\n",
    "        qconfig = aoq.get_default_qat_qconfig(backend)\n",
    "    except Exception:\n",
    "        qconfig = aoq.get_default_qat_qconfig(\"fbgemm\")\n",
    "    qat.qconfig = qconfig\n",
    "    ```\n",
    "\n",
    "4. ***Insert observers & fake-quant***\n",
    "    Use eager-mode QAT preparation:\n",
    "    ```\n",
    "    aoq.prepare_qat(qat, inplace=True)\n",
    "    qat.train()  # ensure training mode for QAT fine-tuning\n",
    "    ```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: prepare_qat\n",
    "\n",
    "def prepare_qat(model, backend=\"fbgemm\"):\n",
    "    \"\"\"\n",
    "    Return a **QAT-ready copy** of `model`:\n",
    "      - Sets quantized backend (default: 'fbgemm')\n",
    "      - Applies best-effort fusion (Conv+BN(+Act))\n",
    "      - Attaches a default QAT qconfig\n",
    "      - Runs eager-mode prepare_qat to insert observers/fake-quant\n",
    "      - Returns the prepared module in **train()** mode\n",
    "\n",
    "    The original `model` **must not** be mutated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        FP32 model to prepare for QAT.\n",
    "    backend : str\n",
    "        Quantized engine (use 'fbgemm' on x86; 'qnnpack' on ARM).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        A new, QAT-ready model (with observers) in training mode.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # 1) Work on a copy; do not mutate the original\n",
    "    qat = copy.deepcopy(model).train() # @REPLACE qat = None, # Create a deep copy of the model and set it to train mode\n",
    "\n",
    "    # 2) Configure quantized backend\n",
    "    has_quantized = hasattr(torch.backends, \"quantized\") # @REPLACE has_quantized = None, # Check if torch.backends has quantized\n",
    "    has_engine = hasattr(torch.backends.quantized, \"engine\") # @REPLACE has_engine = None, # Check if torch.backends.quantized has engine\n",
    "    if has_quantized and has_engine: # @REPLACE if True: # Check if torch.backends has quantized and torch.backends.quantized has engine\n",
    "        try: # @KEEP\n",
    "            torch.backends.quantized.engine = backend # @REPLACE torch.backends.quantized.engine = None, # Set the quantized engine to the backend\n",
    "        except Exception:  # \n",
    "            pass  # keep current engine if backend is unsupported\n",
    "\n",
    "    # 3) Fuse eligible modules (best-effort; safe no-op if unsupported)\n",
    "    fuse_model_inplace(qat) # @REPLACE fuse_model_inplace(None) # Fuse the eligible modules (qat)\n",
    "\n",
    "    # 4) Attach default QAT qconfig\n",
    "    try: # @KEEP\n",
    "        qconfig = aoq.get_default_qat_qconfig(backend) # @REPLACE qconfig = None, # Get the default QAT qconfig from aoq for the backend\n",
    "    except Exception: # @KEEP\n",
    "        # Fallback to a gener ic default if backend-specific isn't available\n",
    "        qconfig = aoq.get_default_qat_qconfig(\"fbgemm\") # @REPLACE qconfig = None, # Get the default QAT qconfig from aoq for \"fbgemm\"\n",
    "    qat.qconfig = qconfig # @REPLACE qat.qconfig = None, # Set the qconfig to the qconfig\n",
    "\n",
    "    # 5) Prepare for QAT (insert observers/fake-quant)\n",
    "    aoq.prepare_qat( # @REPLACE aoq.prepare_qat( # Prepare the model for QAT\n",
    "        qat, # @REPLACE None,, # The model to prepare for QAT\n",
    "        inplace=True # @REPLACE inplace=None, # Set the correct value for inplace\n",
    "        ) # @KEEP\n",
    "\n",
    "    # Ensure model is in training mode for QAT fine-tuning\n",
    "    qat.train() # @REPLACE None # Set the model to train mode\n",
    "    ### END CODE HERE ###\n",
    "    return qat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "# Use the base model to start fresh\n",
    "model = StreetClassifier().to(device)\n",
    "checkpoint = torch.load(\"vgg_16.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# wrap the base model in QATWrapper\n",
    "wrapped_model = QATWrapper(model)\n",
    "\n",
    "print(\"Base Model loaded and wrapped\")\n",
    "\n",
    "# Prepare the QAT model\n",
    "qat_model = prepare_qat(wrapped_model, backend=\"fbgemm\")\n",
    "print(\"Model prepared for qat\")\n",
    "\n",
    "# Fine-tune with fake-quant in the loop (can be on GPU)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD((p for p in qat_model.parameters() if p.requires_grad),\n",
    "                            lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "qat_model.to(device)\n",
    "\n",
    "helper_utils.train_model(\n",
    "    qat_model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    2,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save_path=\"fine_tuned_qat_model.pt\")\n",
    "    \n",
    "qat_model.to(\"cpu\")\n",
    "\n",
    "# Convert to real INT8 (runs on CPU)\n",
    "qat_model.eval()\n",
    "int8_model = convert_qat(qat_model)\n",
    "print(\"Model converted to int8\")\n",
    "\n",
    "# Save the quantized model with full state\n",
    "torch.save({\n",
    "    'model_state_dict': int8_model.state_dict(),\n",
    "    'quantization_config': int8_model.state_dict()\n",
    "}, \"quantized_int8_model.pt\")\n",
    "\n",
    "print(\"Saved quantized model checkpoint to quantized_int8_model.pt\")\n",
    "\n",
    "# Evaluate int8 model on test data\n",
    "int8_model.eval()\n",
    "print(\"Testing model on cpu\")\n",
    "test_acc = helper_utils.compute_accuracy(int8_model, test_loader, device=\"cpu\")\n",
    "print(f\"Test accuracy in base model: {base_accuracy:.2f}%\")\n",
    "print(f'\\nInt8 model test accuracy: {test_acc:.2f}%')\n",
    "\n",
    "# Measure inference time for both models\n",
    "base_time = helper_utils.bench(model)\n",
    "int8_time = helper_utils.bench(int8_model)\n",
    "\n",
    "# Calculate percentage improvement\n",
    "time_improvement = ((base_time - int8_time) / base_time) * 100\n",
    "\n",
    "print(f\"\\nInference time comparison:\")\n",
    "print(f\"Base model: {base_time:.4f} seconds per batch\")\n",
    "print(f\"Int8 model: {int8_time:.4f} seconds per batch\") \n",
    "print(f\"Speed improvement: {time_improvement:.1f}%\")\n",
    "\n",
    "# Save both models weights to compare sizes\n",
    "torch.save(model.state_dict(), \"base_model_weights.pt\")\n",
    "torch.save(int8_model.state_dict(), \"int8_model_weights.pt\")\n",
    "\n",
    "# Get file sizes in MB\n",
    "base_size = os.path.getsize(\"base_model_weights.pt\") / (1024 * 1024)\n",
    "int8_size = os.path.getsize(\"int8_model_weights.pt\") / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nModel size comparison:\")\n",
    "print(f\"Base model: {base_size:.2f} MB\")\n",
    "print(f\"Int8 model: {int8_size:.2f} MB\")\n",
    "print(f\"Size reduction: {((base_size - int8_size) / base_size * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "\n",
    "```\n",
    "Base Model loaded and wrapped\n",
    "Model prepared for qat\n",
    "\n",
    "New best accuracy: 0.9519, saved model to best_model.pt\n",
    "\n",
    "New best accuracy: 0.9667, saved model to best_model.pt\n",
    "\n",
    "Training completed:\n",
    "Best accuracy: 0.9667\n",
    "Final accuracy: 0.9667\n",
    "Final model saved to final_model.pt\n",
    "Model converted to int8\n",
    "Saved quantized model checkpoint to quantized_int8_model.pt\n",
    "Testing model on cpu\n",
    "\n",
    "Test accuracy in base model: 0.97%\n",
    "\n",
    "Int8 model test accuracy: 0.96%\n",
    "\n",
    "Inference time comparison:\n",
    "Base model: 0.0346 seconds per batch\n",
    "Int8 model: 0.0193 seconds per batch\n",
    "Speed improvement: 44.2%\n",
    "\n",
    "Model size comparison:\n",
    "Base model: 512.22 MB\n",
    "Int8 model: 128.31 MB\n",
    "Size reduction: 75.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "unittests.exercise4(prepare_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You started from a solid baseline and made it deployable. By saving and reloading state dictionaries you protected your progress and created a repeatable path back to working models. With pruning you explored how zeroing less useful parameters can make a network leaner, and you saw how masks change behavior without immediately changing tensor shapes. Dynamic quantization gave you a rapid path to smaller weights and faster CPU inference, and the fusion plus quantization aware training workflow helped you fine tune under simulated int8 effects so the final converted model holds onto accuracy while gaining speed.\n",
    "\n",
    "You now have a StreetClassifier that is lighter, faster, and easier to package. The same workflow scales to larger architectures and different tasks: prune with intent, quantify the accuracy and latency impact, and then use quantization aware training when you want the best balance. As next steps, you can automate experiment tracking, prune with channel removal to alter layer shapes, or export to formats that run beyond PyTorch so your models serve users wherever they live."
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
