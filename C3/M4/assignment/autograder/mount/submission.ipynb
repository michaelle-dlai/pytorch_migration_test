{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanStreetNet ‚Äî Edge Vision for Cleaner Cities\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You have already trained many models; now it is time to shape one for life outside a notebook. This lab guides you through taking a pre-trained VGG16 based StreetClassifier and turning it into a version that is easier to store, faster on CPU, and ready for deployment on edge targets. Along the way you will practice saving and reloading model state, pruning parameters, applying quantization, and evaluating the trade offs among accuracy, latency, and size.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- Load the CleanStreetDataset, initialize a pre-trained StreetClassifier, and establish a baseline evaluation.\n",
    "\n",
    "- Work with checkpoints by saving and restoring state_dict objects for both training and inference.\n",
    "\n",
    "- Implement magnitude based pruning across convolutional and linear layers, with options for unstructured and structured strategies, and verify sparsity and accuracy.\n",
    "\n",
    "- Apply dynamic quantization to linear layers for a fast CPU speedup and benchmark the effect.\n",
    "\n",
    "- Fuse common layer patterns and prepare a quantization aware variant, fine tune briefly, then convert to an int8 model.\n",
    "\n",
    "- Compare accuracy, inference time, and file size before and after compression to understand the impact of each step.\n",
    "\n",
    "By the end, you will have a compact classifier that keeps performance close to the original while being far more efficient to run and ship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h4 style=\"color:green; font-weight:bold;\">TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:</h4>\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- - To submit your notebook for grading, first save it by clicking the üíæ icon on the top left of the page and then click on the <span style=\"background-color: blue; color: white; padding: 3px 5px; font-size: 16px; border-radius: 5px;\">Submit assignment</span> button on the top right of the page.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as aoq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper_utils\n",
    "import unittests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Clean StreetClassifier data and model\n",
    "\n",
    "**StreetClassifier** is a deep learning model designed to classify urban scene images into three categories:  \n",
    "- **clean**  \n",
    "- **litter**  \n",
    "- **recycle** \n",
    "\n",
    "### Dataset\n",
    "\n",
    "For this task, we will use the **CleanStreetDataset**, which is already divided into **training**, **development**, and **test** splits.  \n",
    "In the code below, you‚Äôll see how the datasets are loaded and how data preprocessing and augmentation transforms are applied to prepare the data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "dataset_path = \"./data/\"\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasetsfrom torch.nn.utils import prune\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=train_transform)\n",
    "dev_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'dev'), transform=eval_transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'test'), transform=eval_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(dev_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))\n",
    "print(\"\\nClass mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize some examples with the following helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "helper_utils.display_some_images(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "The **StreetClassifier** model builds on **ResNet-18**, a well-known convolutional neural network pretrained on ImageNet.  \n",
    "In this implementation the final classifier layer was replaced so the model can predict the three target classes of our dataset (*clean*, *litter*, and *recycle*). \n",
    "\n",
    "To make your work easier the ResNet-18 has been modified so it can use QTA in latter exercises.\n",
    "You can see its architecture and details with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the base model\n",
    "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=False).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pretrained Checkpoint\n",
    "\n",
    "For the exercises, we will work with a **pretrained StreetClassifier model**. The model weights are stored in the file **`street_classifier_weights.pt`**.  \n",
    "\n",
    "We will load the weights and then use the `compute_accuracy` function to evaluate the model‚Äôs performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the final model checkpoint\n",
    "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# Compute accuracy of the loaded model\n",
    "base_accuracy = helper_utils.compute_accuracy(model, test_loader, device)\n",
    "print(f\"Model accuracy: {base_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Utilities\n",
    "\n",
    "In this section, we will explore **model pruning**, a technique to reduce the size and complexity of neural networks by removing less important weights.  \n",
    "To simplify the process, we provide a couple of helper functions:  \n",
    "\n",
    "- `_iter_prunable_modules(model)` ‚Äì iterates over all `Conv2d` and `Linear` layers, which are the layers we will target for pruning.  \n",
    "- `finalize_pruning(model)` ‚Äì makes pruning permanent by removing reparametrization and storing the pruned weights directly.  \n",
    "\n",
    "These utilities will make it easier to apply and finalize pruning across the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def _iter_prunable_modules(model):\n",
    "    \"\"\"\n",
    "    Iterate over modules that are eligible for pruning.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Tuple[str, nn.Module]\n",
    "        Pairs of (fully-qualified module name, module) for layers that are\n",
    "        prunable in this assignment: `nn.Conv2d` and `nn.Linear`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The qualified name comes from `model.named_modules()` and reflects the\n",
    "      path within the module hierarchy (e.g., \"block.0\", \"classifier.fc\").\n",
    "    - Use this generator to systematically apply pruning across the model.\n",
    "    \"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            yield name, m\n",
    "\n",
    "def finalize_pruning(model):\n",
    "    \"\"\"\n",
    "    Make pruning permanent by removing reparametrization wrappers.\n",
    "\n",
    "    This converts any pruned parameter from the (`weight_orig`, `weight_mask`)\n",
    "    reparametrization back to a regular `weight` `nn.Parameter` where the\n",
    "    zeros are **materialized** in the stored tensor.\n",
    "    \"\"\"\n",
    "    for _, module in _iter_prunable_modules(model):\n",
    "        # Only remove if the parameter has been pruned\n",
    "        if hasattr(module, \"weight_orig\") and hasattr(module, \"weight_mask\"):\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementing Model Pruning\n",
    "\n",
    "Now you will implement the function `prune_model`, which applies pruning to all `Conv2d` and `Linear` layers in the **StreetClassifier** model.  \n",
    "You can choose between two pruning modes:  \n",
    "\n",
    "- **`l1_unstructured`** ‚Äì removes a fraction of the smallest-magnitude weights.  \n",
    "- **`ln_structured`** ‚Äì removes entire output channels (structured pruning), which can lead to faster inference on some hardware.  \n",
    "\n",
    "#### Why Prune a Model?\n",
    "- **Smaller models** ‚Üí reduced memory footprint and easier deployment on resource-constrained devices.  \n",
    "- **Faster inference** ‚Üí especially with structured pruning, which eliminates entire channels.  \n",
    "- **Regularization effect** ‚Üí pruning can sometimes improve generalization by removing redundant connections.  \n",
    "\n",
    "### prune_model -- details\n",
    "\n",
    "Implement ```prune_model(model, amount=0.3, mode=\"l1_unstructured\")```, a function that:\n",
    "\n",
    "Applies magnitude-based pruning to the weights of every nn.Conv2d and nn.Linear layer in a given model using PyTorch‚Äôs pruning reparametrization API. The pruning is applied in-place (adds weight_orig and weight_mask) and does not change any tensor shapes. To permanently bake zeros into the stored weights (remove reparametrization), a separate helper finalize_pruning(model) is provided for you to call afterward.\n",
    "\n",
    "The function proceeds through these stages:\n",
    "\n",
    "Validate Inputs\n",
    "\n",
    "1. Ensure amount is a float in [0, 1].\n",
    "\n",
    "2. Ensure mode is one of {\"l1_unstructured\", \"ln_structured\"}.\n",
    "\n",
    "3. Find Prunable Modules\n",
    "    - Iterate over the model and select only ```nn.Conv2d``` and ```nn.Linear``` layers (a helper _iter_prunable_modules(model) is available).\n",
    "    - Skip any module that does not have a weight attribute.\n",
    "4. Apply Pruning (In-Place Reparametrization)\n",
    "     - For unstructured pruning (default): Use ```prune.l1_unstructured(module, name=\"weight\", amount=amount)``` to zero the smallest-magnitude individual weights within each tensor.\n",
    "     - For structured pruning: Use ```prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)``` to zero out entire output channels (L2-norm across channel filters), leaving tensor shapes unchanged but producing channel-wise sparsity masks.\n",
    "5. Return the Same Model Instance\n",
    "Return the input model with pruning reparametrization attached (i.e., weight becomes a computed tensor from weight_orig * weight_mask).\n",
    "\n",
    "Note: Parameters and buffers now include weight_orig and weight_mask for pruned layers.\n",
    "\n",
    "Downstream code can call ```finalize_pruning(model)``` to remove the reparametrization objects and write the masked values into the raw weight tensors.\n",
    "\n",
    "Even after finalization, shapes remain the same; channels are zeroed, not physically removed.\n",
    "\n",
    "The output of this function‚Äîa model with pruning masks attached‚Äîwill later be used by grading code that (a) checks the presence of reparametrized weights, (b) inspects sparsity levels, and (c) optionally calls finalize_pruning(model) before export or quantization.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "1) Iterate only over prunable layers\n",
    "\n",
    "You‚Äôll be given a helper like:\n",
    "\n",
    "```\n",
    "    for _, module in _iter_prunable_modules(model):\n",
    "```\n",
    "\n",
    "\n",
    "This should yield nn.Conv2d and nn.Linear modules. Still, defensively skip modules that don‚Äôt have module.weight.\n",
    "\n",
    "2) Two pruning modes you must support\n",
    "\n",
    "Unstructured (default):\n",
    "```\n",
    "    prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "```\n",
    "\n",
    "Zeros individual weights with the smallest L1 magnitudes.\n",
    "\n",
    "Structured:\n",
    "```\n",
    "    prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "```\n",
    "\n",
    "Zeros entire output channels based on L2 norms. Shapes do not change here; channels are masked.\n",
    "\n",
    "3) Validate arguments early\n",
    "\n",
    "Raise:\n",
    "```\n",
    "if not (0.0 <= amount <= 1.0):\n",
    "    raise ValueError(f\"amount must be in [0,1], got {amount}\")\n",
    "```\n",
    "\n",
    "and for unsupported modes:\n",
    "```\n",
    "    raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
    "```\n",
    "\n",
    "4) Remember: pruning is in-place & reparametrized\n",
    "\n",
    "After pruning a layer, you‚Äôll see attributes:\n",
    "\n",
    "    - weight_orig (the original parameter)\n",
    "    - weight_mask (a buffer of 0/1s)\n",
    "    - weight becomes a computed tensor (not a leaf parameter).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: prune_model\n",
    "\n",
    "def prune_model(model, amount=0.3, mode=\"l1_unstructured\"):\n",
    "    \"\"\"\n",
    "    Apply pruning to **weights** of all `Conv2d` and `Linear` layers.\n",
    "\n",
    "    This uses PyTorch's pruning reparametrization (adds `weight_orig` and\n",
    "    `weight_mask`) without changing the tensor shape. To permanently embed\n",
    "    zeros into the stored weights, call `finalize_pruning(model)` afterward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model to prune. Pruning is applied **in-place** via\n",
    "        `torch.nn.utils.prune`.\n",
    "    amount : float, optional (default=0.3)\n",
    "        Fraction in [0, 1] to prune.\n",
    "        - For **unstructured** pruning: fraction of smallest-magnitude weights\n",
    "          within each tensor.\n",
    "        - For **structured (ln)** pruning: fraction of **output channels**\n",
    "          (dimension 0) to remove using L2-norm (n=2).\n",
    "    mode : {\"l1_unstructured\", \"ln_structured\"}, optional\n",
    "        Pruning strategy:\n",
    "        - `\"l1_unstructured\"` ‚Üí `prune.l1_unstructured(..., name=\"weight\", amount=amount)`\n",
    "        - `\"ln_structured\"`   ‚Üí `prune.ln_structured(..., name=\"weight\", amount=amount, n=2, dim=0)`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        The same model instance with pruning **reparametrization** applied\n",
    "        (not yet made permanent).\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    if not (0.0 <= amount <= 1.0): # @REPLACE if None: # Check if amount is in [0,1]\n",
    "        raise ValueError(f\"amount must be in [0,1], got {amount}\") # @KEEP\n",
    "\n",
    "    for _, module in _iter_prunable_modules(model):\n",
    "        if not hasattr(module, \"weight\"): # @REPLACE if None: # Check if module has \"weight\" attribute\n",
    "            continue # @KEEP\n",
    "\n",
    "        if mode == \"l1_unstructured\": # @REPLACE if None: # Check if mode is \"l1_unstructured\"\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount) # @REPLACE None # l1_unstructured from prune with module, name(\"weight\"), and amount\n",
    "        elif mode == \"ln_structured\": # @REPLACE elif None: # Check elif mode is \"ln_structured\"\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)  # @REPLACE None # ln_structured from prune with module, name(\"weight\"), amount, n(2), and dim(0)\n",
    "        else: # @KEEP\n",
    "            raise ValueError(\"mode must be 'l1_unstructured' or 'ln_structured'\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your code, run the following cells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "# Create baseline model statistics\n",
    "base = helper_utils.sparsity_report(model)\n",
    "print(\"[BASE] global_sparsity:\", base[\"global_sparsity\"])\n",
    "base_time = helper_utils.bench(model, device=device)\n",
    "print(\"[BASE] time:\", base_time)\n",
    "\n",
    "# We prune 50% of the model\n",
    "prune_model(model, amount=0.5, mode=\"l1_unstructured\")\n",
    "\n",
    "after = helper_utils.sparsity_report(model)\n",
    "print(\"[AFTER PRUNE] global_sparsity:\", after[\"global_sparsity\"])\n",
    "\n",
    "after_acc = helper_utils.compute_accuracy(model, test_loader, device=device)\n",
    "print(\"[AFTER PRUNE] accuracy:\", after_acc)\n",
    "\n",
    "pruned_time = helper_utils.bench(model, device=device)\n",
    "\n",
    "print(f\"\\nInference time comparison:\")\n",
    "print(f\"Base model: {base_time:.4f} seconds per batch\")\n",
    "print(f\"Pruned model: {pruned_time:.4f} seconds per batch\") \n",
    "print(f\"Speedup: {base_time / pruned_time:.2f}x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "[BASE] global_sparsity: 0.0\n",
    "[BASE] time: 0.004570210154633969\n",
    "[AFTER PRUNE] global_sparsity: 0.5\n",
    "[AFTER PRUNE] accuracy: 0.9914529914529915\n",
    "\n",
    "Inference time comparison:\n",
    "Base model: 0.0046 seconds per batch\n",
    "Pruned model: 0.0035 seconds per batch\n",
    "Speedup: 1.29x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test 1: Prune Model\n",
    "\n",
    "unittests.exercise1(prune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization in Deep Learning\n",
    "\n",
    "Quantization is a technique that reduces the **precision** of model weights and activations, typically converting them from 32-bit floating point (FP32) to lower-precision formats like 8-bit integers (INT8).  \n",
    "This can drastically reduce the size of the model and improve inference speed, especially on CPUs and edge devices, while keeping accuracy close to that of the original FP32 model.  \n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Quantization\n",
    "\n",
    "- **Smaller model size** ‚Üí INT8 weights require 4√ó less storage than FP32.  \n",
    "- **Faster inference** ‚Üí Integer operations are usually faster than floating-point on CPUs.  \n",
    "- **Lower memory bandwidth** ‚Üí Reduced precision means less data transfer, improving latency.  \n",
    "- **Deployment-friendly** ‚Üí Ideal for running models on devices with limited resources (mobile, IoT, embedded).  \n",
    "\n",
    "---\n",
    "\n",
    "### Dynamic Quantization\n",
    "\n",
    "There are different approaches to quantization:  \n",
    "- **Static Quantization**: requires calibration with representative data before inference.  \n",
    "- **Quantization Aware Training (QAT)**: simulates quantization effects during training for maximum accuracy retention.  \n",
    "- **Dynamic Quantization**: the simplest approach ‚Äî it keeps weights in INT8 but performs activations in FP32, dynamically quantizing them at runtime.  \n",
    "\n",
    "**Dynamic quantization is particularly effective for models dominated by `nn.Linear` layers (e.g., Transformers, LSTMs, fully-connected classifiers).**  \n",
    "It requires **no retraining or calibration** and is CPU-only, making it the fastest way to get the benefits of quantization.  \n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Implementing Dynamic Quantization\n",
    "\n",
    "In this exercise, you will complete the function `quantize_dynamic_linear`, which should:  \n",
    "\n",
    "- Make a **deep copy** of the original model (do not modify the original).  \n",
    "- Apply **dynamic quantization** to **only** the `nn.Linear` layers, converting them to INT8.  \n",
    "- Return the quantized model in `eval()` mode.  \n",
    "- Ensure it works in CPU-only environments.  \n",
    "\n",
    "This will give you hands-on practice with PyTorch‚Äôs `torch.quantization.quantize_dynamic` utility and help you understand how quantization can be applied selectively to certain model components.\n",
    "\n",
    "### Details -- `quantize_dynamic_linear`\n",
    "\n",
    "Implement `quantize_dynamic_linear`, a function that: returns a new model in eval() mode where all nn.Linear layers are dynamically quantized to INT8. Dynamic quantization stores weights as INT8 and quantizes activations on-the-fly at runtime, giving CPU speed/memory wins without calibration.\n",
    "\n",
    "What your function must do\n",
    "\n",
    "1. Clone the model (don‚Äôt mutate the original) and switch to eval mode\n",
    "    - copy.deepcopy(model) so the input model is untouched.\n",
    "    - Call .eval() on the copy.\n",
    "\n",
    "2. (CPU) Select a sensible quantization engine if available\n",
    "    - On x86 CPUs, fbgemm is common. Set it if present:\n",
    "    - torch.backends.quantized.engine = \"fbgemm\" inside a try/except.\n",
    "\n",
    "3. Apply dynamic INT8 quantization to nn.Linear only\n",
    "    - Use torch.quantization.quantize_dynamic(...) on the FP32 copy.\n",
    "    - Pass the module set {nn.Linear} so only linear layers are quantized.\n",
    "    - Use dtype=torch.qint8.\n",
    "\n",
    "4. Return the quantized model in eval() mode\n",
    "\n",
    "Ensure you return .eval() (no training-time behavior).\n",
    "\n",
    "The function should work CPU-only (no CUDA, no calibration steps).\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "1. Work on a copy, in eval mode\n",
    "    ```\n",
    "        model_fp32 = copy.deepcopy(model).eval()\n",
    "    ```\n",
    "2. Pick a CPU quantization engine if available (harmless if not)\n",
    "    ```\n",
    "        if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
    "            try:\n",
    "                torch.backends.quantized.engine = \"fbgemm\"\n",
    "            except Exception:\n",
    "                pass\n",
    "    ```\n",
    "\n",
    "3. Quantize ONLY Linear layers to INT8 dynamically\n",
    "    ```\n",
    "        qmodel = torch.quantization.quantize_dynamic(\n",
    "            model_fp32,\n",
    "            {nn.Linear},          # target modules\n",
    "            dtype=torch.qint8,    # INT8 weights\n",
    "        ).eval()                   # ensure eval mode\n",
    "    ```\n",
    "\n",
    "4. Return the quantized model\n",
    "    ```\n",
    "        return qmodel\n",
    "    ```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: quantize_dynamic_linear\n",
    "\n",
    "def quantize_dynamic_linear(model):\n",
    "    \"\"\"\n",
    "    Return a **new** model where all nn.Linear layers are dynamically quantized to INT8.\n",
    "\n",
    "    Requirements checked by the autograder\n",
    "    --------------------------------------\n",
    "    - Do NOT mutate the original model; use a deepcopy.\n",
    "    - Quantize ONLY Linear modules (e.g., {nn.Linear}).\n",
    "    - Use dynamic quantization with INT8 dtype.\n",
    "    - Return the quantized model in eval() mode.\n",
    "    - Should run on CPU-only environments (no CUDA, no calibration).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        An eval-mode copy of `model` with Linear layers using INT8 dynamic quantization.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    model_fp32 = copy.deepcopy(model).eval() # @REPLACE model_fp32 = None # Create a deep copy of the model and set it to eval mode\n",
    "\n",
    "    # Ensure a sensible engine on CPU (x86). If unavailable, this line is harmless.\n",
    "    has_quantized = hasattr(torch.backends, \"quantized\") # @REPLACE has_quantized = None # Check if torch.backends has quantized\n",
    "    has_engine = hasattr(torch.backends.quantized, \"engine\") # @REPLACE has_engine = None # Check if torch.backends.quantized has engine\n",
    "    if has_quantized and has_engine: # @KEEP\n",
    "        try:\n",
    "            torch.backends.quantized.engine = \"fbgemm\" # @KEEP\n",
    "        except Exception: # @KEEP\n",
    "            pass  # keep whatever the runtime supports # @KEEP\n",
    "    # Quantize only Linear layers to INT8\n",
    "    quantized = torch.quantization.quantize_dynamic( # @REPLACE quantized = None ( # Use quantize_dynamic from quantization in torch to quantize the model_fp32 to INT8\n",
    "        model_fp32, # @REPLACE None, # The model to quantize\n",
    "        {nn.Linear}, # @REPLACE {None}, # The layers to quantize (only Linear layers)\n",
    "        dtype=torch.qint8, # @REPLACE dtype=None # The dtype to quantize to qint8\n",
    "    )\n",
    "    \n",
    "    quantized.eval() # @REPLACE None # Set the quantized model to eval mode\n",
    "\n",
    "    return quantized # @REPLACE return None # Return the quantized model\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "# Use the base model to start fresh\n",
    "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=True).to(device)\n",
    "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(model_weights)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(5)\n",
    "\n",
    "# Quantize the model\n",
    "qmodel = quantize_dynamic_linear(model)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "qacc = helper_utils.compute_accuracy(qmodel, test_loader, device=\"cpu\")\n",
    "print(f\"\\nAccuracy on test dataset after quantization: {qacc:.2f}%\")\n",
    "\n",
    "# Benchmark the models  \n",
    "t_fp32 = helper_utils.bench(model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
    "t_int8 = helper_utils.bench(qmodel, device=\"cpu\", shape=(32, 3, 224, 224))\n",
    "print(\"\\n[TIMING] avg forward per batch (CPU)\")\n",
    "print(f\"  - FP32 : {t_fp32*1e3:.2f} ms\")\n",
    "print(f\"  - INT8 : {t_int8*1e3:.2f} ms (‚Üì is better)\")\n",
    "print(f\"  - Improvement: {((t_fp32 - t_int8)/t_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output:\n",
    "```\n",
    "Accuracy on test dataset after quantization: 0.99%\n",
    "\n",
    "[TIMING] avg forward per batch (CPU)\n",
    "  - FP32 : 948.63 ms\n",
    "  - INT8 : 947.09 ms (‚Üì is better)\n",
    "  - Improvement: 0.2%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "unittests.exercise2(quantize_dynamic_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Fusion to Quantization-Aware Training (QAT)\n",
    "\n",
    "In the previous sections you reduced model size and latency with pruning and dynamic quantization.  \n",
    "Now you‚Äôll make the model **quantization-friendly** and prepare it for **INT8 training** by:\n",
    "\n",
    "1) **Fusing ops** that commonly appear together (e.g., `Conv + BN + ReLU`) into single fused modules.  \n",
    "2) **Preparing for QAT**, which inserts observers and fake-quantization modules so the model ‚Äúfeels‚Äù INT8 during training and learns robust, quantization-tolerant weights.\n",
    "\n",
    "### What is QAT and why use it?\n",
    "**Quantization-Aware Training (QAT)** simulates INT8 behavior during training (via fake-quant) so that, after conversion, the **final INT8 model preserves more accuracy** than post-training quantization alone‚Äîespecially on CNNs.\n",
    "\n",
    "**Benefits:**\n",
    "- **Higher accuracy under INT8** vs. dynamic/static PTQ on many convolutional models.  \n",
    "- **Production-ready path**: train with fake-quant ‚Üí convert to real INT8 ‚Üí deploy.  \n",
    "- **Works with standard PyTorch tooling** (eager mode, observers, qconfigs).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 ‚Äî Implement `fuse_model_inplace(model)`\n",
    "\n",
    "You‚Äôll write a recursive, **best-effort** fusion routine that:\n",
    "- Walks the model tree (`named_children`) and recurses into submodules.\n",
    "- When it finds a `nn.Sequential`, scans adjacent layers and tries to fuse the patterns listed above using `_try_fuse`.\n",
    "- Leaves unsupported cases untouched (no errors).\n",
    "\n",
    "**What to expect after fusion**\n",
    "- In `print(model)`, some sequences will be replaced by intrinsic fused modules (e.g., `ConvBnReLU2d`, `ConvReLU2d`, `LinearReLU`).\n",
    "- Forward outputs in `eval()` should remain (nearly) identical, showing fusion preserved behavior.\n",
    "- This step improves **CPU inference efficiency** and provides **better numerics for QAT**.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary><b><font color=\"green\">Additional Code Hints for <code>fuse_model_inplace</code> (click to expand)</font></b></summary>\n",
    "\n",
    "\n",
    "### 1) Recursing through the model\n",
    "- You want to visit children first (depth-first), so use PyTorch‚Äôs iterator:\n",
    "  ```python\n",
    "  for _, child in model.named_children():\n",
    "      fuse_model_inplace(child)\n",
    "  ```\n",
    "This ensures you fuse inner blocks before the parent.\n",
    "\n",
    "2) Only scan nn.Sequential blocks\n",
    "After recursing, if a child is a Sequential with at least 2 modules, scan it:\n",
    "\n",
    "  ```python\n",
    "    if isinstance(child, nn.Sequential) and len(child) >= 2:\n",
    "  ```\n",
    "Store training state, then temporarily switch to eval mode for safer BN folding:\n",
    "\n",
    "  ```python\n",
    "    was_training = child.training\n",
    "    child.eval()\n",
    "  ```    \n",
    "3) Sliding window over adjacent layers\n",
    "Use an index i and look at child[i], child[i+1], and (if present) child[i+2]:\n",
    "  ```python\n",
    "    i = 0\n",
    "    while i < len(child) - 1:\n",
    "        a, b = child[i], child[i + 1]\n",
    "        c = child[i + 2] if i + 2 < len(child) else None\n",
    "  ```\n",
    "Check patterns in priority order. After a successful fusion, bump i past the fused group and continue so you don‚Äôt re-scan the just-fused spots.\n",
    "\n",
    "4) The fusion checks (pattern + call + index advance)\n",
    "Use torch.quantization.fuse_modules(child, [str(i), ...], inplace=True).\n",
    "\n",
    "Conv + BN + ReLU\n",
    "  ```python\n",
    "    if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d) and isinstance(c, nn.ReLU):\n",
    "        torch.quantization.fuse_modules(child, [str(i), str(i+1), str(i+2)], inplace=True)\n",
    "        i += 3\n",
    "        continue\n",
    "  ```\n",
    "Conv + BN\n",
    "  ```python\n",
    "    if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d):\n",
    "        torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
    "        i += 2\n",
    "        continue\n",
    "  ```    \n",
    "Conv + ReLU\n",
    "  ```python\n",
    "    if isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU):\n",
    "        torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
    "        i += 2\n",
    "        continue\n",
    "  ```\n",
    "Linear + ReLU\n",
    "  ```python\n",
    "    if isinstance(a, nn.Linear) and isinstance(b, nn.ReLU):\n",
    "        torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True)\n",
    "        i += 2\n",
    "        continue\n",
    "  ```\n",
    "\n",
    "If no pattern matched, just i += 1.\n",
    ".\n",
    "\n",
    "5) Restore training state\n",
    "  ```python\n",
    "    if was_training:\n",
    "        child.train()\n",
    "  ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: fuse_model_inplace\n",
    "\n",
    "def fuse_model_inplace(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Recursively apply best-effort eager fusion to:\n",
    "      Conv+BN+ReLU, Conv+BN, Conv+ReLU, Linear+ReLU\n",
    "    Only fuses *adjacent* modules inside nn.Sequential blocks.\n",
    "    Modifies `model` in-place and returns the *same instance*.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    for _, child in model.named_children(): # @REPLACE for _, child in [ (None, None)]: # Iterate over the named children of the model\n",
    "        # Recurse first\n",
    "        fuse_model_inplace(child) # @REPLACE fuse_model_inplace(None) # Recursively apply best-effort eager fusion to the child\n",
    "\n",
    "        # Then scan this child if it's a Sequential\n",
    "        if isinstance(child, nn.Sequential) and len(child) >= 2: # @REPLACE if True: # Check if the child is a Sequential and has at least 2 layers\n",
    "            # BN folding prefers eval; don‚Äôt mutate outer state permanently\n",
    "            was_training = child.training # @REPLACE was_training = None # Get the training state of the child\n",
    "            child.eval() # @REPLACE child.eval() # Set the child to eval mode\n",
    "            i = 0 # @KEEP   \n",
    "            while i < len(child) - 1: # Iterate over the child layers - 1\n",
    "                a, b = child[i], child[i + 1] # @REPLACE a, b = None, None # Get the two adjacent layers at i and i + 1\n",
    "                if i + 2 < len(child): # @REPLACE if None: # Check if the third layer (i+2 > len(child))exists\n",
    "                    c = child[i + 2] # @REPLACE c = None # Get the third layer\n",
    "                else:\n",
    "                    c = None # @REPLACE c = None # set the third layer to None\n",
    "\n",
    "                # Conv + BN + ReLU\n",
    "                if isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d) and isinstance(c, nn.ReLU): # @REPLACE if None: # Check if the first layer is a Conv2d, the second layer is a BatchNorm2d, and the third layer is a ReLU\n",
    "                    torch.quantization.fuse_modules(child, [str(i), str(i+1), str(i+2)], inplace=True) # @REPLACE torch.quantization.fuse_modules(child, [str(i), str(i+1), str(i+2)], inplace=True) # Try to fuse the three layers\n",
    "                    i += 3 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Conv + BN\n",
    "                elif isinstance(a, nn.Conv2d) and isinstance(b, nn.BatchNorm2d): # @REPLACE if None: # Check if the first layer is a Conv2d and the second layer is a BatchNorm2d\n",
    "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # @REPLACE torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Conv + ReLU\n",
    "                elif isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU): # @REPLACE if None: # Check if the first layer is a Conv2d and the second layer is a ReLU\n",
    "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # @REPLACE torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                # Linear + ReLU\n",
    "                elif isinstance(a, nn.Linear) and isinstance(b, nn.ReLU): # @REPLACE if None: # Check if the first layer is a Linear and the second layer is a ReLU\n",
    "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # @REPLACE torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "                \n",
    "                # Conv + ReLU\n",
    "                elif isinstance(a, nn.Conv2d) and isinstance(b, nn.ReLU): # @REPLACE if None: # Check if the first layer is a Conv2d and the second layer is a ReLU\n",
    "                    torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # @REPLACE torch.quantization.fuse_modules(child, [str(i), str(i+1)], inplace=True) # Try to fuse the two layers\n",
    "                    i += 2 # @KEEP\n",
    "                    continue # @KEEP\n",
    "\n",
    "                i += 1 # @KEEP\n",
    "\n",
    "            if was_training: # @REPLACE if None: # Check if the child was training\n",
    "                child.train() # @REPLACE child.train() # Set the child to train mode\n",
    "\n",
    "    # IMPORTANT: return the same object (tests check identity)\n",
    "    return model # @REPLACE return None # Return the model\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code\n",
    "\n",
    "# Create a toy model to test your code\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "toy = helper_utils.ToyNet().eval().to(device)\n",
    "\n",
    "# Keep a copy for numerical comparison\n",
    "toy_copy = helper_utils.ToyNet().eval().to(device)\n",
    "toy_copy.load_state_dict(toy.state_dict())\n",
    "\n",
    "# Show BEFORE\n",
    "helper_utils.list_children(toy, \"Before fusion\")\n",
    "\n",
    "# Forward pass BEFORE\n",
    "x = torch.randn(2, 3, 32, 32, device=device)\n",
    "with torch.no_grad():\n",
    "    y_before = toy(x)\n",
    "\n",
    "# Apply your fusion function (assumes fuse_model_inplace is defined + _try_fuse available)\n",
    "ret_model = fuse_model_inplace(toy).eval()\n",
    "# Show AFTER\n",
    "helper_utils.list_children(toy, \"After fusion\")\n",
    "\n",
    "# Forward pass AFTER\n",
    "with torch.no_grad():\n",
    "    y_after = toy(x)\n",
    "\n",
    "# Report numerical closeness and fused-layer counts\n",
    "max_abs_diff = (y_before - y_after).abs().max().item()\n",
    "fused_counts = helper_utils.count_fused_layers(toy)\n",
    "\n",
    "\n",
    "print(\"\\n== Verification ==\")\n",
    "print(f\"Max |y_before - y_after|: {max_abs_diff:.6g}  (expect ~0)\")\n",
    "print(\"Fused intrinsic layers found:\", fused_counts if fused_counts else \"{} (none)\")\n",
    "\n",
    "# sanity check on output shape\n",
    "print(\"Output shapes -> before:\", tuple(y_before.shape), \", after:\", tuple(y_after.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "```\n",
    "== Before fusion ==\n",
    "\n",
    "[stem]\n",
    "  0: Conv2d\n",
    "  1: BatchNorm2d\n",
    "  2: ReLU\n",
    "\n",
    "[block]\n",
    "  0: Conv2d\n",
    "  1: ReLU\n",
    "  2: Conv2d\n",
    "  3: BatchNorm2d\n",
    "  4: ReLU\n",
    "\n",
    "[head]\n",
    "  0: AdaptiveAvgPool2d\n",
    "  1: Flatten\n",
    "  2: Linear\n",
    "  3: ReLU\n",
    "  4: Linear\n",
    "\n",
    "== After fusion ==\n",
    "\n",
    "[stem]\n",
    "  0: ConvReLU2d\n",
    "  1: Identity\n",
    "  2: Identity\n",
    "\n",
    "[block]\n",
    "  0: ConvReLU2d\n",
    "  1: Identity\n",
    "  2: ConvReLU2d\n",
    "  3: Identity\n",
    "  4: Identity\n",
    "\n",
    "[head]\n",
    "  0: AdaptiveAvgPool2d\n",
    "  1: Flatten\n",
    "  2: LinearReLU\n",
    "  3: Identity\n",
    "  4: Linear\n",
    "\n",
    "== Verification ==\n",
    "Max |y_before - y_after|: 1.11759e-08  (expect ~0)\n",
    "Fused intrinsic layers found: {'ConvReLU2d': 3, 'LinearReLU': 1}\n",
    "Output shapes -> before: (2, 3) , after: (2, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "unittests.exercise3(fuse_model_inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Auxiliary Functions (used by Exercise 4)\n",
    "To complete exercise 4 you will need the following functions:\n",
    "\n",
    "#### `QATWrapper`\n",
    "Wraps your FP32 model with `QuantStub`/`DeQuantStub`. During QAT/inference prep:\n",
    "- Inputs are quantized ‚Üí model body runs with fake-quant/observers ‚Üí outputs dequantized.\n",
    "- This scaffolding allows eager-mode QAT to be applied cleanly.\n",
    "\n",
    "#### `convert_qat(model)`\n",
    "After you finish QAT fine-tuning:\n",
    "- Switch the model to `eval()`\n",
    "- Use the configured backend (e.g., **`fbgemm`** on x86, **`qnnpack`** on ARM)\n",
    "- **Convert** the QAT model to a **real INT8** model for deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Wrapper for QAT\n",
    "class QATWrapper(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.quant = aoq.QuantStub()\n",
    "        self.m = m\n",
    "        self.dequant = aoq.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.m(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 ‚Äî Implement `prepare_qat(model, backend=\"fbgemm\")`\n",
    "\n",
    "Create and return a **QAT-ready copy** of the input model. Your function should:\n",
    "\n",
    "1. **Do not mutate the original**: `deepcopy` the model and work on the copy.  \n",
    "2. **Set backend**: choose the quantized engine (`\"fbgemm\"` for x86, `\"qnnpack\"` for ARM).  \n",
    "3. **Fuse first**: call your fusion pass (`fuse_model_inplace`) on the copy.  \n",
    "4. **Attach a QAT qconfig**: use `get_default_qat_qconfig(backend)` (fall back to a sensible default if needed).  \n",
    "5. **Prepare for QAT**: run eager-mode `prepare_qat` to insert observers/fake-quant modules.  \n",
    "6. **Return in `train()` mode** so the learner can fine-tune with QAT.\n",
    "\n",
    "**Outcome:**  \n",
    "You‚Äôll have a training-ready module that, after a brief fine-tuning, can be converted with `convert_qat(...)` into a compact, fast **INT8 inference model** with strong accuracy retention.\n",
    "\n",
    "#### Details ‚Äî `prepare_qat`\n",
    "\n",
    "Implement `prepare_qat(model, backend=\"fbgemm\")`, a function that: returns a QAT-ready copy of an FP32 model by selecting an appropriate quantized backend, fusing eligible blocks (e.g., `Conv+BN(+ReLU)`), attaching a default QAT qconfig, and running eager-mode prepare_qat to insert observers and fake-quant modules.\n",
    "\n",
    "***Note:*** The original model must remain unmodified; the returned module must be in train() mode.\n",
    "\n",
    "The function has the following stages:\n",
    "\n",
    "1. Clone & Switch to Train Mode (No Mutation)\n",
    "    - Create a deep copy of the input model so the original remains intact.\n",
    "    - Put the copy in training mode: .train() (QAT requires training mode).\n",
    "2. Select Quantized Backend\n",
    "    - Set torch.backends.quantized.engine to the requested backend if available.\n",
    "    - Defaults: \"fbgemm\" (x86) or \"qnnpack\" (ARM).\n",
    "    - If unsupported, keep the runtime‚Äôs current engine (best-effort).\n",
    "3. Fuse Eligible Modules (Best-Effort)\n",
    "    - Call the created helper fuse_model_inplace(qat) to fuse common patterns like (Conv, BN, ReLU), (Conv, BN), (Conv, ReLU), (Linear, ReLU).\n",
    "    \n",
    "4. Insert Observers & Fake-Quant (Eager QAT Prepare)\n",
    "    - Call aoq.prepare_qat(qat, inplace=True) to add observers and fake-quant modules throughout the network.\n",
    "    - These modules simulate quantization effects during training.\n",
    "    - Ensure the returned module is in train() mode and ready for QAT fine-tuning.\n",
    "\n",
    "Return the new (deep-copied) QAT-ready model.\n",
    "\n",
    "The output of this function‚Äîa QAT-ready model‚Äîwill then be fine-tuned for a few epochs. During training, the inserted observers and fake-quantization modules learn appropriate scales/zero-points, enabling a high-accuracy post-training convert to INT8 later.\n",
    "\n",
    "<details> <summary><b><font color=\"green\">Additional Code Hints (Click to expand if you are stuck)</font></b></summary>\n",
    "\n",
    "1. ***Backend selection (best-effort)***\n",
    "    Prefer:\n",
    "        - \"fbgemm\" on x86/AVX2+ CPUs\n",
    "        - \"qnnpack\" on ARM\n",
    "    Safe pattern:\n",
    "    ```\n",
    "    if hasattr(torch.backends, \"quantized\") and hasattr(torch.backends.quantized, \"engine\"):\n",
    "        try:\n",
    "            torch.backends.quantized.engine = backend  # e.g., \"fbgemm\" or \"qnnpack\"\n",
    "        except Exception:\n",
    "            pass  # leave current engine if unsupported\n",
    "    ```\n",
    "\n",
    "2. ***Fusion helper***\n",
    "    You are given fuse_model_inplace(qat). Call it after copying and before preparing QAT:\n",
    "    ```\n",
    "    fuse_model_inplace(qat)  # best-effort; no-op if pattern not found\n",
    "    ```\n",
    "    Fusing improves numerical stability and performance for quantization workflows.\n",
    "\n",
    "3. ***QAT configuration***\n",
    "    Pick a default QAT config tied to the backend; fall back to \"fbgemm\" if needed:\n",
    "    ```\n",
    "    qat.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "    ```\n",
    "\n",
    "4. ***Insert observers & fake-quant***\n",
    "    Use eager-mode QAT preparation:\n",
    "    ```\n",
    "    aoq.prepare_qat(qat, inplace=True)\n",
    "    qat.train()  # ensure training mode for QAT fine-tuning\n",
    "    ```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: prepare_qat\n",
    "\n",
    "def prepare_qat(model, backend=\"fbgemm\"):\n",
    "    \"\"\"\n",
    "    Return a **QAT-ready copy** of `model`:\n",
    "      - Sets quantized backend (default: 'fbgemm')\n",
    "      - Applies best-effort fusion (Conv+BN(+Act))\n",
    "      - Attaches a default QAT qconfig\n",
    "      - Runs eager-mode prepare_qat to insert observers/fake-quant\n",
    "      - Returns the prepared module in **train()** mode\n",
    "\n",
    "    The original `model` **must not** be mutated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        FP32 model to prepare for QAT.\n",
    "    backend : str\n",
    "        Quantized engine (use 'fbgemm' on x86; 'qnnpack' on ARM).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        A new, QAT-ready model (with observers) in training mode.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # 1) Work on a copy; do not mutate the original\n",
    "    qat = copy.deepcopy(model).train() # @REPLACE qat = None # Create a deep copy of the model and set it to train mode\n",
    "    qat.eval()\n",
    "\n",
    "    # 2) Fuse eligible modules (best-effort; safe no-op if unsupported)\n",
    "    fuse_model_inplace(qat) # @REPLACE fuse_model_inplace(None) # Fuse the eligible modules (qat)\n",
    "\n",
    "    # 3) Attach default QAT qconfig\n",
    "    qat.qconfig = torch.quantization.get_default_qat_qconfig(backend) # set to qconfig from qat the value of the default qconfig\n",
    "\n",
    "    # 4) Prepare for QAT (insert observers/fake-quant)\n",
    "    qat.train()\n",
    "    torch.quantization.prepare_qat( # @REPLACE torch.quantization.prepare_qat( # Prepare the model for QAT\n",
    "        qat, # @REPLACE None, # The model to prepare for QAT\n",
    "        inplace=True # @REPLACE inplace=None, # Set the correct value for inplace\n",
    "        ) # @KEEP\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return qat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Verify your code here\n",
    "\n",
    "# Use the base model to start fresh\n",
    "\n",
    "model = helper_utils.resnet18_qat_ready_pretrained(num_classes=3, use_quant_stubs=False).to(device)\n",
    "model_weights = torch.load(\"street_classifier_weights.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(model_weights)\n",
    "# wrap the base model in QATWrapper to add stubs for quantization\n",
    "wrapped_model = QATWrapper(model)\n",
    "\n",
    "print(\"Base Model loaded and wrapped\")\n",
    "\n",
    "# Prepare the QAT model\n",
    "qat_model = prepare_qat(wrapped_model, backend=\"fbgemm\")\n",
    "print(\"Model prepared for qat\")\n",
    "\n",
    "# Fine-tune with fake-quant in the loop (can be on GPU)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD((p for p in qat_model.parameters() if p.requires_grad),\n",
    "                            lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "qat_model.to(device)\n",
    "\n",
    "helper_utils.train_model(\n",
    "    qat_model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    1,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save_path=\"fine_tuned_qat_model.pt\")\n",
    "    \n",
    "qat_model.to(\"cpu\")\n",
    "\n",
    "# Convert to real INT8 (runs on CPU)\n",
    "qat_model.eval()\n",
    "int8_model = torch.quantization.convert(qat_model)\n",
    "print(\"Model converted to int8\")\n",
    "\n",
    "# Save the quantized model with full state\n",
    "torch.save({\n",
    "    'model_state_dict': int8_model.state_dict(),\n",
    "    'quantization_config': int8_model.state_dict()\n",
    "}, \"quantized_int8_model.pt\")\n",
    "\n",
    "print(\"Saved quantized model checkpoint to quantized_int8_model.pt\")\n",
    "\n",
    "# Evaluate int8 model on test data\n",
    "int8_model.eval()\n",
    "print(\"Testing model on cpu\")\n",
    "test_acc = helper_utils.compute_accuracy(int8_model, test_loader, device=\"cpu\")\n",
    "print(f\"Test accuracy in base model: {base_accuracy:.2f}%\")\n",
    "print(f'\\nInt8 model test accuracy: {test_acc:.2f}%')\n",
    "\n",
    "# Measure inference time for both models on cpu\n",
    "model.to(\"cpu\")\n",
    "int8_model.to(\"cpu\")\n",
    "base_time = helper_utils.bench(model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
    "int8_time = helper_utils.bench(int8_model, device=\"cpu\", shape=(32, 3, 224, 224))\n",
    "\n",
    "# Calculate percentage improvement\n",
    "time_improvement = ((base_time - int8_time) / base_time) * 100\n",
    "\n",
    "print(f\"\\nInference time comparison:\")\n",
    "print(f\"Base model: {base_time:.4f} seconds per batch\")\n",
    "print(f\"Int8 model: {int8_time:.4f} seconds per batch\") \n",
    "print(f\"Speed improvement: {time_improvement:.1f}%\")\n",
    "\n",
    "# Save both models weights to compare sizes\n",
    "torch.save(model.state_dict(), \"base_model_weights.pt\")\n",
    "torch.save(int8_model.state_dict(), \"int8_model_weights.pt\")\n",
    "\n",
    "# Get file sizes in MB\n",
    "base_size = os.path.getsize(\"base_model_weights.pt\") / (1024 * 1024)\n",
    "int8_size = os.path.getsize(\"int8_model_weights.pt\") / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nModel size comparison:\")\n",
    "print(f\"Base model: {base_size:.2f} MB\")\n",
    "print(f\"Int8 model: {int8_size:.2f} MB\")\n",
    "print(f\"Size reduction: {((base_size - int8_size) / base_size * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "\n",
    "```\n",
    "Base Model loaded and wrapped\n",
    "Model prepared for qat\n",
    "\n",
    "New best accuracy: 0.9519, saved model to best_model.pt\n",
    "\n",
    "New best accuracy: 0.9667, saved model to best_model.pt\n",
    "\n",
    "Training completed:\n",
    "Best accuracy: 0.9667\n",
    "Final accuracy: 0.9667\n",
    "Final model saved to final_model.pt\n",
    "Model converted to int8\n",
    "Saved quantized model checkpoint to quantized_int8_model.pt\n",
    "Testing model on cpu\n",
    "\n",
    "Test accuracy in base model: 0.97%\n",
    "\n",
    "Int8 model test accuracy: 0.96%\n",
    "\n",
    "Inference time comparison:\n",
    "Base model: 0.0346 seconds per batch\n",
    "Int8 model: 0.0193 seconds per batch\n",
    "Speed improvement: 44.2%\n",
    "\n",
    "Model size comparison:\n",
    "Base model: 512.22 MB\n",
    "Int8 model: 128.31 MB\n",
    "Size reduction: 75.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "unittests.exercise4(prepare_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You started from a solid baseline and made it deployable. By saving and reloading state dictionaries you protected your progress and created a repeatable path back to working models. With pruning you explored how zeroing less useful parameters can make a network leaner, and you saw how masks change behavior without immediately changing tensor shapes. Dynamic quantization gave you a rapid path to smaller weights and faster CPU inference, and the fusion plus quantization aware training workflow helped you fine tune under simulated int8 effects so the final converted model holds onto accuracy while gaining speed.\n",
    "\n",
    "You now have a StreetClassifier that is lighter, faster, and easier to package. The same workflow scales to larger architectures and different tasks: prune with intent, quantify the accuracy and latency impact, and then use quantization aware training when you want the best balance. As next steps, you can automate experiment tracking, prune with channel removal to alter layer shapes, or export to formats that run beyond PyTorch so your models serve users wherever they live."
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
